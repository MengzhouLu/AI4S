# 神经网络tricks

## data

## model

## loss

## optim

# Pytorch函数

## 指定CUDA（多个CUDA核心时）:

放在代码最开始，尤其是import torch前

import os
os.environ["CUDA_VISIBLE_DEVICES"] = "1"#两个CUDA时指定第二个，即系统此时只可见第二个cuda

#对于程序来说我们指定的cuda"1"被认为是cuda:0，即在下面代码中作为cuda:0使用

# 论文

![image-20240626173137132](.\image-20240626173137132.png)![image-20240626173253733](.\image-20240626173253733.png)作者：ML researcher
链接：https://www.zhihu.com/question/422296229/answer/3459572262
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。



ICML、ICLR、NeurIPS三个会各自有各自的特点。从不同角度看排名也各有千秋。以下是笔者根据自己审稿、投稿以及读论文的经验，从不同的目的和角度给出的个人看法：

**论文水平下限：ICML>ICLR>=NeurIPS:**

从我的审稿经验来看，ICML对论文的solid程度要求显著高于另外两个会。这其中的原因不止一个。首先，大部分reviewer本身对于ICML就有**要有理论、有大量实验支撑**的印象，因此reviewer在审稿的时候就是设置好了这个格子然后把论文往里面放。有朋友说，这三个会的审稿人都是一波人，但是即使是同一个人，审不同会的标准也会是不一样的。另外一个可能的原因（笔者审稿时候的个人感觉，不一定对），ICML的AC似乎比NeurIPS的AC更被动。NeurIPS的AC给人一种更相信自己判断的感觉，既敢于捞低分论文，也敢于拒绝高分论文，遇到分数不一致（比如7744僵持不下这种）更敢于自己下判断。但是ICML的AC似乎就更保守，比起7644这种论文更喜欢录665这种，这从一定程度上抬高了论文的下限（但是笔者本人其实非常反对这点，后面会具体说）。而ICLR更倾向于录有[novelty](https://www.zhihu.com/search?q=novelty&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A"3459572262"})的论文，哪怕论文内容没那么solid或是争议性很大。NeurIPS属于二者取平衡，在solid达标的基础上倾向于比较新鲜的工作。这样做虽然拉低了录取文章的下限，但是也是有好处的。

**对于找idea的读者，如果自己水平初级：NeurIPS>=ICLR>>ICML；水平高级：ICLR>NeurIPS>>ICML**

如果屏幕前的你读paper是为了给自己找idea，笔者更推荐ICLR和NeurIPS，因为这两个会有很多很新鲜的idea。刚才笔者提到，和ICML比起来，ICLR和NeurIPS更偏向novel，对solid程度有一定包容性。而新鲜的idea刚出来的时候往往是有很多不足和改进空间的。或者说，正是这样的不足和改进空间为后来者提供了自由发挥的基础。但是ICML对于这种不足相对缺乏包容性，所以ICLR和NeurIPS在提供idea这件事上，要比ICML好很多。ICML的审稿机制和风格导致它对对下面这种类型的论文尤其友好：

***已知一个很著名的模型work了，这个模型的某个零部件（比如[损失函数](https://www.zhihu.com/search?q=损失函数&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A"3459572262"})）是A+B，作者对这个模型做了一大堆实验，发现这里面有个问题，然后根据某个很强的假设，用一系列让人望而生畏的巨大公式，推导出这里不应该是A+B，而应该是c\*A+(1-c)\*B，并给出了这个c的一个形式或者某个下界，但是这个c是某个我们算不出来的东西（比如c可能包含有PAC可学习里面那一堆我们算不出来的复杂度），但是知道这个c大概的范围，因此可以把c当成参数来调。于是作者把A+B换成了[c\*A+(1-c)\*B](https://www.zhihu.com/search?q=c\*A%2B(1-c)\*B&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A"3459572262"})，通过在validation集上调参c，在测试集上将各种指标提高了1-2个百分点。***

这里我主要拿损失函数举例子。事实上还有别的情况，比如哪些参数共享哪些参数不共享，或者哪一层在前哪一层在后。我们不能说这种论文没有价值。实际上这种论文确实可能在工程上提供帮助。但是，对于找idea的读者，我们读完之后除了能说一句“woc，nb！”，好像也说不出别的什么话了。而相比之下，NeurIPS和ICLR上有不少“璞玉”型工作或者是可以借鉴的工作。

笔者私下和朋友聊天时也讨论过，我们发现好像那些特别突破性的工作的第一篇，发在NeurIPS和ICLR上的要比发在ICML上的多得多。这件事至少对于2019年之前是成立的，而2019年之前这仨会接受论文数量其实差不多，所以这件事似乎不是论文接受量造成的。笔者和朋友捋过的论文包括：GAN（NeurIPS），VAE（好像一直在Arxiv），AlexNet（NeurIPS），Transformer（NeurIPS），Attention（ICLR），GCN（ICLR），Adam（ICLR），NeuralODE（NeurIPS）。当然，这也许是因为ICML早期主要面向全体ML，而NeurIPS和ICLR主要面向[Deep Learning](https://www.zhihu.com/search?q=Deep Learning&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A"3459572262"})和神经网络。也有可能笔者和朋友过的论文不够全面，如果有朋友有其他意见的话可以在评论区补充。这个印象主要是笔者看[Ilya](https://www.zhihu.com/search?q=Ilya&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A"3459572262"})和另外几个大佬的google scholar感觉到的。

另外，感兴趣的朋友也可以看看我最近一篇[关于GAN的文章](https://zhuanlan.zhihu.com/p/691002382)。文章的最后笔者提到了GAN投稿到NeurIPS时的审稿意见。就笔者个人的审稿经历而言，如果当时GAN的审稿意见平移到ICML，那大概AC就听从低分审稿人的意见把论文拒绝了

不过，尽管同样是找idea，ICLR和NeurIPS差别有的，ICLR的论文天马行空程度要更大一些，对于初级水平的读者反而容易把握不住。所以对于初级读者，更推荐NeurIPS。

**笔者个人投稿/审稿体验：NeurIPS>=ICML>>ICLR**

笔者个人的投稿体验似乎和很多知乎上的朋友不太一样。ICLR对于笔者的主要槽点在于[openreview](https://www.zhihu.com/search?q=openreview&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A"3459572262"})。zhihu上的朋友们大多认为有openreview审稿人不敢乱来。但是笔者今年审ICLR的时候并没有这种感觉。笔者今年审稿ICLR时发现审稿人彼此可以看到个人信息，其中复数篇论文都有非常junior的审稿人，甚至可能包括没有领域内顶会发表的本科生。但是这些junior审稿人给低分的勇气和对自己判断的信心让笔者这个发过复数篇三大会的老phd感到望尘莫及，深感后生可畏。而与此同时，openreview有两个大问题：

第一，openreview上的论文在审稿阶段就对所有人开放。这就意味着，完全存在洗稿的空间。笔者和朋友合作的ICLR投稿就曾经被人洗稿然后投到某个领域内顶会（真顶会，是这个大领域内的三大会这个级别）并且还中稿了。最倒霉的是笔者因为平时基本只看三大会和自己的领域顶会，不怎么看这个洗稿论文投的领域（是我合作者的领域），所以几年后才被笔者的合作者发现。而笔者自己的论文被拒稿多次后则兜兜转转投了个CCF-C会然后不了了之。

第二，openreview上的论文审稿意见对所有人可见。因此，一旦被ICLR拒绝一次，后面recycle的时候审稿人可能就会带有偏见。笔者的另一篇ICLR投稿就遭遇过这个问题。当时笔者的论文ICLR上遇到了三个拒绝交流的审稿人。最后AC直接复制了在[rebuttal](https://www.zhihu.com/search?q=rebuttal&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A"3459572262"})里已经解答了的问题然后拒稿了。这种事情本来很常见，笔者也没有太放在心上，然后便将论文根据ICLR的审稿意见进行修改之后投稿给了ICML，结果一个ICML的审稿人直接复制了ICLR的[meta review](https://www.zhihu.com/search?q=meta review&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A"3459572262"})。。。

以上的问题让笔者对ICLR望而却步。至于ICML和NeurIPS，笔者审稿时感觉ICML的AC似乎主动性更低。笔者审稿NeurIPS时基本每篇论文的AC都会催促审稿人回复rebuttal，但是ICML似乎就不都是这样。而笔者投稿ICML时，则遇到过另一个事情。当时，AC在发现reviewer复制ICLR审稿意见之后又添加了一个新审稿人。结果这个新审稿人反复纠缠我们的理论证明，然而其提出的问题基本都是大一微积分（甚至是大一上半个学期的，比如这个审稿人看不懂[epsilon-delta](https://www.zhihu.com/search?q=epsilon-delta&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A"3459572262"})语言）里学过的知识。。。。。。最后论文被ICML再次拒稿（不过最后还是被NeurIPS捞了起来，所以笔者对NeurIPS可能也有一些倾向性的印象加分）

以上都是笔者的个人经验，其中包含不少小样本带来的幻觉。如果有朋友有不同的意见，可以分享在评论区，更好的帮助大家

## 如何读论文

1.title
2.abstract
3.introduction
4.method
5.experiments
6.conclusion
第一遍：**标题、摘要、结论。可以看一看方法和实验部分重要的图和表**。这样可以花费十几分钟时间了解到论文是否适合你的研究方向。

第二遍：确定论文值得读之后，可以快速的把整个论文过一遍，不需要知道所有的细节，**需要了解重要的图和表**，知道每一个部分在干什么，**圈出相关重要文献**。觉得文章太难，可以读引用的文献。

第三遍：提出什么问题，用什么方法来解决这个问题。实验是怎么做的。合上文章，回忆每一个部分在讲什么。



​	第二遍阅读完之后，你就对整个论文的各个部分，都有一个大概的了解，中间可以把作者引用的别人的相关文献圈出来，比如作者是在某某某的方法上进行了改进，做了哪些改进之类的。这里需要注意的是，如果你发现作者引用的这些重要文献是你没有读过的，那么你需要把它圈出来，作为你的稍后阅读清单

​	第三遍是最后一遍了，也是最详细的一遍，这里就需要自己知道每一句话在干什么，每一段在说什么
​        一边读，可以一边在脑子里面思考一些问题：
​            比如说，如果要是我来写这篇文章，我会如何组织这个结构？
​            读实验部分的时候，可以思考一下，作者是如何描述自己的实验的，你可以思考，如果换自己来做的话，能不能比作者做得更好？
​            这一遍读的时候，一定要明白作者每句话，每个字在说什么，并且最好可以脑补出它整个流程是什么样子的，似乎是自己在做实验，写论文一样。如果有困难的话，可以借助思维导图或者流程图这样的工具，把他的整个流程以可视化的形式展现出来，帮助自己理解。

## 大模型时代做科研的几点思路

<img src=".\image-20240621112318416.png" alt="image-20240621112318416" style="zoom:80%;" /><img src=".\image-20240621112435523.png" alt="image-20240621112435523" style="zoom:80%;" /><img <img src=".\image-20240621171615036.png" alt="image-20240621171615036" style="zoom:80%;" /><img src=".\image-20240621174951130.png" alt="image-20240621174951130" style="zoom:80%;" /><img src=".\image-20240621175015003.png" alt="image-20240621175015003" style="zoom:80%;" /><img src=".\image-20240621175152777.png" alt="image-20240621175152777" style="zoom:80%;" /><img src=".\image-20240621175220635.png" alt="image-20240621175220635" style="zoom:80%;" /><img src=".\image-20240621175245182.png" alt="image-20240621175245182" style="zoom:80%;" /><img src=".\image-20240621175339552.png" alt="image-20240621175339552" style="zoom:80%;" /><img src=".\image-20240621175450598.png" alt="image-20240621175450598" style="zoom:80%;" /><img src=".\image-20240621175517687.png" alt="image-20240621175517687" style="zoom:80%;" /><img src=".\image-20240621175538218.png" alt="image-20240621175538218" style="zoom:80%;" />

## NLP

### Transformer

### Bert

### GPT

## CV

### InstDisc（个体判别+memory bank）

链接

[李沐论文精读系列三：MoCo、对比学习综述（MoCov1/v2/v3、SimCLR v1/v2、DINO等） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/639246058)

意义

这篇文章提出了个体判别任务（代理任务）以及`memory bank` ，非常经典，后人给它的方法起名为InstDisc。

在有监督学习的分类模型中，如果给一张豹子图片进行分类，会发现排前几名的都是跟这张图很像的图片，而排名靠后的那些往往是跟豹子一点关系都没有的类别。

作者研究发现，让这些图片聚集在一起的原因并不是因为它们有相似的语义标签，而是因为这些照片里的物体都很相似。最后作者由此提出了个体判别任务：**把每一个instance（实例，这里就是指每一张图）都看成是一个类别（有多少图就有多少类），目标是学一种特征，把每张图片都区分开来。**

![img](https://pic4.zhimg.com/80/v2-03d50bfd7efab8b1e0ccabc2fa2a9e03_720w.webp)

模型概述

将图片经过**CNN网络**（CNN backbone）编码后得到的图片特征，使用对比学习的方式将其在特征空间中尽可能的区分开来（因为每张图都是自己的类）。

既然是对比学习，就需要正负样本。InstDisc中正样本就是就是这个图片本身（可能经过一些数据增强），负样本就是数据集里所有其它的图片，这些负样本都存储在 memory bank里。对于ImageNet有128万张图片，那么memory bank就要存储128万行，所以最后每张图都用128维特征表示（维度太高存储不了）

![在这里插入图片描述](https://img-blog.csdnimg.cn/20191111232810315.png)**前向过程**

- 𝑖𝑚𝑎𝑔𝑒→𝑅𝑒𝑠𝑁𝑒𝑡502048𝐷𝑖𝑚→128𝐷𝑖𝑚，即经过ResNet50编码得到128维的图片特征
- 论文的softmax不设置参数w。而是和Word2vec一样把特征当作参数，并创建一个叫做memory bank的堆进行存储所有单词的128维特征，**每次通过loss更新**。这样训练和测试通过存储的memory bank同使用一个度量空间。
- 论文取batch_size=256，则每个batch有256个正样本，然后从 memory bank 里**随机地抽取4096个负样本**。根据正负样本计算对比学习目标函数`NCELoss`。然后根据**loss更新backbone和memory bank**（把 mini batch里的数据样本所对应的那些特征，在 memory bank 里更换掉，这样无论是训练还是测试就都来自于一个度量空间了）。
- 测试时，使用KNN(K Nearest Neighbors)进行分类
  我们获得了训练好的模型后，对于一张图片提取他的特征，将他和memorybank中所有的存储图片特征计算相似度，然后采用k近邻算法，返回最相似的k张图片。最后根据相似度权重投票，得到其类别c。

**LOSS：NCE** ,详见MOCO

结论

`Inst Disc` 这篇论文也是一个里程碑式的工作：它不仅提出了**个体判别**这个代理任务，而且用这个代理任务和 **NCE loss**做对比学习，从而取得了不错的无监督表征学习的结果。同时它还提出了用别的数据结构存储这种大量的负样本，以及如何对特征进行动量的更新，所以真的是对后来对比学习的工作起到了至关重要的推进作用。



### Inva Spread（simCLR前身）

链接

[李沐论文精读系列三：MoCo、对比学习综述（MoCov1/v2/v3、SimCLR v1/v2、DINO等） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/639246058)

意义

`nva Spread`是一种**端到端**（也就是模型整体调整）的训练方式，直接训练特征本身，无需额外的数据结构（比如上文的memory bank），提升了效率和准确度。作者还使用了新的采样方式，降低了计算复杂度。

`Inva Spread`可以看做是`SimCLR`的前身，但由于数据增强策略不足以及负样本数量太少，也没有`SimCLR`提出的mlp projector ，使得最终的训练效果不好，没有太大的影响力。

> `Inva Spread`的作者太穷，没有TPU，只能选择`batch_size=256`来训练。这样每次迭代的负样本只有255*2个，数量太少，对比学习的效果不够好（也就是在MOCO中说过的字典太小）。而`SimCLR`的作者来自谷歌，可以使用大量的TPU，最终训练的`batch_size=8192`，足以达到不错的训练效果。





模型概述

作者认为提升效率的方法就是直接优化特征本身，拒绝额外的数据结构，也就是用端到端的方式。但这样做会有两种阻碍：一是如果抛弃通过参数w来学习，也不采用memory bank利用时间差更新而让特征自己乘自己，就会使得网络得不到训练。二是不采用NCE等方式，训练的复杂度就太大了。

作者认为，相似图片通过编码器以后，它的特征应该很类似，不同的图片，它的特征出来就应该不类似，这就是题目中说的invariant和 spreading 。于是作者提出的孪生神经网络结构，有效地解决了这两个问题：

![img](https://pic4.zhimg.com/80/v2-fc62a83132992e017c7cb3ddfd513f53_720w.webp)

- 设batch_size=256，即输入256张图片。经过数据增强，又得到了256张增强后的图片。这样每个batch有256个正样本和（256-1）*2个负样本。
- 根据正负样本计算loss（NCE loss 的一个变体），然后更新网络参数。
- 训练结果表示在最后特征空间中，就是绿色的两个球靠近，和所有别的球远离；其余类似。



### CPC（生成式代理任务）

**链接**

[李沐论文精读系列三：MoCo、对比学习综述（MoCov1/v2/v3、SimCLR v1/v2、DINO等） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/639246058)



**意义**

之前的几篇代理任务都是个体判别任务，那么自然也有**生成式的代理任务**，CPC就是其中之一，它**使用预测的代理任务去做对比学习**。CPC是一个**通用结构**，其输入是一个序列，可以是图片（不同patch）、文字或者音频、视频等等。



**模型概述**

- 对于一个输入序列x，当前时刻为t。t时刻输入经过编码器𝑔𝑒𝑛𝑐得到编码特征𝑧𝑡。

- 𝑧𝑡经过自回归模型𝑔𝑎𝑟（比如RNN/LSTM）得到输出𝑐𝑡（context representation，上下文特征，因为含有之前时刻的信息）。如果𝑐𝑡表示的足够好，包含之前所有时刻的信息，那么应该可以用来预测未来时刻的输出特征𝑧𝑡+𝑖。

- 对比学习的正样本就是未来的输入通过编码器以后得到的未来时刻的特征输出，负样本就是任意输入通过这个编码器得到输出。（感觉这里负样本都没说明白，老师一句话带过。别的博文说负样本是其它的输入序列，比如另一段音频的编码输出，但如果这样的话，前面一大段讲𝑐𝑡预测𝑧𝑡+𝑖有啥意义。还有很重要的互信息也没讲，这里先放着了）

  ![img](https://pic4.zhimg.com/80/v2-88c5a0c5b3ca3fef114638ce0e33a5a7_720w.webp)

  





### CMC（多视角正样本）

链接

[李沐论文精读系列三：MoCo、对比学习综述（MoCov1/v2/v3、SimCLR v1/v2、DINO等） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/639246058)

意义

`CMC`使用**一个物体的多个视角来作为正样本**。这个思想来自于人类对世界的感受、观察。
在摘要中，作者说人类观察这个世界是通过很多个不同视角的传感器，比如说眼睛或者耳朵，来给大脑提供不同的信号。每一个视角都是带有噪声的，而且有可能是不完整的。但是最重要的那些信息，比如物理性质，几何形状以及语义信息，在所有的这些视角中间共享（即互信息）。例如一只狗可以被看到、听到、感受到。

基于此，作者认为**一个强大的特征，应该具有视觉不变性**（不论是看到还是听到，都应该能判断出那是一只狗）。所以CMC目的，**就是最大化同一个场景不同视角的互信息，并且可以扩展到任意数量的未知视角，且视角越多效果越好。**



`CMC`正负样本确定的方式由个体升级成了个体的不同的视角（如色彩模型）。它同样使用了NCE，但将其扩展以适应不同的视角。`CMC`采用多视角对比学习，证明了对比学习的灵活性，也同时证明了多视角多模态的可行性，为之后的CLIP工作（图文配对的多模态对比学习）打下了基础。

但是本文也有一个局限，即处理不同的视角（模态）时，可能需要不同的编码器，因为不同的输入特点不一样。 如果每个视角都有一个编码器，那么训练的成本就有点高（比如在CLIP里，文本编码器是BERT，图片编码器是ResNet或者ViT）。

所以这也是现在`Transformer`最吸引人的地方，这个结构可以同时处理文本和图片，那么就可以用一个解码器处理两种模态，而不用做针对每种数据去做特有的改进。今年在ICLR上发表的[MA-CLIP](https://link.zhihu.com/?target=https%3A//paperswithcode.com/paper/ma-clip-towards-modality-agnostic-contrastive)，就是用一个`Transformer`去同时处理两个输入模态，效果反而更好。





模型概述

![img](https://pic2.zhimg.com/80/v2-620ff8c6a998947916efbf055ec08239_720w.webp)

`CMC`选用 `NYU RGBD` 数据集进行 训练。数据集中每张图有4个视角（view）：原始的图像、原图对应的深度信息（每个物体离观察者到底有多远）、SwAV ace normal以及原图的分割图像。

在CMC中，一张图的四个视角就是互为正样本，因为其代表的是同一个东西；其它的图片就是负样本。在上图表示，就是特征空间中四个绿色的点互相靠近，而都和红色的点远离。



### MOCO(队列存储+动量编码器)



链接

[李沐论文精读系列三：MoCo、对比学习综述（MoCov1/v2/v3、SimCLR v1/v2、DINO等） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/639246058)



意义

它是用一种**对比学习的方式进行无监督训练**的模型。`MoCo`是第一个在很多主流的机器视觉领域上（比如分类、检测、分割、人体关键点检测等），都超越了有监督预训练模型的无监督模型，从某种程度上证明了无监督学习在机器视觉领域，也能达到很好的效果。

`MoCo`虽然是基于对比学习的，但是本文是从另外一个角度来看对比学习，即把对比学习看作是一个**字典查询任务**。



模型概述

将对比学习中提到的𝑥𝑖当做是`query`，其它包括𝑥𝑖1、𝑥𝑖2这些图片都是字典中的`key`。我们每次判断正负样本，就是看字典中的这些key和query是否相似，而这些`key`都是通过`encoder`来更新的。

具体来说，我们构建了一个**动态的字典**，这个字典有两个特性：**队列**特性和`moving-averaged encoder`

> GPT和BERT已经证明了无监督的表征学习在NLP领域是非常成功的，但是在视觉领域，无监督学习效果差很多，作者认为可能是二者的原始信号空间不同。

- 在**NLP**任务中，原始信号空间是离散的（都是一些含有不同语义的单词或者词根），信号本来就拉得比较开，容易建立tokenize（将单词映射成向量）的字典。这样无监督学习容易建模，且模型容易优化。
- CV中，视觉信号都是在一个连续且高维的空间里，不像单词那样信息和语义浓缩的那么好，不够简洁，这样就不容易建立一个这样的字典，也就不容易进行无监督学习。

有一些无监督学习方法表现不错，但是都可以归结为**建立动态字典**。
如果将对比学习讲到的所有样本都构建到一个字典中，**字典的key就是各个样本，字典的value就是编码之后的特征**（后面直接以𝑘0表示第一个样本的编码特征）。我们先编码好锚点的特征，当做query；其它所有样本特征当做字典中不同的key，那么那对比学习就转化成为了一个字典查询的问题了。
如下图所示，我们训练一些编码器，再根据q去字典中查找key。查找的目的，就是让已经编码好的特征q，和与它匹配的特征key（其实就是正样本𝑥𝑖2的特征）最相似；与其它不匹配的特征不相似

![img](https://pic2.zhimg.com/80/v2-3408289e7fbb640198043ffc95065329_720w.webp)

一个好的字典应该：**1.字典非常大**，**2.特征一致性非常好**，从而便于进行对比学习。

- **字典足够大**

- - 字典越大，`key`越多，所能表示的视觉信息、视觉特征就越丰富 ，这样拿`query`去做对比学习的时候，才越能学到图片的特征。
  - 反之，如果字典很小，模型很容易通过学习一些捷径来区分正负样本，这样在碰到大量的真实数据时，泛化就会特别差（我的理解是，字典中只有猫和狗，狗都是黑色，猫都是黄色。模型简单的判断图片中物体是否是黄色，来区分猫和狗，而不是真的学到了猫和狗的特征）

- **编码的特征尽量保持一致性**
  **字典里的`key`都应该用相同或者说相似的编码器去编码得到**，否则模型在查找`query`时，可以简单的通过找到和它使用相同或者相似编码器的`key`，而不是真的和它含有相同语义信息的key（变相引入两一个捷径）。
  以前的对比学习，都至少被上述所说的两个方面中的一个所限制（要么一致性不好，要么字典不够大）。本文最大的贡献，就是使用队列以及动量编码器来进行对比学习，解决了这个问题。具体来说：

- `key`（编码特征）并**不需要梯度更新**，而是通过更新编码器，新的编码器使输出的`key`更新。（每个batch更新encoder,则新一批的key由新encoder生成）

- `queue` ：**整个队列里面的元素都是字典(即每个batch)**，队首输入当前batch的编码特征，队尾弹出最旧的batch特征。**每次移除的是最老的那些key**，从一致性的角度来说 ，有利于对比学习。

- - 用队列的好处是可以重复使用那些已经编码好的key，而这些key是从之前的那些mini-batch中得到的。

  - 用队列结构，就可以把的mini_batch的大小和队列的大小直接分开了，所以最后这个队列的大小，也就是字典的大小可以设的非常大，因为它大部分的元素都不是每个iteration都需要更新的。

  - 在字典里计算loss而不是整个数据集上计算loss，使用队列的数据结构，可以让维护这个字典的计算开销非常小。

    

- `momentum encoder`：

- - 如果只有当前batch的key是从当前的编码器得到特征，其它的key都是另外时刻的编码器输出的特征，这样就无法保证字典中key的一致性。所以作者又提出了动量编码器
  - 动量编码器，即编码器参数的更新方式就是𝑦𝑡=𝑚⋅𝑦𝑡−1+(1−𝑚)⋅𝑥𝑡（`MoCo`中`m=0.999`）。
  - 初始化的编码器来自于`query`的编码器，之后每次更新，**只有`1‰`的参数会从`query`的编码器参数里拿过来更新**，所以这个编码器参数更新的非常缓慢。从而保证了字典中所有的key都是由相似的编码器抽取得到的，尽最大可能地保持了他们的一致性。（直接更新编码器k的所有参数，会导致编码器更新过快，降低了这个队列中所有key的特征的一致性）



- 动态字典：字典中的key都是随机取样的，而且key的编码器在训练的过程中也是在不停的改变。





**损失函数**：infoNCE

![img](https://pic2.zhimg.com/80/v2-f3fbc4e0dfac22445e7c8df8aa61f70d_720w.webp)

<img src=".\image-20240422155211503.png" alt="image-20240422155211503" style="zoom:67%;" />

式子中，`τ`是一个超参数。如果去掉`τ`，整个式子其实就是**交叉熵损失函数**（`cross entropy loss` ），在后面的伪代码中，也是基于cross entropy loss实现。

- 分子表示q和正样本做计算，分母其实是**k个负样本*（k:负样本个数）*上做累加和，因为是从0到k，所以是k+1个样本，也就指的是字典里所有的key。
- 直接计算复杂度太大： `MoCo`使用 `instance discrimination`（**个体判别**）作为代理任务，那么光是ImageNet数据集，就有128万个类别，直接计算，复杂度会非常高，难以训练（128万类的softmax）。
- `NCE loss`（`noise contrastive estimation` ）：**将超级多分类转为二分类**——数据类别data sample和噪声类别noisy sample。这样**解决了类别多**的问题。

> `estimation`：近似的意思。为了**降低计算复杂度**，不是在每次迭代时遍历整个数据集128万张负样本，而是只从数据集中选一些负样本来计算loss（也就是选队列字典中的6万多个负样本(即每个batch)），相当于一种近似。MoCo`一直强调的希望字典足够大，因为越大的字典，越能够提供更好的近似。

`InfoNCE`:NCE的一个简单的变体.

- 作者认为如果只把问题看作是一个二分类（只有数据样本和噪声样本）的话，可能对模型学习不是很友好，毕竟在那么多的噪声样本中，大家很有可能不是一个类，所以还是把它看成一个多分类的问题比较合理。
- 公式中的q * k，其实就相当于是logit，也可以类比为softmax中的z。
- `τ`：**温度系数**，一个超参数，用来控制分布的形状 。τ越大，分布中的数值越小，经过exp之后就更小了，分布就会变得更平滑，相当于对比损失对所有的负样本都一视同仁，导致学习的模型没有轻重。τ越小，分布更集中，模型只关注那些特别困难的样本，其实那些负样本很有可能是潜在的正样本，如果模型过度地关注这些特别困难的负样本，会导致模型很难收敛，或者学好的特征不好去泛化。



**input:**在代理任务不一样的时候，输入𝑥𝑞和𝑥𝑘既可以是图片，也可以是图片块（CPC），或者是含有上下文的一系列的图片块。

**编码器：**`query`的编码器和`key`的编码器既可以是相同的（模型的架构一样，参数完全共享，比如Inva Spread），或者说它们的参数是部分共享的，也可以是彻底不一样的两个网络（CMC，多视角多编码器）。

**伪代码：**

下面是论文中作者给出的伪代码，其中：

- fq、fk分别是query和key的编码器
- queue这个队列指的是字典，里面一共有k个key，所以它的维度是`c*k`，c指的是每个特征的维度（`c=128`）
- `m`是动量，`t`是`InfoNCE`里面的超参数`τ`
- aug表示数据增强

1. 初始化编码器`fq`，并将其参数赋值给编码器`f_k`
2. 从data loader里拿一个batch的数据（n=bacth_size=256，n是采样数）
3. 通过数据增强得到正样本对`x_q`和`x_k`，然后通过各自的编码器得到特征`q`和特征`k`（大小都是`N*C`）。key不需要梯度回传，所以用.detach() 去掉梯度信息。
4. 计算N张图片的自己与自己的增强图的特征的匹配度
   `q 、k`之间计算`logit`（正样本），也就是之前公式1中算InfoNCE loss的时候的分子𝑞∗𝑘+，其特征维度就变成了`n * 1`（`256，1`）。
5. 计算N张图片与队列中的K张图的特征的匹配度
   `q、queue`拿出来计算，得到InfoNCE的分母，也就得到了负样本的logit，维度是`n*k`（`256*65536`，MoCo中，字典大小为65536）
6. 将正负样本logit进行`cat`拼接
7. 通过交叉熵损失函数实现loss计算。具体的，设置一个全0向量作为`ground truth`来进行计算。
   因为按照作者的这种实现方式，所有的正样本永远都是在logit的第一个位置上，也就是位置0，所以对于正样本来说，如果找对了那个key，在分类任务中得到的正确的类别就是类别0，所以巧妙地使用了这种方式创建了一个ground truth，从而计算出了对比学习的loss
8. 根据loss进行梯度回传，更新编码器`fq`
9. 动量更新编码器`f_k`
10. 更新队列（队首压入新的batch编码的key，队尾弹出最旧的key）

```python
f_k.params = f_q.params # 初始化
for x in loader: # 输入一个图像序列x，包含N张图，没有标签
    x_q = aug(x) # 用于查询的图（数据增强得到）
    x_k = aug(x) # 模板图（数据增强得到），自监督就体现在这里，只有图x和x的数据增强才被归为一类
    q = f_q.forward(x_q) # 提取查询特征，输出NxC
    k = f_k.forward(x_k) # 提取模板特征，输出NxC
    # 不使用梯度更新f_k的参数，这是因为文章假设用于提取模板的表示应该是稳定的，不应立即更新
    k = k.detach() 
    # 这里bmm是分批矩阵乘法
    l_pos = bmm(q.view(N,1,C), k.view(N,C,1)) # 输出Nx1，也就是自己与自己的增强图的特征的匹配度
    l_neg = mm(q.view(N,C), queue.view(C,K)) # 输出Nxk，自己与上一批次所有图的匹配度（全不匹配）
    logits = cat([l_pos, l_neg], dim=1) # 输出Nx(1+k)
    labels = zeros(N)
    # NCE损失函数，就是为了保证自己与自己衍生的匹配度输出越大越好，否则越小越好
    loss = CrossEntropyLoss(logits/t, labels) 
    loss.backward()
    update(f_q.params) # f_q使用梯度立即更新
    # 由于假设模板特征的表示方法是稳定的，因此它更新得更慢，这里使用动量法更新，相当于做了个滤波。
    f_k.params = m*f_k.params+(1-m)*f_q.params 
    enqueue(queue, k) # 为了生成反例，所以引入了队列
    dequeue(queue)
```



相关工作

**SimCLR**：端到端的学习方式

字典大小和mini_batch大小一致，但是现在一般是存不了太大的batch的，而且太大的batch难以优化，处理不好的话，不容易收敛，所以最终模型效果没那么好。

**memory bank （InstDisc模型）**

InstDisc有一个明显的问题，就是特征的一致性非常差。并且对于一个拥有亿级图片规模的数据，存储所有的特征就需要几十G甚至上百G的内存了，所以memory bank的扩展性不如MoCo好。

`MoCo` 的主要贡献就是把之前对比学习的一些方法都归纳总结成了一个**字典查询**的问题，并提出了**队列存储和动量编码器**。前者解决字典太大不好存储和训练的问题，后者解决了字典特征不一致的问题；从而形成一个又大又一致的字典，能帮助模型更好的进行对比学习。





### SimCLR（Projector head）

链接

[李沐论文精读系列三：MoCo、对比学习综述（MoCov1/v2/v3、SimCLR v1/v2、DINO等） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/639246058)

意义

- `SimCLR`可以被认为是`inva spread`的改进工作。其最大创新点就是在图片编码特征之后加了一个`projector`，但就这么简简单单的一层mlp，能让模型在ImageNet 分类任务上直接涨了近10个点。
- 使用更优的数据增强技术
- 使用更大的batch_size（256→8192）
  `SimCLR`的前两点贡献，添加`projector`和使用的数据增强，在之后的对比学习模型（MoCov2、BYOL）中也一直被沿用。

模型概述

- 图片x经过不同的数据增强得到不同的图片𝑥𝑖~和𝑥𝑗~，这两个就是互为正样本；同一个mini_batch里面的其它图片都是负样本，这点和`inva spread`一样。
- 正负样本经过同一个编码器𝑓(⋅)（权重共享）得到编码特征ℎ𝑖,ℎ𝑗。比如encoder选ResNet50，就是输出2048维特征。
- ℎ𝑖,ℎ𝑗经过同一个projector即图中的𝑔(⋅)（其实就是一个mlp层，**全连接层+relu激活**，具体见下图）得到最终的对比学习特征𝑧𝑖,𝑧𝑗（**128维**，消融实验证明128就够了）。是**最大创新点**
- 对比学习的训练目标就是使正样本特征更相似（同一张图片得到的 𝑧𝑖,𝑧𝑗），而负样本的特征不相似。
- 选用的损失函数是 `NT-Xent loss`（the normalized temperature-scaled cross entropy loss）。normalized是指在特征后面进行了 L2 归一化，temperature-scaled 就是说在 loss 里加了个τ，所以和`infoNCE loss`也是非常接近的。
- **projector在训练时才使用，推理时直接去掉，只用特征h特征。，也就是下游任务只用f不用g**

![img](https://pic4.zhimg.com/80/v2-cbc36c4b3d3ceb823fe993ef04118b87_720w.webp)

![img](https://pic4.zhimg.com/80/v2-b41624f71214a7f89ee6d635c3e61c53_720w.webp)

作者试验了10种数据增强，比如随机裁剪、变换色彩、翻转、Cutout、高斯噪声、blur噪声等等；并做了如下的消融试验（除了最后一列，余下是两两组合）。最后发现**随机的裁剪和随机色彩变换组合**效果最好。

![img](https://pic3.zhimg.com/80/v2-acdf14dd4df549d4293fbac811c2ea66_720w.webp)

![img](https://pic2.zhimg.com/80/v2-30f4913447ae729bb27c817d0078d97d_720w.webp)



### MOCOV2(引入Projection head)

链接

[李沐论文精读系列三：MoCo、对比学习综述（MoCov1/v2/v3、SimCLR v1/v2、DINO等） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/639246058)

意义

`MoCov2`主要是借鉴了`SimCLR`而做的优化，比如引入了**mlp projection head以及使用更多的数据增强**。`MoCov2`刷新了ImageNet 上的最好成绩，比之前的`MoCo`以及最新的`SimCLR`都高很多 。其上传的日期是3月9日，离`SimCLR`的发布还不到一个月。

直到现在，做一些一些对比学习的尝试工作时，还是会用`MoCov2`当基线模型，因为**训练快、效果稳，而且下游任务迁移的好**。

模型概述



![img](https://pic3.zhimg.com/80/v2-021d7cf1cb1b57fd9e82b834ae3606f6_720w.webp)

`MoCov2`对比`MoCo`主要有4个改动：

- 添加 projection head
- 使用更多的数据增强
- 训练时使用cosine的learning rate schedule
- 训练的epoch，从200增加到800

![img](https://pic1.zhimg.com/80/v2-150b67bb7630521d7fd06f66e87df2cc_720w.webp)



上图列出了模型效果对比图。

- MLP表示增加projection head，可以看到只增加这一点，就提了近6个点
- aug+和cos分别表示上面提到的数据增强和cosine schedule（余弦退火）
- 灰色行是有监督baseline模型





### SimCLRv2（引入动量编码器）

**链接**

[李沐论文精读系列三：MoCo、对比学习综述（MoCov1/v2/v3、SimCLR v1/v2、DINO等） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/639246058)



**意义**

`SimCLRv2`的主要思想体现在其标题里，即大的自监督模型很适合做半监督学习。在摘要中，作者提出：一种从少量带标签数据+大量无标签数据中进行学习的方案是：无监督预训练（必须是大模型）+有监督微调，这种半监督学习的方案在ImageNet上极为有效，具体的可以总结为三步：

1. `pretrain`：在无标签数据上无监督训练（SimCLR对比学习）一个Big ResNet模型（模型大小至关重要）以学习广义视觉特征表达。
2. `fine-tune`：在少量有标签数据上通过进行有监督的微调
3. `distill`：用微调后的模型作为`teacher`模型，在之前的无标签数据集上生成**伪标签**，然后训练一个`student`模型进行自监督训练（蒸馏阶段采用KL散度）。

> 微调后，作者发现：模型的任务已知预测属性可以进一步改善并蒸馏到一个更小的网络中。为此，作者对无标签数据进行了二次利用以促使学生网络尽可能的模拟老师网络的标签预测性能，且蒸馏阶段采用伪标签方式且不会造成额外的更多复杂度。
> 整个框架其实也是受启发于google的另外一篇工作 [Noisy Student](https://link.zhihu.com/?target=https%3A//paperswithcode.com/method/noisy-student)。`noisy student`就是在`ImageNet`数据集上先训练了一个 teacher 模型，然后在`JFT 300M`那个数据集上生成了很多的伪标签，最后一起训练了一个student模型，其精度为88，霸榜ImageNet快一年。

`SimCLRv2`在仅仅采用`1%/10%`有标签数据时，backbone使用ResNet50就取得了`73.9%/77.5%`的top-1精度。



**模型概述**

`SimCLRv2`相比`SimCLRv1`有三处改进：

- 大模型：backbone从`ResNet50`替换为`ResNet152+SK net` （selective kernels）
- 加深`protection head` ：**从一层加到两层**。
  protection head在SimCLRv1和MOCOv2中都被证明很有用，所以作者考虑多加几层。最后发现加到**两层**效果就够了
- 引入了**动量编码器**：使用了类似`MOCO`的动量编码器，效果提升了一个点。
  作者解释是，`SimCLR`模型的 batch_size已经够大了，也就是字典的大小和字典里特征一致性，SimCLR v2 都已经做的很好了。换成`MOCO`这种队列结构的动量编码器，虽然可训练的负样本更多，但是提升没有那么明显了。
- **微调**
  - `SimCLRv1`在微调时，是去掉𝑔(⋅)（projector层），只保留编码器𝑓(⋅)进行微调，即𝑓𝑡𝑎𝑠𝑘(𝑥𝑖)=𝑊𝑡𝑎𝑠𝑘𝑓(𝑥𝑖)；
  - `SimCLRv2`在微调时，是保留𝑔(⋅)的第一层 ，即𝑓𝑡𝑎𝑠𝑘(𝑥𝑖)=𝑊𝑡𝑎𝑠𝑘⋅𝜎(𝑊𝑀𝐿𝑃⋅𝑓(𝑥𝑖))

![img](https://pic1.zhimg.com/80/v2-14836db9959217f6cd46a5002a20b384_720w.webp)







### SwAV(负样本->聚类中心)

**链接**

[李沐论文精读系列三：MoCo、对比学习综述（MoCov1/v2/v3、SimCLR v1/v2、DINO等） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/639246058)



**意义**

`SwAV`即swap assignment view的缩写，意思就是一张图片不同视角的特征可以互相预测，因为来自同一张图片的不同视角特征按道理来说都是相似的。具体的做法，就是**将聚类加入到了对比学习**中。（将匹配问题转为预测问题，预测时借助簇类中心，而不是和所有负样本直接进行对比）。可以认为SwAV是承上启下。

`SwAV`也提出了一种**新的数据增强方法`Multi-crop`（多次裁剪）**。这个想法非常简单但是确实有用，**也是真正提点的技术**，在后面的很多对比学习中也被一直沿用。



**模型概述**

作者认为之前的对比学习，直接拿所有图片的编码特征去做对比有点原始而且计算量太大，因为所有的图片都是自己的类。作者考虑，能不能不做近似，能不能借助一些先验信息，一些更简洁的东西比进行对比，而不是和所有负样本直接进行对比。由此作者提出了可以**和聚类中心特征进行对比**（128万张图片被聚成3000个簇类中心`cluster center`）。

> 比如MoCo在ImageNet上训练那就有128万类，即使在计算loss时取近似，只是取队列编码器里的作为负样本，那负样本也有6万多个。
>
> 之前的一些聚类方法常常将`ImageNet`数据集聚成`3000`个簇类中心。

作者选择聚类这个想法有两个原因。首先，聚类方法也是一种无监督的特征表示学习方式，其目标也是希望相似的物体聚在一起，不相似的物体尽量互相远离，这个思想与做法和对比学习都比较接近；第二就是论文一作之前是做聚类的，比如deep cluster，也是一篇很好的无监督学习论文。

![img](https://pic4.zhimg.com/80/v2-b46c2da3e526693e68ae76f6b6cd2953_720w.webp)

- 左图：普通的对比学习方法。
  同一张图片，做两次数据增强得到𝑥1,𝑥2（正样本），然后所有的样本通过一个图片编码器（比如ResNet50等等，也可以加几层mlp之类的，这里没有具体说明）输出编码特征𝑧1,𝑧2，然后在编码特征z上去做对比学习。

- 右图：`SwAV`的做法

- - 每个batch输入数据为 𝑥∈𝑅𝑁∗𝐶∗𝐻∗𝑊， 分别经过不同的Aug， 得到 𝑥1,𝑥2
  - 将𝑥1,𝑥2 输入编码器中，得到编码特征𝑧1,𝑧2∈𝑅𝑁∗𝑑
  - **已知K个聚类中心prototypes** {𝑐1,...,𝑐𝐾}，表示为𝐶∈𝑅𝐾∗𝑑。**将编码特征与聚类中心计算相似度**，得到相似度矩阵𝑄∈𝑅𝐾∗𝑁，这样算完又获得了一个新的表示 𝑞1,𝑞2（Codes）
    理想情况下，样本与自己的类簇中心相似度为1，与其他的为0，类似于有监督任务中的one-hot label。不过作者发现soft label效果会好一些。
  - **理论上同一张图片不同view（比如Augment）所产生的 z 和 q 可以相互预测**。也就是说，如果拿𝑧1这个特征去跟c去做点乘，按道理来说也是可以去预测𝑞2；反之亦然。所以说点乘之后的结果就是预测，而ground truth就是之前按照clustering分类而得到的q1和q2。作者由此定义了新的loss：

𝐿(𝑧𝑡,𝑧𝑠)=𝑙(𝑧𝑡,𝑞𝑠)+𝑙(𝑧𝑠,𝑞𝑡)

其中

𝑙(𝑧𝑡,𝑞𝑠)=−∑𝑘𝑞𝑠(𝑘)log⁡𝑔𝑝𝑡(𝑘)

𝑝𝑡=𝑒𝑥𝑝(𝑧𝑡𝑇𝑐𝑘/𝜏)∑𝑘′𝑒𝑥𝑝(𝑧𝑡𝑇𝑐𝑘//𝜏)

- 通过这种Swapped prediction，也就是**换位预测**的方法，SwAV可以对模型进行训练 。

**用聚类做对比学习的好处到底有哪些？**

1. 减少计算量
   聚类可以将需要对比的样本数大大减少。比如之前的对比学习，需要去和成千上万的负样本进行对比，即便如此也只是算一个近似。而如果只是跟聚类中心做对比，则只需要最多3,000个聚类中心。因为ImageNet也就1,000类，COCO才80类，所以说 3,000个聚类中心就足够用了。
2. 聚类对比更加合理
   随机抽取的负样本中，有的可能还是正样本（本身就相似的图片），而且有的时候抽出来的负样本类别也不均衡。但是聚类中心是有明确的语意含义的，自然更加有效，这也是SwAV的基本思想。

**Multi-crop增强**

`SwAV`也提出了一种新的数据增强方法`Multi-crop`（多次裁剪）。

- 之前的那些对比的学习方法都是用的两个crop。比如输入一张图片，先把它resize 到256×256，然后随机crop两个224×224的图片当成 正样本对𝑥1,𝑥2
- `Multi-crop`：一张图片经过两个160×160的crop，和四个96×96的crop得到6个正样本。
  **前两个crop争取学习到全局特征，后四个crop争取学到局部特征**。因为之前crop尺寸是224，明显非常大，学习到的基本都是全局特征。如果可以学习局部特征，就更容易关注到局部的物体了。但是为了保持和原来计算量差不多，所以原先的尺寸从224降到了160。

这个想法非常简单但是确实有用，在后面的很多对比学习中也被一直沿用。





### CPCv2（融合tricks）

CPCv2其实也是融合了很多的技巧，它用了更大的模型、用了更大的图像块、做了更多方向上的预测任务，把batch norm 换成了 layer norm，而使用了更多的数据增强，所以这一系列操作下来，CPC v2直接就把CPC v1之前在 ImageNet 上40多的准确率一下就拔到70多。



### **BYOL**（不用负样本的对比学习）

**链接**

**意义**

`BYOL`就是论文标题`Boostrap Your Own Latent`的缩写。**Latent、Hidden、Feature、Embedding其实都是特征的意思**，就是各种花里胡哨的用法而已；Boostrap就是类似自我改造的意思。

`BYOL`使用了一种新的对比学习方法（A New approach），即**没有引入任何形式的负样本**，而是用图片的编码特征（梯度更新）去预测自己的编码特征（动量更新），模型就这样训练起来了。（相当于用一个视角的特征取预测另一个视角的特征，将匹配转为预测问题）

这种训练方式类似`SwAV`，但是这次连簇类中心都没了，所以听起来有点不可思议。后来还有一篇博文分析了`BYOL`，认为其实是在使用BacthNorm时引入了隐式的负样本进行对比学习（BN相当于提前看了一整个batch的内容）。BYOL作者一听不高兴了，这样不是说明我的工作大大折扣了吗，所以立马写了一篇技术实验论文驳斥了这个说法，证明了对比学习完全不使用负样本是可行的（后面会详细介绍）。



**模型概述**

![img](https://pic4.zhimg.com/80/v2-7d13797524edef7239f9c79cb05621ef_720w.webp)

![img](https://pic2.zhimg.com/80/v2-de94ebcc90a9daebd978e8080e91d575_720w.webp)

sg: stop gradient,即不计算梯度

前向过程：

- 输入x经过两次不同的Aug得到𝑣,𝑣′。

- 编码特征

- - 上面的online分支𝑣经过编码器𝑓𝜃得到编码特征𝑦𝜃，𝑓𝜃是梯度更新
  - 下面的target分支𝑣′经过编码器𝑓𝜉得到编码特征𝑦𝜉′，𝑓𝜉和𝑓𝜃模型结构一样，但用的是动量更新的方式。也就是说， 𝑓𝜉引入了MoCo中的**动量编码器**，其参数和𝑓𝜃不同，但是结构一样。
  - 如果这两个编码器都是ResNet50，则输出特征是2048维



- projection head

- - 使用类似`SimCLR`中一样的projection head 𝑔𝜉和𝑔𝜃（也是一个MLP，`BYOL`中也把这个结构叫`predictor`），将特征降到256维，得到特征𝑧𝜃,𝑧𝜉′。
  - 𝑔𝜉和𝑔𝜃分别是梯度更新和动量更新，但二者结构一样。



- 对比预测

- - 在 SimCLR中，是在𝑧𝜃,𝑧𝜉′之间做maximum agreement，即使不同增强后再编码和MLP映射后的特征尽可能的接近
  - 在SwAV中，是将𝑦𝜃,𝑦𝜉′分别和K个簇类中心c计算相似度得到𝑞𝜃,𝑞𝜉，然后互相预测作对比学习（𝑦𝜃和相似度矩阵点乘的结果去预测𝑞𝜉，反之亦然）
  - `BYOL`中，上分支使用`prediction head`（也是`predictor`结构）将𝑧𝜃映射为𝑞𝜃(𝑧𝜃)，然后**用𝑞𝜃(𝑧𝜃)去预测𝑠𝑔(𝑧𝜉)′来进行对比学习**，其中sg表示`stop-gradient`，因为下分支编码器是动量更新。
  - 损失函数是`MSELoss`，即直接计算预测特征𝑞𝜃(𝑧𝜃)和标签𝑠𝑔(𝑧𝜉)′这两个向量之间的mse。



**推理**：

**当训练完成只留下编码器𝑦𝜃用于下游任务**，剩下所有的东西都被拿掉了。然后用这个编码器编码图片，输出维特征去做下游任务的推理。

对比：

- 按过程来看，`BYOL`就是将上分支输入经过一个梯度更新的编码器和两个`predictor`得到的𝑞𝜃(𝑧𝜃)，去预测下分输入经过一个动量更新的编码器和一个`predictor`得到的𝑠𝑔(𝑧𝜉)′。
- 所以可以看出`BYOL`使用了MoCo的动量编码器、SimCLR的`projection head`以及预测任务，但是没有负样本，目标函数也不一样。通过自己预测自己就学起来了。
- `BYOL`的两个分支叫online和target，其实就相当于`MoCo`中的query和key分支。

**学习机制分析**

为何不使用负样本这么重要

- 在对比学习中，负样本是一个约束。如果在算目标函数的时候只有正样本，也就是让所有相似的物体的特征也尽可能的相似，此时就有一个很明显的捷径：模型输出恒等于输入，对比学习的loss永远都是0，模型直接就躺平（也叫模型坍塌`model collapse`，表示模型根本就没有在学习）。
- 只有加上负样本这个约束，即不相似的物体也要有不相似的特征，这样模型才会继续学习，否则负样本的loss就无穷大了。所以加入负样本能防止模型学到捷径，是必须的。
- `BYOL`之所以神奇就是它没有用负样本，正样本自己跟自己学最后在ImageNet上也达到了74.3的top-1准确率，也是相当高了。



`BYOL`被认为是使用了**隐式负样本**

`BYOL`发布到arxiv之后，在reddit、twitter、知乎全都引起了剧烈的讨论，因为大家都觉得很不可思议；不用负样本，只是自己预测自己，模型的学习怎么能不坍塌。由此引出了一篇博文[《Understanding self-supervised and contrastive learning with "Bootstrap Your Own Latent" (BYOL)》](https://link.zhihu.com/?target=https%3A//generallyintelligent.ai/blog/2020-08-24-understanding-self-supervised-contrastive-learning/)。
这篇博文的作者在复现`BYOL`时遗漏了一个小细节，即借用了 `MoCov2`的`projection head`导致`projection head`中**没有**加`batch norm`，最终模型坍塌。认为在`Projector`层使用BN之后，是计算了整个batch的均值和方差，这意味着是有信息泄露的（MoCo使用了 Shuffling BN ，就是为了防止这种信息泄露）。模型不光是正样本自己和自己学，还和batch norm产生的平均图片（mode，中值）对比着学，这种平均图片就类似 `SwAV`的聚类中心了。

所以说，这篇博客的作者认为batch norm是`BYOL`能够成功的关键，其实是做了一种隐式的对比学习，这个观点很快就被大家所接受了，因为听起来确实很合理，而且后续试验也都验证了这一点。batch norm确实至关重要，拿掉batch norm以后模型就是不好训练，对超参数的设置非常的敏感，稍有不慎它就啥也不学了。



**反转**

BYOL作者是在encoder（比如ResNet50）和两层`Projector`里分布使用BN/LN和什么都不用去做对比实验，最后发现：

- BN非常关键：只要是projector中没有BN的地方，`SimCLR`性稍微下降；但是`BYOL`全都模型坍塌了

- 有BN也会坍塌：作者找到了特例（红色框），即使当projector有BN的时候，`BYOL` 还是训练失败了 。如果BN真的很关键，它真的提供了隐式负样本的对比学习的话，训练就不应该失败

- 完全没有BN，`SimCLR`也坍塌（最后三列的结果。要注意`SimCLR`只有一层projector）。这表明完全不用归一化，`SimCLR`这种使用负样本进行对比学习的方式也无法训练。
  最终结论：**BN跟它原来的设计初衷一样，主要作用就是提高模型训练时的稳定性，从而不会导致模型坍塌** 。作者进一步延伸，如果一开始就能让模型初始化的比较好，后面的训练即使离开了BN也没有问题。

  作者为此又设计了一个实验，借鉴`BEiT`中的`group norm+weight standardization` （前者也是一种归一化方式，后者是一种模型初始化的方式，但都没有进行批量统计操作），BYOL的top-准确率可以达到74.1%，和原来精度可以认为是一样了（74.3%）。



### SimSiam（大道至简）

**链接**

[李沐论文精读系列三：MoCo、对比学习综述（MoCov1/v2/v3、SimCLR v1/v2、DINO等） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/639246058)

**意义**

`SimSiam`即simple Siamese network（**简单孪生网络**）。在BYOL发布时，就已经有很多对比学习的分析性工作了。大家发现，对比学习的成功好像是被很多trick一点点堆起来的性能，比如projection head、更多的数据增强、使用用动量编码器、更大的 batch size等等，好像都缺一不可。

这样因素太多就不方便分析，也不知道每个点到底带来了哪些贡献，所以凯明团队又再次出手，把整个过程**化繁为简**了一下，最后提出了SimSiam。

SimSiam结构非常简单，不需要用负样本（结构类似 `BYOL`）、大的batch size，也不需要动量编码器。然而即使在这种情况下，模型效果也很好。

实验证明去掉负样本、动量编码器、大的batch-size这些trick，模型也能训练的很好。



**模型概述**

具体的模型总览图如下图所示，整体结构非常类似 `BYOL`：

![img](https://pic2.zhimg.com/80/v2-8f2e9401b442e37e3c086948f803a8b1_720w.webp)

前向过程如下：

- image x经过两次数据增强得到𝑥1,𝑥2
- 经过两个编码器`encoder f`（结构一样参数共享，所以叫孪生网络）得到编码特征𝑧1,𝑧2。
- 𝑧1,𝑧2的经过`Projector`得到预测 𝑝1,𝑝2，然后计算**对称性loss**（𝑝1预测𝑧2，同时𝑝2预测𝑧1，单次结果除以2）。
- 和`BYOL`不同的是，这里没有使用动量编码器，**两个encoder完全一样**。



作者做了一系列实验分析，发现SimSiam能够成功训练，而没有模型坍塌，主要是因为有`stop gradient`这个操作。

BN有助于训练优化，但主要是提高模型训练的稳定性，而非避免模型坍塌（见第一行结果）。

![img](https://pic2.zhimg.com/80/v2-53f9e7da4d38121e8155151eea508e79_720w.webp)

- 对比`SimCLR`：SimSiam可以是作为“SimCLR without negative”（SimCLR依赖于负采样以避免“坍塌”）；
- 对比 `SwAV`：SimSiam可以视作“SwAV without online clustering”；
- 对比`BYOL`: SimSiam可以视作“没有动量编码器的BYOL”。



### MOCOv3（引入transformer）

链接

**意义**

无监督的预训练（BERT/GPT等）已经彻底改变了NLP，自从Vision Transformer成功之后，将**ViT引入CV领域的自监督训练**已经是大势所趋了。作者**将backbone从一个残差网络换成了ViT**,但是使用ViT作为backbone会导致训练很不稳定，这种不稳定性是造成模型准确率降低的一个主要问题。

本文作者发现只需要做一点小小的改动（**冻结ViT的`patch projection`层**），就能让这个训练变得更稳定、效果也更好。所以作者不得不写一篇论文来把这个发现告诉大家，也就是标题说的An Empirical Study （一个实验性的study ）。这篇论文是ICCV 21的一篇口头报告论文，但它的的影响力依旧很大。



**模型概述**

MoCo v3的架构，**其实就相当于是MoCo v2和SimSiam 的一个合体**。因为没有模型总览图，所以直接看伪代码：

```python
解释# f_q: query encoder: backbone + proj mlp + pred mlp
# f_k: key momentum encoder: backbone + proj mlp
# m: momentum coefficient
# tau: temperature，也就是τ
for x in loader: # load a minibatch x with N samples
	x1, x2 = aug(x), aug(x) # augmentation
	q1, q2 = f_q(x1), f_q(x2) # queries: [N, C] each
	k1, k2 = f_k(x1), f_k(x2) # keys: [N, C] each
	loss = ctr(q1, k2) + ctr(q2, k1) # symmetrized
	loss.backward()
	update(f_q) # optimizer update: f_q
	f_k = m * f_k + (1-m) * f_q # momentum update: f_k
	
# 对比 loss
def ctr(q, k):
	logits = mm(q, k.t()) # [N, N] pairs
	labels = range(N) # positives are in diagonal
	loss = CrossEntropyLoss(logits/tau, labels)
	return 2 * tau * loss
```

- 整体的框架来说，它还是有两个网络：query编码器和key编码器（动量编码器），目标函数是对比学习loss，所以说从这个角度讲，它是个`MoCov2`
- query编码器除了backbone之外，还有projection head和predictor，而是还算了对称性loss，即`loss = ctr(q1, k2) + ctr(q2, k1)`。所以从这个角度讲，它又是`SimSiam`。
- 所以说，从整体结构上来看，`MoCov3`就是`MoCov2`和`SimSiam`一个延伸工作。



作者发现，每次准确度大幅下降时，模型第一层梯度也会有一个波峰。于是作者尝试将这一层的权重全部冻住，结果发现问题就解决了。而且很神奇的**是这个trick不光是对`MoCov3`有用，它对`BYOL`和 `SimCLR`也有用。**

> 第一层就是`ViT`的`patch projection`层，会将图片分割成一个个patch，然后经过线性层映射为Pacth embedding。



### DINO(自蒸馏学习)

**链接**

**意义**

`DINO`这个名字，来自于它的题目self distillation with no labels，也就是**无标签的自蒸馏方法**（学生网络预测教师网络的输出）。本文和`MoCov3`一样，也是一种自监督训练Vision Transformer的方式，但作者使用另一种操作——**centering**（可以看做是计算整个batch样本的均值，然后减掉这个均值，类似于BN），使ViT可以稳定训练。另外本文发现自监督训练为 Vision Transformer features提供了一些新的特性。本文将ViT和自监督学习结合，并研究**自监督预训练对ViT feature的影响**。

> 自监督学习通过利用句子中的词创建`pretext tasks` ，相比于有监督学习中每个句子对应一个label， `pretext task`提供了更加丰富的学习信号。类似的，图像层面的有监督学习将丰富的图片信息减少到单一的分类概念。

通过研究，本文发现自监督ViT features具有一些独有的特性

1. 自监督ViT features 中**包含清晰的图像语义分割信息**，而这在有监督ViT和convnets中都没有类似的表现。

   一个完全不用任何标签信息训练出来的`Vision Transformer` ，将它的自注意力图进行可视化，会发现能非常准确的抓住每个物体的轮廓，效果甚至可以媲美对这个物体做分割

   ![img](https://pic4.zhimg.com/80/v2-32b50b01c83df4c7c06019a05414b007_720w.webp)

2. 只使用一个比较小的ViT backbone（`ViT-S/8`），自监督训练出来的ViT features 就能在KNN分类器中表现的很好，ImageNet数据集的 top-1精度达到78.3%，超过之前的自监督方法。（也就是ViT features直接去做最近邻分类，连线性分类头或微调都不需要）。

另外在消融实验中证明，动量编码器、multi-crop 数据增强和更小的 ViT patches（计算量更高）都有重要的作用。



**模型概述**

![img](https://pic1.zhimg.com/80/v2-ad2ebb613007a73c759852f26728ebcc_720w.webp)

前向过程：

- 一张图片经过不同的视角得到𝑥1,𝑥2
- 𝑥1,𝑥2分别经过两个编码器𝑔𝜃𝑠,𝑔𝜃𝑡（结构相同参数不同，包含projection head和prediction head）得到编码特征。
- teacher网络的编码器𝑔𝜃𝑡是动量更新；且为了避免模型坍塌，其编码特征会额外进行一个centering的操作
- 这样学生分支和教师分支经过softmax分别得到K维概率分布𝑝1,𝑝2，然后用𝑝1去预测𝑝2 （−𝑝2𝑙𝑜𝑔𝑝1）



- `DINO`的知识蒸馏是一种范式，是通过训练一个学生网络 𝑔𝜃𝑠 去match一个教师网络𝑔𝜃𝑡的输出。
- 两个网络分支最后分别输出概率分布𝑝𝑠,𝑝𝑡，这里概率P是对网络输出进行softmax归一化的结果：



𝑃𝑠(𝑥)𝑖=𝑒𝑥𝑝(𝑔𝜃𝑠(𝑥)𝑖)/𝜏𝑠∑𝑘=1𝐾𝑒𝑥𝑝(𝑔𝜃𝑠(𝑥)𝑘)/𝜏𝑠

其中温度参数𝜏𝑠>0控制分布的sharp程度。𝑃𝑡结果也是这样的公式算出。然后通过固定教师网络，训练学生网络使其参数𝜃𝑠最小化交叉熵损失函数来匹配分布：

𝑚𝑖𝑛𝜃𝑠𝐻(𝑃𝑡(𝑥),𝑃𝑠(𝑥)),𝑤ℎ𝑒𝑟𝑒𝐻(𝑎,𝑏)=−𝑎𝑙𝑜𝑔𝑏

- Teacher Network：和知识蒸馏不同，这里没有一个预先已知的teacher网络。teacher网络来自过去几轮的student网络，因为作者实验发现经过一个epoch训练后冻结teacher网络的训练方式表现不错。（应该是理解为教师网络使用动量编码器，如果参数全部从student网络复制，模型坍塌）



前向过程可以看出，DINO也是**自己预测自己**（student要预测teacher，teacher的输出当成是ground truth ），所以叫自蒸馏。DINO其实就是延续的BYOL，只不过是换了个名字。

| 模型 | 左分支          | 右分支          |
| ---- | --------------- | --------------- |
| MoCo | query 编码器    | key编码器       |
| BYOL | online network  | target network  |
| DINO | student network | teacher network |

- centering：可以看作在teacher分支上加一个偏置项c：𝑔𝑡(𝑥)←𝑔𝑡(𝑥)+𝑐，其中c通过EMA更新：𝑐←𝑚𝑐+(1−𝑚)1𝐵∑𝑖=1𝐵𝑔𝜃𝑡(𝑥𝑖)，m是一个大于0的参数。
- centering可以看做是计算整个batch样本的均值，然后减掉这个均值。centering类似BYOL对于 batch norm 的讨论，因为batch norm也是对整个batch里的样本做了一个均值和方差 。

### Vit（Transformer in CV,打破CV和NLP的模型壁垒）

**链接**

[李沐论文精读系列二：Vision Transformer、MAE、Swin-Transformer-CSDN博客](https://blog.csdn.net/qq_56591814/article/details/127358168)



**意义**

VIT的出现，打破了AlexNet出现以来CNN网络在CV领域的统治地位。VIT表明，在图片分类任务中，只使用纯的Vision Transformer结构也可以取的很好的效果（最佳模型在ImageNet1K上能够达到88.55%的准确率）开启CV新时代。而且 VIT将CV直接当做NLP来做，还打破了CV和NLP的模型壁垒，推进了多模态领域的发展。**比运用ResNet架构的CNN更便宜**



**模型概述**

vit/16,16:patchsize；有监督学习（作者尝试自监督学习，效果不好——》MAE）

简单而言，模型由三个模块组成：

- `Embedding`层（线性投射层Linear Projection of Flattened Patches）
- `Transformer Encoder`(图右侧有给出更加详细的结构)
- `MLP Head`（最终用于分类的层结构）

在CV中运用transformer,将图片**分成一个个16X16的Patch**，降低input的规模防止内存不够（Bert的d_model才512）**使图片各部分类似于句子中的一个个单词作为输入进transformer.**

(如图片原始分辨率224X224=50000，降低为196个图像块（224/16=14,14X14=196），每个图像块为16X16X3(channel)=768,

最终经线性层(Linear projection of FP,可训练，是一个768X768的矩阵，得到patch embedding)并与位置编码（197X768，1D位置编码，同Bert）**sum**后进入transfomer的**input**为197(196+CLS,借鉴bert)X768)，

经transfomer的**output**也为197X768(**输入输出同维，所以可以无限叠加transformer模块**)，

而CLS（视为图像整体特征）经过transformer后的输出作为transformer的最终输出，再进MLP Head![image-20240421114511057](.\image-20240421114511057.png)

因为Vit没有用到CNN的先验知识（平移不变性与locality），所以在小数据集上效果不好。

![image-20240421123334467](.\image-20240421123334467.png)











### MAE（Bert in CV,基于Vit并引入自监督学习）

链接

[Self-Supervised Learning 超详细解读 (六)：MAE：通向 CV 大模型 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/432950958)

意义

有标签的数据集**很贵**，打标签得要多少人工劳力去标注，那成本是相当高的，太贵。相反，无标签的数据集网上随便到处爬，它**便宜**

**模型概述**

MAE 的方法很简单：**Mask 掉输入图像的随机的 patches 并重建它们。**它基于两个核心理念：研究人员开发了一个**非对称**编码器 - 解码器架构，其中一个编码器**只对可见的 patch 子集进行操作** (即没有被 mask 掉的 token)，另一个简单解码器可以从**潜在表征和被 masked 掉的 token** 重建原始图像。Decoder 的架构可以是十分轻量化的模型，且具体的架构对模型性能影响很大。研究人员进一步发现，Mask 掉大部分输入图像 (例如 75%) 会产生重要且有意义的自监督任务。结合这两种设计，我们就能高效地训练大型模型：提升训练速度至 3 倍或更多，并提高准确性。

相较于文本，图片含有大量冗余信息。为避免Model可以根据邻居的冗余信息（局部的）仅进行简单工作（如插值）就能还原出图片，作者选择**盖住图片的大部分**（如3/4），以**降低信息冗余**，并**使模型去学全局的而非局部的信息**，即给模型创造出一个很有挑战性的任务。

并且，因为文本的语义空间更高维，从而在NLP领域解码器只是很简单的MLP即可（如Bert的下游矩阵），但是CV就需要复杂的解码器。（我的理解是：因为单词的语义信息比较丰富，所以不需要通过复杂解码器获取低层特征，可以直接由高层特征确定单词；而图片则需要用如转置卷积去重构图片）

对图片的处理同VIT，将图片分成若干Patch,随机抽样出一些Mask住.

**MAE Encoder** 采用 ViT 架构，但只会作用于 unmasked images。被Mask的块就不看了以节省开销。被Mask的块（统一用同一个共享的、可以学习到的向量表示，类似于Bert的[Mask]）与经过VIT的块（Patch被浅表示为特征向量）加上位置信息后（认为MASK块仅包含位置信息，不确定经过VIT的块还要不要再加一次位置信息，因为进VIT的时候就加过一次了）一起进入decoder

**MAE Decoder**，负责根据中间向量重构图片，其LOSS使用MSE（计算重构块**各像素**与原始块间差异），并只在MASK块上计算MSE（同Bert,没被盖住的块Model已经看过了，没必要算LOSS）。所以Decoder实际上是**预测被mask的pixel**而非patch.

<img src=".\image-20240421162514790.png" alt="image-20240421162514790" style="zoom:50%;" />



MAE 的具体实现方法是：

1. 首先通过 Linear Projection （即VIT）和位置编码得到 image tokens。
2. 随机 shuffle 这些 tokens，按照 masking ratio 扔掉最后的一部分。
3. 把 unmasked patches 输出到 Encoder 中，得到这些 tokens 的表征。
4. 把 Encoder 的输出，结合 masked tokens (可学习的向量)，执行 unshuffle操作恢复顺序，再一起输入到 Decoder 中。
5. shuffle 和 unshuffle 操作的时间开销可忽略不计。

**训练时训Auto-encoder（非对称的解码器与编码器，编码器更复杂，主要计算量来自于编码器）,实作时只用encoder，将其产生的中间向量（特征向量）拿去做实际下游任务**（如encoder+分类头）

### CLIP（跨模态无监督训练）

### Swin transformer

## 多模态

现有的VLP模型（Vision-and-Language Pre-training，视觉文本多模态模型）**抽取文本特征基本上都使用 pre-trained BERT的 tokenizer**来得到text embedding，但抽取视觉特征存在着差异。往往处理视觉特征的网络越复杂，模型效果就越好，所以抽取视觉特征是现有VLP模型的瓶颈。图上图所示，**获取visual embedding**的方法总共有三大类：

​	**Region feture**：通常采用Faster R-CNN二阶段检测器提取区域性特征，这种操作也是最贵的；比如图像经过ResNet101 backbone(一系列的卷积层用于提取图像的feature maps)提取特征，再经过RPN(Region Proposal Network，区域生成网络)得到一些RoI(Region of interest，感兴趣区域)，然后使用NMS(Non-Maximum Suppression，非极大抑制)过滤冗余的RoI，最后经过RoI Head(在RPN生成的候选区域中，对候选区域进行分类和边界框回归的神经网络模块)得到一些一维的向量（Region Feature），也就是一个个bounding box（检测框）。
​	**Grid feature**：将CNN backbone得到的feature map，作为网格特征，大大简化了计算量。比如将ResNet50最后得到的7×7特征图拉直为一个序列，或者是上一层的14×14的特征图。
​	**Patch projection**：使用类似ViT模型中的patch projection层直接得到patch embeddings，ViLT是首个这么做的。



***为什么用目标检测器来处理图像特征？***

​		1.图像的像素不能直接扔给Transformer，不然**序列长度就太长**了，Transformer处理不了。ViT提出将图片分割成一个个固定大小的patch，然后使用线性层映射为patch embedding输入网络（比如patch size=16×16时，处理后序列长度从224×224降为14×14）。
​		2.上面提到了**VLP想要的是离散的且语义性强的特征表示形式**，而目标检测正好是一个离散化的过程，返回的是 bounding box，它代表一个个物体，有明确的语义信息（可类比文本中的token），而且还是离散化的。
​		3.跟当时的 VLP 下游任务有关，当时主要是 VQA（Visual Question Answering）、Image Captioning、Image Retrieval 等等（这些任务的简介可以参考VL (Vision and Language) 任务简介及数据集），这些任务往往都跟物体有非常直接的联系，有非常**强的对物体的依赖性**。



**多模态特征的融合**有两种常见方式：

Single-stream：单通路结构，文本特征和图像特征直接concat连接，然后输入一个transformer进行交互；
Dual-stream：双通道结构，文本特征和图像特征分别过一个文本模型和图像模型，充分挖掘单模态特征，然后再经过一个transformer layer做融合。

 这两种方法的效果其实差不多，dual-stream明显更贵，参数量、计算量更多。







### 多模态论文串讲

### Transformer Encoder

#### ViLT（引入VIT）

**链接**

[【李沐论文精读】ViLT精读_李沐 vilt-CSDN博客](https://blog.csdn.net/qq_45276194/article/details/136648796)

**意义**

 ViLT是一个极其简单的**视觉-文本多模态**的框架。其最主要贡献：就是**把多模态学习框架中的目标检测，也就是论文中反复强调的Region Feature（区域性特征，也就是检测框）直接拿掉了**。这个操作简直算是神来之笔，因为它极大地简化了视觉模态特征的抽取过程，大大提高了模型的推理速度，可称之为多模态领域一个里程碑式的工作。

ViLT的三大贡献：

​	使用 **Patch projection层抽取视觉特征**，极大**简化**了多模态学习框架，减少了运行时间和参数量；
​	ViLT是第一个不使用卷积特征和区域性特征的同时（Without Convolution or Region Supervision），模型性能还**表现的比较好**的模型；
​	首次在VLP训练中使用了**整词掩码**和**图像数据增强**，并被证明可以明显提升模型性能。

![img](https://img-blog.csdnimg.cn/direct/f5df225182f740f58fba01a05d8c3053.png)

![img](https://img-blog.csdnimg.cn/direct/965027c805b64bc3a6044d3b9f585936.png)

(a)：VSE/ SCAN等模型的做法，视觉特征的处理远大于文本特征，模态融合只使用了简单的点乘操作或很简单的浅层attention网络；即VE > TE > MI
(b)：CLIP，每个模态单独使用transformer encoder，两者计算量差不多。特征融合部分，只是简单的计算了一下图文特征的相似性；即 VE = TE > MI。CLIP特别适合需要图文特征（GroupViT/GLIP等）或者是图文检索的任务，但做VQA或者visual reasoning（视觉推理，更难的VQA）这种需要视觉推理的任务时，会稍逊一筹。因为一个简单的不可学习的点乘，是没法做深层次的特征融合和分析的。
(c)：这些年80%的工作都是这个方向，比如ViLBERT、UNITER、Pixel-BERT等等。文本侧很轻量，但图像侧使用很重的CNN抽取特征；最后特征融合使用了Transformer，所以VE > MI > TE；
(d)：：本文的模型，ViLT，借助 ViT 的想法**把图像部分也变得非常轻量**。



**模型概述**

现有的VLP模型（Vision-and-Language Pre-training，视觉文本多模态模型）**抽取文本特征基本上都使用 pre-trained BERT的 tokenizer**来得到text embedding，但抽取视觉特征存在着差异。往往处理视觉特征的网络越复杂，模型效果就越好，所以抽取视觉特征是现有VLP模型的瓶颈。

如上图所示，**获取visual embedding的方法**总共有三大类：

​	**Region feture**：通常采用Faster R-CNN二阶段检测器提取区域性特征，这种操作也是最贵的；比如图像经过ResNet101 backbone(一系列的卷积层用于提取图像的feature maps)提取特征，再经过RPN(Region Proposal Network，区域生成网络)得到一些RoI(Region of interest，感兴趣区域)，然后使用NMS(Non-Maximum Suppression，非极大抑制)过滤冗余的RoI，最后经过RoI Head(在RPN生成的候选区域中，对候选区域进行分类和边界框回归的神经网络模块)得到一些一维的向量（Region Feature），也就是一个个bounding box（检测框）。
​	**Grid feature**：将CNN backbone得到的feature map，作为网格特征，大大简化了计算量。比如将ResNet50最后得到的7×7特征图拉直为一个序列，或者是上一层的14×14的特征图。使用目标检测器来提取图像特征实在是太浪费资源，于是就开始尝试把视觉这里的计算量降下来。其中一个尝试就是Pixel-BERT，它是用了一个在 ImageNet 上预训练好的 ResNet，将ResNet最后得到的特征图当成是一个离散的序列，然后和文本特征一起输入transformer做融合，速度就快很多
​	**Patch projection**：使用类似ViT模型中的patch projection层直接得到patch embeddings，ViLT是首个这么做的，有三个原因：
​			1.**不需要使用额外的网络。**无论是CNN backbone还是目标检测，都非常贵。
​			2.**不需要缓存特征。**Region feture和Grid feature都需要在线下使用预训练的模型提前抽取好图片特征，然后再训练。虽然这样训练还是比较轻量的，但在部署的时候是一个很大的局限性。真实场景里每时每刻都在生成新数据，都需要抽取新数据的特征，这时推理速度就是一大瓶颈了，所以作者才想设计一个更轻量更简单的视觉特征抽取方案。
​			3.**ViT的patch projection层表现很好。**和 Vision Transformer（简称ViT）的预处理一样，通过一个Linear Embedding层实现，将 patch 变成 token。ViLT在视觉方面的运行时间仅仅需要0.4ms，相比传统模型的运行时间大大减少，且模型效果并不会下降很多。

在**文本方面**，这些模型都是基本一样的，通过一个Embedding矩阵，变成一个个的word token。得到了视觉的序列和文本的序列后输入到Modality Interaction（基本都是Transformer）进行模态之间的融合。

​	多模态特征的融合有两种常见方式：

Single-stream：单通路结构，文本特征和图像特征直接concat连接，然后输入一个transformer进行交互；
Dual-stream：双通道结构，文本特征和图像特征分别过一个文本模型和图像模型，充分挖掘单模态特征，然后再经过一个transformer layer做融合。
       这两种方法的效果其实差不多，dual-stream明显更贵，参数量、计算量更多 ，所以作者**采用了Single-stream**。

![](https://img-blog.csdnimg.cn/direct/f52de759783a4c758bc0f5b930321c18.png)

文本经过pre-trained BERT tokenizer得到word embedding（前面有CLS token，图中*表示）
图片经过ViT patch projection层得到patch embedding（也是用*表示CLS token）；
文本特征+文本位置编码+模态嵌入得到最终的text embedding，图像这边也是类似的操作得到image embedding；二者concat拼接之后，一起输入transformer layer，然后做MSA交互（多头自注意力）

*问：为什么要进行嵌入区分？*

        模态嵌入即Modal-type embedding，使用0代表文本，1代表图像。因为在Single-stream模型中，图文特征是直接拼在一起输入一个transformer。如果不进行标注，模型是不知道哪一块是文本，哪一块是特征，这样不利于学习。加了模态嵌入可以区分之后，模型就可以在训练时找出图文之间的关系，学习的更好。



**目标函数**

​        ViLT使用了一般VLP模型常用的目标函数，即**图文匹配loss**（ ITM，image text matching）和 **BERT的掩码学习loss**（MLM，Masked Language Modeling）。另外ViLT还使用了**Word Patch Alignment（WPA）**。

ITM loss：以50%的概率将文本对应的图片随机替换成数据集中的其它图片，然后将文本CLS token对应输出使用一个FC层（即分类头）映射成一个二值logits，用来判断图像文本是否匹配（是不是一对）；说白了itm就是个二分类任务。但是太简单了。因为正样本可能不够好找，但是负样本很好判断，所以LOSS 很快会收敛。后期改进是找负样本中与正样本最接近那个负样本（ALBEF）。
MLM loss：随机mask一个文本token，然后将其重建出来。
其实图片这边也可以使用masked patch 重构任务，但是当时MAE还没出来，重构效果还不够好，所以作者没有这么做。后续有VL-BEiT，就使用了图像-文本掩码任务（masked vision-language modeling ）。
WPA：简单理解就是将文本和图像的输出都当做一个概率分布，然后使用最优运输理论计算一下两者的距离。**非常慢**，限制了ViLT的速度。



**Whole Word Masking整词掩码**
        将整个词都 mask 掉。作者在论文中举例，如果有单词 “giraffe”，如果用 tokenizer 如 BPE 等，那么 “giraffe” 就会被分成 [“gi”, “##raf”, “##fe”]，此时这些小部分才是一个个的 token，假设此时把其中的一个 token “##raf” mask 掉，即变成 [“gi”, “[MASK]”, “##fe”]，英文中以 “gi” 开头，“##fe” 结尾的单词很少，那么模型很容易就知道中间填 “##raf”，这就导致再做多模态时，根本不需要借助图像这边的信息，仅根据文本就能判断中间填 “##raf”，这样这个 loss 就失去了意义。既然如此，作者就将整个单词都 mask 掉，也就是在整个句子中去掉了 “giraffe”，这时模型再想把 “giraffe” 重建出来，就必须要借助图像的信息，进一步加强图像与文本间的联系。

 **Image Augmentation**
        ViLT是一个端到端的模型，作者在微调时直接就上了 RandAugment。考虑到需要图文匹配，作者改动了其中两处，即去掉了cutout和color inversion（前者是随机去掉图像中某一区域，后者是进行颜色变换）

*作者还提供了未来的几个可能的研究方向：*

Scalability：如果模型更大，用的数据集更多，那么结果也会更好。
Masked Modeling for Visual Inputs：在图像部分也做掩码重建（完形填空）。这部分当前已经有 BEiT 和 MAE 了，也已经有论文对 ViLT 在这部分进行了图像重建方面的改进。
Augmentation Strategies：根据消融实验来看，数据增强确实是很有效果的，作者希望可以优化这一块。



#### Clip（跨模态无监督训练）

**链接**



**意义**

CLIP(Contrastive Language-Image Pre-training)，是一种**基于对比文本-图像对的预训练方法**，属于对比学习。CLIP用**文本作为监督信号**来训练**可迁移**的视觉模型，使得最终模型的**zero-shot**效果堪比ResNet50，**泛化性非常好**。

CLIP算是在跨模态训练无监督中的开创性工作，作者在开头梳理了现在vision上的训练方式，从有监督的训练，到弱监督训练，再到最终的无监督训练。这样训练的好处在于可以避免的有监督的 categorical label的限制，具有**zero-shot**性质，极大的提升了模型的实用性能。

CLIP 最大的贡献就是**打破了之前固定种类标签的范式**，无论是在收集数据集时，还是在训练模型时，都不需要像 ImageNet 那样做 1000 个类，直接搜集图片和文本的配对就行，然后去预测相似性。在收集数据、训练、推理时都更方便了，甚至可以 zero-shot 去做各种各样的分类任务。**CLIP得到的图片与文本embedding是在同一个语义空间的。**

​	**方法出奇的简单，但是效果呢又出奇的好**，clip的这个**迁移学习能力是非常强**的。它预训好的这个模型能够在任意一个视觉分类的这个数据集上，取得不错的效果，而且最重要的是它是**zero shot**的，意思就是他完全没有在这些数据集上去做训练，就能得到这么高的效果，Clip在不使用ImageNet的训练集的情况下，也就是不使用任何一张的128万张图片训练的情况下，直接zero short做推理就能获得和之前有监督训练好的这个 res50取得同样的效果。

**用CLIP实现zero-shot分类只需要简单的两步**：

- 根据任务的分类标签构建每个类别的描述文本：`A photo of {label}`，然后将这些文本送入`Text Encoder`得到对应的文本特征。如果类别数目为n，那么将得到`n`个文本特征；
- 将要预测的图像送入`Image Encoder`得到图像特征，然后与`n`个文本特征计算缩放的余弦相似度（**和训练过程保持一致**），然后选择相似度最大的文本对应的类别作为图像分类预测结果。进一步地，可以将这些相似度看成logits，送入softmax后可以到每个类别的预测概率。

我们不再需要预先定义好的标签（类别）列表，直接将图片喂给不同的文本句子，就可以知道图片中是否有我们感兴趣的物体。即，CLIP的多模态特性（利用文本监督信号）为具体的任务构建了动态的分类器，使得模型不再受限于预先定义好的类别，更加具有通用性和可用性。

> 比如新增三轮车的图片时，只需要在文本部分也加上三轮车这个类别，模型很有可能直接`zero-shot`推理出图片属于三轮车这个类。而之前的模型，是永远不会预测出ImageNet1000个类之外的类的，这也是`CLIP`最吸引人的地方。
>
> 　　类别单词变成句子，有`prompt engineering`和`prompt ensemble`两种方法，进一步提高模型准确率，在论文后面会讲到
>
> 

CLIP 这种**双塔式多模态特征提取、模态交互部分较为简单**（点乘计算**余弦相似度**）的多模态模型十分适合**图像文本匹配、图像文本检索等多模态任务**。因为这种模型有一个显著的优点，即可以将数据库中的图像或文本特征预先进行特征提取并保存到硬盘中。这使得检索过程仅需要提取待检索图像的特征并计算相似度即可，大大提升了检索过程的效率（矩阵运算非常快）。



**模型概述**

模型的结构很简洁，就是将image和text通过两个各自模态的encoder提取feature之后，将互相配对的image-text所属的feature作为正样本（图中矩阵对角线），其余不配对的样本作为负样本（除对角线之外的元素）来进行**对比学习**。（即文本与图像是两个transformer,模态融合部分是**点乘余弦相似度**)

假如模型输入的是![n](https://latex.csdn.net/eq?n)对图片-文本对，那么这![n](https://latex.csdn.net/eq?n)对互相配对的图像–文本对是正样本（下图输出特征矩阵对角线上标识蓝色的部位），其它![n^2-n](https://latex.csdn.net/eq?n%5E2-n)对样本都是负样本。这样模型的训练过程就是最大化![n](https://latex.csdn.net/eq?n)个正样本的相似度，同时最小化![n^2-n](https://latex.csdn.net/eq?n%5E2-n)个负样本的相似度。相似度是计算文本特征和图像特征的**余弦相似性**cosine similarity。（即模态融合部分是**点乘余弦相似度**)

<img src="https://img-blog.csdnimg.cn/direct/e9da0520bd124a4dbff148b65f7c6f76.png" alt="img" style="zoom: 80%;" />



**预训练方法（训练效率至关重要）**

CV领域的模型都很大，训练起来也很贵。比如[noise student](https://link.zhihu.com/?target=https%3A//paperswithcode.com/paper/self-training-with-noisy-student-improves)之前在ImageNet一直霸榜，但是这个模型需要在一个 TPUv3上训练33年，这还只是在包含1000类的ImageNet上预训练的，而且只训练视觉特征。

由于训练数据量和模型计算量都很大，训练效率成为一个至关重要的因素。作者做了很多尝试，最终选择了**对比学习**：

- `VirTex`模型：预测文本，对应下图蓝色线`Transformer Language Model`

- - `Image Encoder`使用CNN模型，`Text Encoder`使用transformer模型，两个模型一起从头训练，任务是预测图片对应的文本（image caption）。
  - 这种方法的训练效率太慢，因为根据图片进行文本描述，可能性太多了，你可以从各个角度去描述一张图片。

- `Bag of Words Prediction`（橘色线）：不要求每个词都是按顺序的进行预测，所有词都预测出来就行。这样放宽了约束，训练速度提高了三倍。

- `CLIP`：简化版的`ConVIRT`，基于对比学习。

- - **只需要判断图文是否配对**，进一步简化了训练任务，训练效率一下子提升4倍（绿色线）
  - 训练任务更加合理。因为训练数据所包含的文本-图像对是从互联网收集来的，它们存在一定的噪音，二者并不完全匹配。适当的降低训练目标，反而能取得更好的收敛。





![img](https://pic2.zhimg.com/80/v2-a44d2d31afcb41ea2f6f0147cd3dda9d_720w.webp)

**模型的输入**是若干个**图像-文本对**

如图最上面的数据中图像是一个小狗，文本是 ”Pepper the aussie pup”）。

- 文本部分：文本通过一个Text Encoder得到一些文本的特征。同样假设每个training batch都有 N 个图像-文本对，那么就会得到N 个文本的特征（如下图中的![T_1,T_2,...,T_N](https://latex.csdn.net/eq?T_1%2CT_2%2C...%2CT_N))![img](https://img-blog.csdnimg.cn/direct/6737a17ef14643baa8a70fe3836fe568.png)
- 图像部分：图像通过一个Image Encoder 得到一些特征，这个 encoder 既可以是 ResNet，也可以是 Vision Transformer。假设每个 training batch 都有 N 个 图像-文本 对儿，那么就会得到 N 个图像的特征（如下图中的）![img](https://img-blog.csdnimg.cn/direct/a1bdf47f7433454c8c492a7016081c19.png)

CLIP 就是在以上这些特征上去做对比学习，对比学习非常灵活，只需要正样本和负样本的定义，其它都是正常套路。这里配对的图像-文本对就是正样本（即下图中对角线（蓝色）部分，![I_1\cdot T_1,I_2\cdot T_2,...,I_N\cdot T_N](https://latex.csdn.net/eq?I_1%5Ccdot%20T_1%2CI_2%5Ccdot%20T_2%2C...%2CI_N%5Ccdot%20T_N))

![img](https://img-blog.csdnimg.cn/direct/1f3a5c9bfccc4e2a810d89fdecf91f89.png)



LOSS:ITC（image-text contrastive）来对齐文本和图像的嵌入空间  LOSS：目的是把成对的图像-文本拉近。让正确的图像文本对之间的距离越小越好，让错误的图像文本对间的距离越大越好。

![image-20240424121134960](.\image-20240424121134960.png)

axis=0:横轴   axis=1：纵轴

> OpenAI是一家GPT化的公司，从GPT系列、DALL-E到Image-GPT等等都是基于GPT做的，唯有`CLIP`因为效率的原因，选择了对比学习进行训练。

最终`Text Encoder`固定选择一个包含63M参数的text transformer模型，而`Image Encoder`采用了两种的不同的架构。因为CLIP虽然是多模态模型，但它主要是用来**训练可迁移的视觉模型**。

- `Image Encoder`架构

- - ResNet：ResNet50，ResNet101，RN50x4，RN50x16和RNx64（后面三个模型是按照EfficientNet缩放规则对ResNet分别增大4x，16x和64x得到）
  - ViT：ViT-B/32，ViT-B/16和**ViT-L/14**。

- ViT-L/14效果最好，所以作者还将其在336的分辨率下**额外finetune了一个epoch来增强性能**，记为 `ViT-L/14@336px`。后面论文中没有特别说明的情况下，进行对比实验的CLIP模型都是指这个。

![img](https://pic2.zhimg.com/80/v2-f6853b4b1545eabd5b31d34c0a8ecef5_720w.webp)



- 训练细节

- - 数据集非常大，几乎不会出现过拟合，所以`Image Encoder`和`Text Encoder`不需要提前进行预训练。
  - 只使用线性投射层（线性非线性影响不大）。
  - 数据增强只使用图片的随机剪裁，这是因为数据集非常大。
  - 对比学习目标函数中的超参数`τ`，设置成可学习的标量，在训练中自动优化，而不用慢慢调参（还是因为数据集太大，训练很贵）。

**prompt engineering** 

作者提出 **prompt template**：以 ImageNet 为例，CLIP 先把 ImageNet 这1000个类（如图中"plane", “car”, “dog”, …, “brid”）变成一个句子，使推理和预训练时保持一致（**消除distribution gap**）。，也就是将这些类别去替代 “A photo of a {object}” 中的 “{object}” ，以 “plane” 类为例，它就变成"A photo of a plane"，那么 ImageNet 里的1000个类别就都在这里生成了1000个句子，然后通过先前预训练好的 Text Encoder 就会得到1000个文本的特征。

  其实如果直接用单词（“plane”, “car”, “dog”, …, “brid”）直接去抽取文本特征也是可以的，但是因为在模型预训练时，**与图像对应的都是句子**，如果在推理的时候，把所有的文本都变成了单词，那这样就跟训练时看到的文本不太一样了，所以效果就会有所下降。并且词语存在**歧义性**：

- 比如在做物体检测时，有一个类别是remote（遥控器）。但如果直接喂给文本编码器，很可能被模型认为是遥远的意思。
- 同一个词语在不同数据集中所表示的意思可能有所不同。例如在 Oxford-IIIT Pets 数据集中，boxer指的是狗的一个种类，在其他数据集中指的是拳击运动员。
- 所以 CLIP预训练时，用来描述图片内容的文本是一个句子，比如`A photo of {label}`。这里的label就只能是名词，一定程度上消除了歧义性。



**prompt ensembling**

作者尝试了集成多个模板的效果，即在多个zero-shot分类器上进行集成，这些分类器使用不同的提示模板来构造不同的文本。由于是在嵌入空间(embedding space)而不是概率空间(probability space)上集成的，因此节约了计算成本。在大多数数据集上，`prompt ensembling`都能够提升模型性能。

最终作者使用了80种模板来进行集成，每种模板使用了不同的形容词，来描述不同的情境。



![img](https://img-blog.csdnimg.cn/direct/24a615463c3a46eead897d200b49ab00.png)

**VE（视觉嵌入）、TE（文本嵌入）和 MI（模态交互）**分别表示不同的网络组件，他们在途中模块的大小表示对应网络的复杂度。

**视觉部分**：在图 1 所示的四种结构中，(a)、(b)、(c)三种方法中，由于使用**目标检测器**确定图像的区域，因此视觉端（Visual Embed）都是一个复杂的网络。在 (d) 中，也就是 ViLT 中，使用简单的**线性映射**(patch project)，实现了视觉端处理。ViLT 将网络的大部分复杂度放在多模态任务中重要的模态交互部分。 问题是图像特征提取过于简单。一张图像中包含的信息多于一个句子的信息，按照道理，图像特征提取器应该比文本特征提取器更复杂。

**文本部分**：a:词嵌入 b:transfomer c:词嵌入 d:词嵌入   词嵌入：一般是pre-trained BERT的 tokenizer，比较轻量化

**MI**：a: 点乘 b:点乘 c,d:transformer			只采用点乘作为模态融合手段。这样无法处理复杂的多模态融合。多个模态的融合要有一定的复杂度。



ViLT论文的研究动机其实就是为了把目标检测从视觉端拿掉。图文多模态任务，关键是提取视觉特征和文本特征，然后对齐。在之前的多模态研究工作中，视觉侧通常需要一个目标检测器来确定图像中物体所在的区域，再提取各区域的特征。ViT 将 Transformer 迁移到视觉领域之后，人们意识到，直接使用 patch projection 来处理图像输入也是可行的。由此，ViLT 首次使用 patch projcetion 来直接提取图像特征，摆脱了笨重的目标检测器。  但ViLT由两个局限性  

虽然 ViLT 通过改用线性映射，降低了视觉端嵌入网络的复杂度，但是**性能有所下降**。原因是文本端的 tokenizer 已经有一定语义理解能力了，而视觉端的 patch embedding 是随机初始化的。
虽然 ViLT 的推理很快，但是训练时间比较长。

 **CLIP**一种典型的双塔结构：在视觉端和文本端分别有一个独立的编码器来提取视觉特征和文本特征，而模态间的交互就是简单的**点乘余弦相似度**。CLIP可以提前把数据库里所有的图像文本的特征提前抽好，想用的时候直接点乘。

CLIP的缺陷：

CLIP 这种**双塔式多模态特征提取、模态交互部分较为简单**的多模态模型十分适合图像文本匹配、图像文本检索等多模态任务。因为这种模型有一个显著的优点，即可以将数据库中的图像或文本特征预先进行特征提取并保存到硬盘中。这使得检索过程仅需要提取待检索图像的特征并计算相似度即可，大大提升了检索过程的效率。然而，也正是由于**模态交互过程过于简单**，这类模型在面对视觉问答、视觉推理等需要处理复杂的模态间关系的任务时，效果一般。
        总结：根据ViLT论文里的figure2, 可以得出这样一个结论，我们需要**好的visual embedding**，图像编码器比文本编码器要大（假设1），因为图像更为复杂，同时**modality interaction也要很好**（假设2）而不只是一个简单的点乘交互；text embedding已经很成熟，一般是pre-trained BERT的 tokenizer，这个已经很轻量化了。因此我们总结出理想化的情况应该是接近下图(c)的情况。



至此，我们可以在图 1 的基础上，分析现有模型结构与损失函数，得到接下来多模态学习可行的方向。

我们可以考虑一些常用的loss: ，下面前三个loss是比较好用的。

**ITC**:Image text contrastive loss（CLIP模型训练方式） 
**ITM**:Image text matching loss（ViLBERT和ViLT使用过）
**MLM**:Masked language modelling loss（BERT训练方式）
**WPA**:Word patch alignment (这个在vilt中用到，但是计算很慢)
        通过总结，就可以引出ALBEF



#### ALBEF(ALign BEfore Fuse，博采众长，开源代码)

**链接**

**意义**

在 ALBEF之前，多模态领域的工作通常是使用一个 Transformer 模型来联合编码文本 token 和图像 token。其中图像 token 是经过目标检测器检出的图像区域，目标检测器是预训练得到，而非与整体网络一起进行端到端的训练。因此文本与图像没有“**对齐**”（align）。ALBEF 提出在进行多模态交互之前，先通过一个**对比损失**（其实就是 CLIP 中的 **ITC 损失**）来对齐图像和文本数据(拉近配对的图文见距离)。这是 ALBEF 的第一个贡献。

第二个贡献是在训练数据和训练方式上。ALBEF 在训练时，通过**动量蒸馏**（momentum distillation）这种自训练的学习方式来从网络图文对数据中学习，缓解原始数据中噪声较大的问题。ALBEF 通过改进训练方式，通过自学习生成伪标签的方式来进行数据清洗，改进数据的质量。在理论上，ALBEF 论文通过互信息最大化的角度，解释**不同的损失函数LOSS**其实是在为同一个图像文本对，其实就是在为图文对提供不同的视角（view），类似于在做一种数据增强，使得训练得到的多模态模型能理解不同模态下的语义，即具备 Semantic Preserving 的能力。



**模型概述**

![img](https://img-blog.csdnimg.cn/direct/162f7395c3574b0db600b07f271c17c5.png)

在这张图中，满足了我们两个假设，**文本编码器（6X）比图像编码器（12X）小**且**模态融合（6X）也大**。

同时也用了我们提到的三个loss去训练模型：Image-text contrastive loss(**ITC**)、Image text matching loss(**ITM**)和Masked language modelling loss(**MLM**)。



**目标函数**
        通过对比学习可知，只要定义一个正样本对和多个负样本对，就可以进行对比了。我们希望正样本对之间的距离越来越近，正负样本对之间的距离越来越远。首先要做的就是去出去这种全局特征，在特征之间去做embedding space上的拉近和拉远。

ALBEF 有三个预训练 loss：

- Image-Text Contrastive Loss (**ITC** loss)：让正确的图像文本对之间的距离越小越好，让错误的图像文本对间的距离越大越好。它就是 CLIP 中的损失函数。图像和文本分别通过encoder tokenise, CLS token是一个全局特征（图中绿色方块旁边的黄色方块）， down sample和normalisation(786x1 => 256x1)，然后进行正负样本的对比学习（预先存了很65536个负样本q，没有gradient，由Momentum Model产生），这一步就是**align**。
- Masked Language Modeling (**MLM** loss)：类似BERT的完形填空，mask一个词语，去预测mask的词语，但是借助了图像的信息。
- Image-Text Matching (**ITM** loss)：判断某个图像和文本是不是正确的一对，是一个二分类任务。在multimodal encoder的输出之后加一个二分类头(FC层)，这里很特别的是，每个batch里我拿一个图片和batch里除了配对文本之外的所有的文本做cosine similarity (，余弦相似度，借助之前ITC的那个模块)，挑一个相似度第二高的作为负样本 (**hard negative**) 来训练，来加大难度（这个相似度第二高的负样本几乎已经可以当正样本来使用）。


​        在这里有一个小细节，计算ITC和ITM loss的时候，输入的都是原始的image and text embedding（下图橙色的T '表示masked text embedding），算MLM loss的时候，用的是原始的image embedding，但是是masked后的text embedding，因此每一次训练iteration其实做了**2次forward**，一次用了原始的image and text embedding，另一次用了原始的image和masked的text embedding，因为你要算多个loss函数，这也是多模态往往训练较慢的原因，每次迭代要两遍forward。

![img](https://img-blog.csdnimg.cn/direct/efff86198ee44d95b8bbf9c930d855da.png)



最后的目标函数计算如下：![L=L_{itc}+L_{mlm}+L_{itm}](https://latex.csdn.net/eq?L%3DL_%7Bitc%7D&plus;L_%7Bmlm%7D&plus;L_%7Bitm%7D)



 **Momentum Distillation**

ALBEF 中动量蒸馏的提出，是为了解决网络图文对训练数据噪声过大的问题。

​        做动量蒸馏(Momentum Distillation)的动机：从网上爬下来的图像文本对通常weakly-correlated，即文本并没有很好地描述图像，从而产生了noise。这种弱关联的训练样本中可能出现某些负样本的图文匹配程度，比 GT 中正样本的 one-hot 标签的匹配程度更高的情况，见下图，一昧的惩罚这种负样本不利于 ITC 和 MLM 两种任务的训练。

比如一张青山绿水的景点照片，网络上的对应文字不会是“一座很美丽的山，下面有清澈的河流”这种我们想要的描述性的文本，而很可能会是这个景点的名字，如“桂林山水”。从语义的角度来说，这样的图文对是弱关联（weakly correlated）的，不是我们想要的训练样本。

![img](https://img-blog.csdnimg.cn/direct/f684323c8d6b4c9a80fade5987bbf289.png)

​	ALBEF 中除了梯度更新的主模型之外，还有一个**动量模型**，用于为主模型的训练生成 multi-hot 的**伪标签**（为经过softmax的概率分布，即**softmax score**，而非one hot label）。动量模型通过滑动指数平均(EMA)的方式，根据主模型进行动量更新。这样，除了Ground Truth(GT，真实标签)中的 one-hot 标签，我们就又得到了multi-hot的伪标签(pseudo-targets)，用于 ITC 和 MLM 任务的损失计算（**计算KL散度**）。具体的说，我们希望模型输出同时与GT和伪标签接近。

补充一句，对于 ITM 任务，由于其本身就是基于 GT 的二分类任务，并且通过 ITC 中计算的相似度结果进行了难负例挖掘，因此无需进行动量计算(有冲突)。

    则实际上论文设计了5个loss，2个ITC（基于GT的和基于伪标签的），2个MLM，1个ITM，ITM这个loss ground truth很清晰，所以不需要momentum的版本。



虽然 ViLT 的卖点是推理速度快，但是它的训练还是很慢。**而 ALBEF 的训练速度与推理速度**都很快，预训练阶段只用了 8 张 A100；作为对比，ViLT 的预训练用到了 64 张 V100

不管是性能还是速度，ALBEF 都非常出色，算是 2021 年多模态领域承上启下的一篇工作。



#### VLMo（统一框架，最大化利用各领域数据集分阶段训练）

链接

**意义**

**贡献1**：dual-encoder (双塔模型，如CLIP) 解决了检索问题，而fusion encoder，也叫单塔模型，解决了不同模态之间的交互问题，VLMo就把2种的好处都结合了起来，一个模型，想当双塔和单塔 (论文命名为vision-language expert, language expert, vision expert，其实就是不共享参数的FC层) 用都可以。

**贡献2**：**分阶段模型训练**的改进(stage-wise pre-training), 简单来说就是多模态的数据集不够大，那我就先预训练单独的一个模态。

双编码器模型（dual-encoder，结构如图 1 (b)）的优点是在进行检索等任务时，可以预先对数据库中的数据进行特征提取，运行效率高。缺点是模态交互部分只有一个简单的余弦相似度的计算，过于简单，在视觉推理等模态交互复杂的任务上表现较差。与之相反的，融合编码器模型（fusion-encoder，结构如图 1 (c/d)）的优点是模态交互充分，缺点是无法预先进行特征提取，效率稍差。

为了解决这种冲突，VLMo 提出了 MoME（Mixture of Multi Expert），**由不同的 “专家” 来处理不同类型（文本/图像）的输入数据**。简单来说，就是在每个 Tranformer 块中：**自注意力层权重在不同类型输入间共享**，而 **FFN 层权重则根据输入类型的不同而不同**。

VLMo的目标函数和ALBEF一样也是**ITC、ITM和MLM**。因为在NLP使用Transformer时，数据集越大训练效果越好，在做多模态时也希望如此，但是在当时还没有开源的大规模数据集。**曲线救国**：所以VLMo的作者想到可以用文本和视觉各自领域的超大规模数据集先分别对 “文本专家” 和 “视觉专家” 进行预训练(stage-wise pre-training)，然后再在多模态数据集上进行预训练。



**模型概述**

![img](https://img-blog.csdnimg.cn/direct/5883a67b05f74cf5a53e27fb97664487.png)

预训练任务的选择上，VLMo 与 ALBEF 一致，同样使用 **ITC、ITM 和 MLM** 三种，并且同样借助 ITC 为 ITM 进行**hard negtives**。在进行不同的任务时，会使用 MoME 结构中不同的 FFN 层参数进行训练：

ITC：在计算 ITC 损失时，VLMo 的模型是一种 “dual encoder” 模型，以双塔的结构分别对文本和图像进行嵌入。即**变成了CLIP**
ITM、MLM：在计算 ITM、MLM 损失时，VLMo 模型又成了一种 “fusion encoder” 模型，分别提取图像文本的特征之后，再用 FFN 层 Transformer Block 进行模态融合。
       MoME 结构最大的优势就是**灵活**。在训练时，对应不同的任务时使用不同结构计算损失函数，并更新对应参数。这样的训练有一个**缺点**是需要做多次模型前向。**在推理时，灵活性的优势得到体现。**如果要做检索类任务，可以用单独的文本/图像编码器去提取特征(把已经看过的特征存起来)，提高处理效率（即用**CLIP**）；而如果要做推理类任务，又可以通过图文编码器进行充分的模态交互（即用**Fusion model**）。巧妙地解决了前言部分提到的两种结构的冲突。

**分阶段训练方式**
首先，VLMo **先**在单独的图像数据上训练自注意力层和视觉 FFN ；
然后，在单独的文本数据上训练文本 FFN ；
最后，在多模态数据上训练自注意力层和三种 FFN 专家。
        这里特别有趣的点是在单独的文本数据上进行训练时，自注意力层是冻结的。也就是说，通过图像数据训练出的自注意力层，在文本数据上甚至连微调都不需要，就能工作得很好。如果换过来，先文本，在视觉，效果会怎样呢（不好）？是否不同模态间的注意力是可以通用的呢？

![img](https://img-blog.csdnimg.cn/direct/7376b68df98649539daecf2fd2c7e292.png)****



### Transformer Encoder-Decoder

#### BLIP(融合了ALBEF的VLMo，可以作为多模态领域通用的数据清洗模型)

**Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation**

**链接**

**意义**

BLIP是 ALBEF 原班人马做的，基本可以看做吸收了 VLMo 思想的 ALBEF。训练的 loss 和技巧都与 ALBEF一致，属于 ALBEF 的后续工作。

研究动机：

模型：最近要么用transformer encoder，要么用SimVLM**，encoder only的模型很难用在文本生成任务，encoder decoder模型没有统一的框架很难做image-text retrieval 任务。**
数据层面：CLIP,ALBEF,SIMVLM数据都是爬的，存在**noisy**，使用起来不好。如何更好利用？设计了**captioner（生成好的描述性文本）和filter**可以选择GT和cap的文本选个好的，以训练出更好的模型。

BLIP 的两个关键点都包含在标题内，一是 **bootstrapping**，是数据方面的改进，指的是用含噪声的数据训练出模型后，再用某些方法**得到更干净的数据**，用这些干净的数据**训练出更好的模型**；二是 **unified**，指的是 BLIP 作为一种 encoder-decoder 架构，不只能做 understanding 类的任务（如上一节介绍的下游任务），也能做 **generation** 类的任务，如图像字幕 image captioning。

![img](https://img-blog.csdnimg.cn/direct/2bb0b18bac6048dfa977c9a8d33e80ec.png)

**模型概述**

![img](https://img-blog.csdnimg.cn/direct/35b8faa4ac794e0aa288b5c414d22ac7.png)

BLIP 的模型结构和目标函数如上图所示，图中**相同的颜色表示相同的参数**（共享参数，思想来自VLMo）。

细分开来，BLIP 模型共包含四个网络。图中左侧是一个**标准的 ViT 模型，用于处理图像数据**。右侧三个网络都用于处理**文本数据**，但他们的细节有所不同，细节如下：

 **Text Encoder**，提取文本特征，用于与视觉特征计算 ITC 损失。Text Encoder 不与视觉特征计算交叉注意力。**文本输入是CLS token**
**Image-gounded Text Encoder**，与视觉特征计算交叉注意力，提取文本特征用于计算 ITM 损失。**文本输入是Encode token**
 **Image-gounded Text Decoder**，与视觉特征计算交叉注意力，用于进行 LM （是GPT的LOSS，而不是Bert的LOSS即MLM）语言建模训练。为了进行语言建模训练，需要 mask 掉后面的单词。因此该网络的注意力层是 Causal SA，而非 Bi SA。**文本输入是Decode token**
        BLIP模型是ALBEF模型原班人马做的，所以与 ALBEF 一样，**同样采用动量模型为 ITC 生成伪标签**；**同样使用 ITC 为 ITM 进行难负例（Hard Negative）挖掘**。

​    BLIP 的整个模型称为**MED**（Mixture of Encoder and Decoder）。虽然看起来模型很多，但实际上大部分网络是共享参数的，因此实际模型参数增加并不多。和ALBEF的区别是 **fusion的text是直接输入的，不是从文本的encoder获取的**，因为参数共享，不用劈成两份了，第一个文本编码器和第二个文本编码器基本一样。

**除了模型结构的创新之外，BLIP 的另一个贡献在数据清洗方面**，其方法流程如图 9 所示。图中 *I , T*  分别表示图像数据和文本数据；红色、绿色字体分别表示噪声较大、较小的文本；下标 *h , w , s* 分别表示人工标注数据、网络数据和模型生成数据。

BLIP 先使用含噪声的数据训练一个 MED 模型，然后将该模型的 Image-grounded Text Encoder 和 Image-grounded Text Decoder 在人工标注的 COCO 数据集上进行微调，分别作为 Filter 和 Captioner。**Captioner 为图像数据生成对应的文本，FIlter 对噪声较大的网络数据和生成数据进行过滤清洗**(**算余弦相似度**，淘汰不匹配的图文对)，得到较为可靠的训练数据。

为什么有Captioner呢？BLIP（decoder）训练出来的图像文本对**比原始的图像文本对的文本描述更贴切，质量更高，更匹配**。然后就有了Captioner去训练出质量更高的图像文本对。

**经数据清洗后，D(data)由两项(CC12M+COCO)变为三项(CC12M-F即过滤后的，CC12M-S即新生成的CC12M文本对，COCO，即人工标注的文本对)**，再根据这些可靠的训练数据，训练更好地 MED 模型，从而实现 bootstraping 训练。

 BLIP的特点：最显著的特点就是解码器输出的 caption 确实是不错，以至于很多下游任务都拿 BLIP 来生成 caption。实际上，BLIP 中 Captioner + Filter 的数据处理策略可以为任何需要图像文本对来训练的模型进行数据生成和清洗，**可以视作为多模态学习领域的一个通用的数据处理工具**。如 lambdalabs 用 BLIP 生成宝可梦图像的文本描述、LAION 用 BLIP 进行数据集清洗等。

![img](https://img-blog.csdnimg.cn/direct/f73f5a7adbec483b8f2ae622b88537d4.png)



#### CoCa（一次迭代只前向一次）

[CoCa: Contrastive Captioners are Image-Text Foundation Models](https://arxiv.org/pdf/2205.01917)

**链接**

**意义**

它也是 ALBEF 的后续工作，模型非常像。区别在于：

​		图像用了 attentional pooling（**可学习的pooling**），这在本文的实验中有效
​		**去掉了 ITM loss**，目的是加快训练，原本文本需要 forward 2-3 次，去掉 ITM loss 之后只需要 forward 一次就可以了。
​				在 ALBEF 中，ITM 需要完整的 text，而 MLM 需要掩码，所以是两次输入。
​				在 BLIP 中，ITC 一次，ITM 因为在文本模型中插入了新的模块，所以得单独做forward。而 LM 因为用了既多了新的模块又得用 causal self-attention 所以又得单独做一次。
​				在 CoCa 中，为了完成captioning loss(LM)和ITC loss，只需要做一次forward即可。GPT 中把 cls-token 放在最后面就可以得到全局表征来做 ITC loss 了。

**注意这里长度cls token是放在句子的后面的，这样mask的时候这个token就能看到全部句子了**。这里与ALBEF不同

![在这里插入图片描述](https://img-blog.csdnimg.cn/19899d275f064d3695ba8140ff868278.png#pic_center)

**模型概述**

CoCa（Contrastive Captioning），看方法名称就能猜测出它是**使用对比损失(ITC loss)和文本生成损失(LM loss)进行训练**，实际上也的确如此，CoCa 的模型框架和目标函数如图 所示。

从上图可以看出来，CoCa使SimVLM和ALBEF一个非常直接的后续工作。从结构上看来，CoCa 与 ALBEF 十分接近，都是左侧网络处理图像，右侧网络劈开，前半段处理文本，后半段进行多模态交互。与 ALBEF 最大的不同在于：

CoCa 左侧处理文本和进行多模态交互的网络是一个**文本解码器**（Text Decoder）而非文本编码器。目标函数为 ITC 对比损失和文本解码器的语言建模损失 。使用文本解码器，模型能够处理生成式多模态任务（如 image captioning）。

并且，CoCa 在图像编码器的最后使用**可学习**的attention pooling进行降采样。

另外，CoCa 没有使用 ITM 损失，**减少了模型参数每次迭代所需前向传播的次数**，大大降低了训练时间。输入从刚开始就是经过了causal SA,即mask住后面的部分。这样经Unimodal Text Decoder处理后的特征既可以直接算ITC LOSS,也可以直接算LM LOSS。

![image-20240426090419140](.\image-20240426090419140.png)



#### BEiTv3(更大一统的框架)

**链接**

**意义**

BEITv3 的关键词就是**大一统（big convergence）**，输入形式大一统，目标函数大一统，模型大一统。**统一的multi-way transformer (mixture of experts ) 架构和单个masked modeling loss，将任意模态看做是同一个模态来建****

BEITv3 将图像也视作一种语言**（Imglish），与文本输入（English），图像文本对输入（parallel sentence）一起，实现了输入形式的大一统。在输入形式统一之后，也不需要 ITC、ITM、MLM、WPA 等其他目标函数，而是可以使用**统一的 mask language modeling (MLM)来驱动训练**。模型层面上，自从 ViT 在视觉领域取得成功之后，Transformer 架构已有一统多模态模型的趋势。虽然在纯视觉领域，CNN 与 Transformer 谁更适合至今尚无定论，但如果要实现多模态模型大一统，Transformer 无疑更加适合。BEITv3 使用本组之前工作 VLMo 中提出的 **MoME（本文中称为 Multi-way Transformer），对不同模态使用不同的专家 FFN，实现统一**。

BEITv3 的性能对比实验直接放在了论文的第一页。同样采用了 CoCa 中的多边形图，展现出了 BEITv3 相较于现有工作的巨大提升。除了多模态的任务之外，BEITv3 还可以做语言、视觉单模态的任务，图中展示了语义分割、图像分类、目标检测等视觉任务，但没有展示语言单模态的性能。可以看到，BEITv3 相较于 CoCa，也实现了全面的领先。并且，BEITv3 使用的训练数据量是远小于 CoCa 的，并且全都是公开数据集（CoCa 使用了谷歌自家的 JFT-3B）。BEITv3 的大一统框架取得了巨大的领先。

![img](https://img-blog.csdnimg.cn/direct/9efb1917cdd840faa84d12d64ffe8e53.png)

**模型概述**

![img](https://img-blog.csdnimg.cn/direct/7d0db575a38b409d9699c1ba7affd3f1.png)

**模型结构就是之前VLMo 中的 MoME，自注意力层权重共享，根据不同的输入来选择不同的 FFN 专家。**与 VLMo 不同之处在于训练的目标函数，是大一统的 masked data modeling，即遮住部分数据，要求模型还原出被遮住的数据即完形填空，BERT。

​		multiway transformer：VLMo 的模型 MoME。该网络的 transformer block 中的自注意力层是共享的，而 FFN 层（模态专家）则有三种，分别针对文本、图像、图文，当接收不同类型的输入数据时，数据会通过对应的 FFN 层进行计算。
​		masked data modeling：**BEiTv3 在单模态、多模态数据上，通过一个统一的掩码数据建模任务进行训练。**在训练时，随机掩码掉一定比例的 token，然后训练模型恢复出被掩码的 token。统一的掩码数据建模不仅能够学习数据的表示，还能学习对不同模态数据进行对齐。BEiTv3 中，使用 SentencePiece 对序列数据进行 tokenize，使用 BEiTv2 中使用 VQ-KD 训练得到的 tokenizer 对图像数据进行 tokenize（得到离散的视觉 token），作为重构目标。



大一统的 BEITv3 具有极高的灵活性，可以处理视觉、文本各自单模态以及视觉文本多模态的各种任务。BEITv3 用于各种下游任务的示意图如图 14 所示。(a)、(b) 中，仅使用视觉编码器或文本编码器，BEITv3 可以处理视觉、文本各自领域的单模态任务；© 中，使用视觉编码器和文本编码器提取特征之后，再经过多模态交互，相当于 Fusion Encoder 多模态模型，适合于处理推理类多模态任务；(d) 中，分别使用视觉编码器和文本编码器提取特征之后计算相似度，相当于 Dual Encoder 多模态模型（CLIP），适合于处理检索类多模态任务；(e) 中，将输入文本 mask 掉，可用于 image captioning 这种生成类多模态任务（把下一个词MASK掉）。就像搭积木一样，大一统的 BEITv3 模型可处理视觉、文本领域各类任务。

![在这里插入图片描述](https://img-blog.csdnimg.cn/d891bc8c684b4c69846e6c4a830a972a.png#pic_center)

### 多模态总结

![在这里插入图片描述](https://img-blog.csdnimg.cn/4d9fd1badfdf4848b9c349920197a64c.png#pic_center)

### 多模态2024上半年进展

​	最新流行的MLLM架构大多采用类LLaVA的**ViT+MLP+LLM**范式

#### **LLaVA-NeXT系列**

##### LLaVA-1.5

23年10月，LLaVA-1.5发布，通过在视觉和语言模态间**添加简单的MLP层**实现了[训练样本](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=训练样本&zhida_source=entity)高效性，为多模态大模型在低数据业务场景的落地提供了可能。

[[2310.03744\] Improved Baselines with Visual Instruction Tuning](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2310.03744)

![img](https://pica.zhimg.com/v2-86fe5dd85d0a10c32de5de6f301c4348_1440w.jpg)

##### LLaVA-NeXT

24年1月，LLaVA-NeXT(1.6)发布，在1.5的基础上保持了精简的设计和数据高效性，支持更高的分辨率、更强的视觉推理和OCR能力、更广泛场景的视觉对话。模型分为两阶段训练：**阶段1预训练只训练连接层，阶段2[指令微调](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=指令微调&zhida_source=entity)训练整个模型。**

[LLaVA-NeXT: Improved reasoning, OCR, and world knowledge](https://link.zhihu.com/?target=https%3A//llava-vl.github.io/blog/2024-01-30-llava-next/)

![img](https://pic2.zhimg.com/v2-a63729c2ceab8fffaaad1a383e32c317_1440w.jpg)

- **动态高分辨率AnyRes**：如上图，为了让模型能感知高分辨率图像的复杂细节，对图像进行[网格划分](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=网格划分&zhida_source=entity)。比如，对于672x672的图像，一方面按2x2的网格切分为4张336px的输入图像送给ViT编码成特征，另一方面将图像直接resize到336px进行编码，最后将两部分特征合并输入到LLM中，这样模型具备了全局和局部的视觉推理能力。
- **指令数据混合**：一方面保证指令数据具有高质量、多样性，反映真实场景的广泛用户意图；另一方面，补充文档和表格数据，提升模型的OCR和图表理解能力。
- **扩大LLM尺寸**：考虑了7B、13B、34B的LLM。

24年5月，团队发布基于更强LLM的LLaVA-NeXT版本，支持LLaMA3(8B)和Qwen1.5(72B/110B)。更大的LLM提供更好的视觉[世界知识](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=世界知识&zhida_source=entity)和[逻辑推理](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=逻辑推理&zhida_source=entity)能力，最大的模型接近GPT-4V的性能，同时保证了训练高效性。

[LLaVA-NeXT: Stronger LLMs Supercharge Multimodal Capabilities in the Wild](https://link.zhihu.com/?target=https%3A//llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/)

##### LLaVA-NeXT-Video

24年4月，LLaVA-NeXT-Video发布，展现出强大的[zero-shot](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=zero-shot&zhida_source=entity)视频理解能力。LLaVA-NeXT中的高分辨率图像动态划分可以很自然地迁移到视频模态用来表示视频的多帧，使得只在图文模态上训练的LLaVA-NeXT能在视频任务上泛化。此外，推理时的长度泛化用于有效处理超出LLM最大长度的长视频输入。基于LLaVA-NeXT-Image模型，作者发布了在视频数据上监督微调的LLaVA-NeXT-Video，以及在AI反馈的监督下使用[DPO](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=DPO&zhida_source=entity)偏好对齐的LLaVA-NeXT-Video-DPO。使用SGLang部署和推理，支持可扩展的大规模视频推理。可以想到，这有助于海量视频的高效文本标注，催生了未来更强大视频[生成模型](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=生成模型&zhida_source=entity)。

[LLaVA-NeXT: A Strong Zero-shot Video Understanding Model](https://link.zhihu.com/?target=https%3A//llava-vl.github.io/blog/2024-04-30-llava-next-video/)

![img](https://pic4.zhimg.com/v2-8419f55dd7eba5d1785876691fa6d527_1440w.jpg)

- **AnyRes**：可以将N帧视频看作{1xN}的网格，而LLM的最大长度限制了可以处理的帧数，很自然地会考虑对图像进行[下采样](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=下采样&zhida_source=entity)减少每帧token数，但作者发现为保证效果仍只能处理16帧。
- **长度泛化**：基于LLM的[长度外推技术](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=长度外推技术&zhida_source=entity)（[RoPE](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=RoPE&zhida_source=entity)的线性扩展），推理时扩展2倍，从之前的16帧扩展到56帧，大大提升了模型分析长视频序列的能力。
- **基于LLM反馈的DPO偏好优化**：偏好数据由LLM生成，视频表示为详细的说明文字，带来了很大的性能增益。
- 对于视频数据的微调，作者进行了**ablation study**：(1) 在LLaVA-NeXT图像级指令微调后，继续在视频级指令上增量微调；(2) 在LLaVA-NeXT图像级预训练后，在图像级和视频级数据联合微调，每个batch数据包含一种类型或者混合两种类型，实验表明混合图像和视频模态数据效果最佳。

###### 指令微调Ablation Study


团队还分享了[视觉指令](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=视觉指令&zhida_source=entity)微调过程中除数据之外的因素的ablation study，从模型架构、视觉表征、训练策略角度进行分析。

[LLaVA-NeXT: What Else Influences Visual Instruction Tuning Beyond Data?](https://link.zhihu.com/?target=https%3A//llava-vl.github.io/blog/2024-05-25-llava-next-ablations/)

- **模型架构**：扩展LLM比扩展视觉编码器更有效，视觉输入配置（分辨率、token数）比视觉编码器大小更关键。
  - 学习率：为了训练更稳定，[视觉编码器](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=3&q=视觉编码器&zhida_source=entity)的学习率通常应该**比LLM学习率小10倍～5倍，更大的LLM需要更小的学习率，尽量避免loss跑飞**。
  - 视觉编码器：相较于模型大小，基于分辨率、token数的视觉特征支持编码更多的视觉细节，预训练数据支持编码更多的视觉知识，作用更重要。
- **视觉表征**：分辨率、特征[空间视觉](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=空间视觉&zhida_source=entity)token数都重要，相对来说扩展分辨率更有效，建议使用AnyRes时下采样。
  - 对于更高分辨率图像或者更长的视频，AnyRes需要更多的格子。比如，对于超过768x768的图像，以前的方案首先resize到768x768会导致细节丢失。这里考虑划分成更多的格子，然后对编码的特征进行[双线性插值](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=双线性插值&zhida_source=entity)（下采样）到更小的特征，以防止视觉token数过多。
- **训练策略**：在互联网级低质数据上大规模预训练后，指令微调前，增加一个阶段，使用一些**高质量合成数据增强知识**。

![img](https://pic2.zhimg.com/v2-7ca3715e81f0d5239970b7454dbf7b31_1440w.jpg)



##### **LLaVA-NeXT-Interleave**

24年6月，LLaVA-NeXT-Interleave发布，提出**图文[交错格式](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=交错格式&zhida_source=entity)**可以作为通用模版统一不同的视觉模态，比如单图像([multi-patch](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=multi-patch&zhida_source=entity))、多图像(multi-image)、视频(multi-frame)、3D(multi-view)。在保证LLaVA-NeXT单图像输入的性能下，可以提高其它模态任务的性能，而且在不同模态任务上具有初步的迁移能力。这种大一统的模型支持更广泛真实场景的应用，比如多页PPT的总结和问答、生成图像编辑的提示词、多文档的汇总和比较。

[LLaVA-NeXT: Tackling Multi-image, Video, and 3D in Large Multimodal Models](https://link.zhihu.com/?target=https%3A//llava-vl.github.io/blog/2024-06-16-llava-next-interleave/)

作者在训练策略上进行了ablation study：

- 从LLaVA-NeXT单图像模型继续训练，从stage2单图像指令微调后的模型开始训练效果更好，可以继承单图像任务的指令遵循能力。
- **两种组织格式**：将所有图像token放在最前面，在文本中使用特殊token指代图像 (in-the-front);将图像token放在其原来的位置，与文本**交错** (interleaved)。实验表明，在训练阶段混合两种格式有助于在推理阶段这两种格式都取得更好的性能。



#### InternVL系列

##### InternVL-1.0

23年12月，[上海AI Lab](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=上海AI+Lab&zhida_source=entity) [@OpenGVLab](https://www.zhihu.com/people/00500f88e7a8ea5ac6d3b48599227457)

发布InternVL。该工作在[模态对齐](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=模态对齐&zhida_source=entity)中视觉编码器和LLM之间在参数规模和特征表征能力上存在较大的差距，自然地提出扩大视觉端的参数量到6B (InternViT-6B)，然后使用不同质量的图文数据逐渐与LLM对齐。此外，连接层的参数量也扩大了，**类似Q-Former**，这里设计了一个8B的语言中间件QLLaMA，使用Chinese-LLaMA的参数初始化增强其跨语言理解能力，新增96个可学习query token和[cross-attention](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=cross-attention&zhida_source=entity)层 (1B)，实现视觉和语言模态进一步对齐。



[[2312.14238\] InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2312.14238)

下图是InternVL的三阶段渐进式训练策略，训练数据质量逐渐提高，最开始使用大规模有噪的图文对进行对比预训练 (类似[CLIP](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=CLIP&zhida_source=entity))，接着加入冻结参数的[QLLaMA连接件](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=QLLaMA连接件&zhida_source=entity)，只学习cross-attention，使用图文匹配/对比/生成loss (类似BLIP)，最后引入LLM进行监督微调，赋予多模态对话和问答能力。

![img](https://picx.zhimg.com/v2-dbe9b9c22a0f54852ad05d1b32ad2905_1440w.jpg)

InternVL训练的多阶段性赋予其内在的多功能性，通过灵活组合不同模块，可以支持各种视觉-语言任务，如下图。

![img](https://picx.zhimg.com/v2-59ea024832ebec7e2196fbe7bd1a204d_1440w.jpg)

这里值得讨论的一个点在于，InternVL为了让视觉端和语言端参数量平衡，对视觉端和连接层都进行了[scale up](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=scale+up&zhida_source=entity)。一个很自然的问题是，视觉端真的需要这么heavy的参数量吗？因为当前最新的LLaVA-NeXT仍然使用约300M的ViT和轻量的MLP连接层，仅通过扩展LLM提升多模态任务性能。我的个人拙见是，视觉理解包括感知和推理，感知部分可能并不需要那么大的参数量，而推理部分作用于[high-level](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=high-level&zhida_source=entity)的视觉特征，通过微调LLM赋予其理解推理视觉模态的能力，所以为了性能、效率和稳定性的平衡，似乎这里scale up必要性不是很强，当然这里值得深入实验的验证和讨论。看到这篇论文中的图，让我想到了22年Google的[Coca](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=Coca&zhida_source=entity)论文，作者把文本解码器按层对半划开，浅层一半用于文本单模态，深层一半用于图文多模态，可以看到下图视觉端参数量占比也相当高。

[[2205.01917\] CoCa: Contrastive Captioners are Image-Text Foundation Models](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2205.01917)

![img](https://picx.zhimg.com/v2-745cd2f9540ae29f643e92f3e9107e2f_1440w.jpg)

##### InternVL-1.5

24年4月，InternVL-1.5发布，综合性能更强，且支持推理时高达4K的分辨率。

[[2404.16821\] How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2404.16821)

![img](https://pic1.zhimg.com/v2-8b0e78ee02f82805fe409de1dd879632_1440w.jpg)

上图为模型整体架构，采用了类LLaVA的ViT+MLP+LLM范式，结合了增强的InternViT-6B-448px-V1.5和中英双语InternLM2-Chat-20B，总体参数约26B。相比于InternVL-1.0，在输入端支持了动态高分辨率，**连接层改为轻量的MLP**，使用[pixel shuffle](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=pixel+shuffle&zhida_source=entity)操作将输出的视觉token数减为1/4。训练分为两阶段，预训练阶段训练InternViT和MLP映射，随后微调整个模型。

- 这里不再使用Q-Former作为连接层的原因，可以参考作者[@Weiyun](https://www.zhihu.com/people/a760ef66079376468e2685723c318ee8)大佬的回答：

  [多模态大语言模型（MLLM）为什么最近的工作中用BLIP2中Q-Former结构的变少了？ - Weiyun的回答 - 知乎](https://www.zhihu.com/question/626796690/answer/3524920516)，大致意思是说相比于MLP，**Q-Former参数量大收敛更慢，数据量小的场景无法达到LLaVA-1.5这样的性能**，而且提高数据量和计算量，Q-Former也没有明显的性能优势。

- 这里的pixel shuffle操作来源于[16年的一篇论文](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1609.05158)，本质是对特征元素进行重排列，将 (C×r2,H,W) 的[特征变换](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=特征变换&zhida_source=entity)为 (C,H×r,W×r) ，对特征进行了空间维度的[上采样](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=上采样&zhida_source=entity)，但[通道维度](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=通道维度&zhida_source=entity)缩小为原来的 1/r2 。这里输出的视觉token数可以理解为通道数，主要目的是通过提升特征维度换取更少的token数，从而可以支持更高的图像分辨率。这样，448x448的输入图像，patch size=14，总共有32x32=1024个token，设置上[采样系数](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=采样系数&zhida_source=entity)r=2，则该图像可以表示为256个token。

接着我们来看InternVL-1.5的三个重要改进：

- **InternViT增强**：V1.2版本去掉了模型的最后3层，将分辨率扩展为固定448x448，而V1.5进一步扩展为动态448x448，即每张训练图像可分块，每块大小为448x448，支持1~12个块。此外，还增强了数据规模、质量和多样性，提高了OCR和高分辨率处理能力。
- **[动态高分辨率](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=3&q=动态高分辨率&zhida_source=entity)**：基于图像的分辨率和[纵横比](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=纵横比&zhida_source=entity)，将图像切分为448x448的分块，训练阶段最多12块，测试阶段可以外推到40块，即4K分辨率，这样模型训练和推理能适应多种分辨率和纵横比，避免了强行resize带来的失真和细节丢失。如下图，具体来说，对于一张800x1300的图像，从预定义的纵横比中匹配一个最接近的纵横比2:3，然后将图像resize到896x1344，并切分为多个448x448的图像块，再添加一个缩略视图 (直接resize到448x448) 用于图像全局理解。
- **高质量中英双语数据集**：包含自然场景、图表、文档、对话等多样化的数据，借助LLM实现数据集英文到中文的转换。

![img](https://pic2.zhimg.com/v2-32c4d73af4c907b7a4dcae549f3c3895_1440w.jpg)

此外，翻译的prompt值得我们学习：

```text
System:
You are a translator proficient in English and {language}. Your task is to translate the following English text into {language}, focusing on a natural and fluent result that avoids “translationese.” Please consider these points:
1. Keep proper nouns, brands, and geographical names in English.
2. Retain technical terms or jargon in English, but feel free to explain in {language} if necessary.
3. Use {language} idiomatic expressions for English idioms or proverbs to ensure cultural relevance.
4. Ensure quotes or direct speech sound natural in {language}, maintaining the original’s tone.
5. For acronyms, provide the full form in {language} with the English acronym in parentheses.
User:
Text for translation: {text}
Assistant:
{translation results}
```

作者在ablation study部分研究了更大的LLM是否需要更大的视觉编码器，实际上是针对我们上面对InternVL-1.0视觉端参数量的问题的实验。实验对比了LLaVA-NeXT和InternVL-1.2，两者都使用34B的LLM，在尽量保证对比公平的条件下，实验证明更大的视觉模型能提供模型解决多模态任务的整体性能（不过原论文好像没有给具体数据？）。团队后续也发布了蒸馏版的视觉模型[InternViT-300M-448px](https://link.zhihu.com/?target=https%3A//huggingface.co/OpenGVLab/InternViT-300M-448px)，与LLaVA-NeXT的视觉端保持了同等规模。



#### **MiniCPM-V系列**

[MiniCPM-V](https://link.zhihu.com/?target=https%3A//github.com/OpenBMB/MiniCPM-V)是 [@面壁智能](https://www.zhihu.com/people/3a3a76e5aad5af8b79491c17047c529e)

 发布的一系列支持高效端侧部署的[多模态LLM](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=多模态LLM&zhida_source=entity)。



##### **MiniCPM-V 2.0**

24年4月，MiniCPM-V 2.0发布，仅有2.8B参数，整体性能超过了[Yi-VL 34B](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=Yi-VL+34B&zhida_source=entity)、CogVLM-Chat 17B、Qwen-VL-Chat 10B等更大的开源模型，OCR能力突出，支持中英双语对话，部分指标接近[Gemini Pro](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=Gemini+Pro&zhida_source=entity)。
视觉编码器使用[SigLIP SO400M/14-384px](https://link.zhihu.com/?target=https%3A//huggingface.co/google/siglip-so400m-patch14-384)，LLM使用[MiniCPM-2.4B](https://link.zhihu.com/?target=https%3A//github.com/OpenBMB/MiniCPM)，连接层使用[Flamingo](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2204.14198)中的[Perceiver Resampler](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=Perceiver+Resampler&zhida_source=entity) (**类似Q-Former使用可学习query提取显著视觉信息，但不以输入文本为条件**)。基于自研的[RLHF-V](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2312.00849)实现可信行为对齐，在缓解多模态幻觉问题上接近GPT-4V。基于自研的[LLaVA-UHD](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2403.11703)支持高达1344x1344的分辨率和任意纵横比输入。基于自研的[VisCPM](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2308.12038)实现跨语言的多模态能力泛化，进而有良好的中英双语能力。此外，该模型在端侧部署内存开销较小、速度较快，即便是处理高分辨率的图像。官方还提供了安卓端部署的[mlc-MiniCPM](https://link.zhihu.com/?target=https%3A//github.com/OpenBMB/mlc-MiniCPM)示例。

##### **MiniCPM-Llama3-V 2.5**

24年5月，MiniCPM-Llama3-V 2.5发布，总共8B参数，整体性能超过了GPT-4V-1106、Gemini Pro、Qwen-VL-Max、[Claude 3](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=Claude+3&zhida_source=entity)等闭源模型，OCR和指令遵循能力进一步增强 (增强了全文本OCR提取、表格到[Markdown](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=Markdown&zhida_source=entity)转换等功能)，支持超过30种语言对话，在量化、编译优化、高效推理等加持下，同样可以在端侧高效部署。
在MiniCPM-V 2.0基础上，LLM替换为[Llama3-8B-Instruct](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=Llama3-8B-Instruct&zhida_source=entity)，基于更新的[RLAIF-V](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2405.17220)进一步降低幻觉率。当前，官方支持了llama.cpp和[ollama](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=ollama&zhida_source=entity)的高效CPU推理、GGUF 16-bit量化、[LoRA微调](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=LoRA微调&zhida_source=entity)等实用功能。

![img](https://pic4.zhimg.com/v2-871c62a743a63f2a7857eff7dddfdee7_1440w.jpg)



#### **VILA1.5**

24年5月，[NVIDIA](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=NVIDIA&zhida_source=entity)发布[VILA1.5](https://link.zhihu.com/?target=https%3A//github.com/NVlabs/VILA)，提供视频理解能力，开源了3B/8B/13B/40B的模型，位于当前开源榜单MMMU和[Video-MME](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=Video-MME&zhida_source=entity)前列。VILA详见我的上篇文章，这里简单回顾一下：VILA在**大规模交错图文数据**上预训练，从而具有多图理解能力，作者通过实验发现：(1) 图文交错排布比较关键；(2) 交错图文预训练过程中微调LLM能赋予其上下文学习的能力；(3) 混合只有文本的指令数据有助于提升性能；(4) 压缩视觉token可以扩展视频帧数。

#### **CogVLM2**

24年5月，[智谱](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=智谱&zhida_source=entity) [@GLM大模型](https://www.zhihu.com/people/f5eb40ad7b1d6fffd4eb8cf21654389b)发布[CogVLM2](https://link.zhihu.com/?target=https%3A//github.com/THUDM/CogVLM2)

，随后发布了GLM-4V。CogVLM2基于Llama3-8B-Instruct，支持8K上下文、1344x1344分辨率、中英双语对话。GLM-4V-9B替换为GLM-4-9B语言模型，采取同样的数据和训练策略，去除CogVLM原有的视觉专家，将模型大小减为13B。CogVLM和CogAgent详见我的上篇文章。



#### **Cambrian-1**

24年6月，LeCun&谢赛宁团队发布Cambrian-1，关注以视觉为中心的多模态LLM，开源了8B/13B/34B的模型。当前多模态LLM仍存在较大的视觉缺陷，**需要增强视觉表征以更好地和语言模态交互**，赋予模型在真实场景更强的感知定位能力。这项研究的一大意义在于影响多模态LLM的工作开始重视视觉表征质量的提升，而非一直[scale up LLM](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=scale+up+LLM&zhida_source=entity)。

[[2406.16860\] Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2406.16860)

![img](https://picx.zhimg.com/v2-6d3b85f75dc201825d461b2102e704b7_1440w.jpg)

如上图，该工作围绕多模态LLM的5个核心设计要素展开研究，分别是：视觉表征、连接器设计、指令微调数据、指令微调策略、评估基准。

1. 视觉表征

作者评估了多种视觉编码器及其组合，下图表明以语言监督的[CLIP模型](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=CLIP模型&zhida_source=entity)优势较强，但自监督方法在提供充足数据和适当微调的情况下性能也能接近。而且，结合多种类型的视觉编码器有助于提升多模态LLM的性能，尤其是以视觉为中心的任务。注意到，高分辨率的编码器大大增强了图表和以视觉为中心任务的性能，而基于[ConvNet](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=ConvNet&zhida_source=entity)的架构适合处理这类任务。

![img](https://pic3.zhimg.com/v2-670cf5761a0f8832e5f040bc275b237c_1440w.jpg)

2. 连接器设计

提出**Spatial Vision Aggregator (SVA)，一个动态的、具备空间感知的连接器，**以将 (**来自多个视觉编码器的**) 视觉特征与LLM深度融合。如下图，该方法设置一些可学习的latent query tokens，通过cross-attention与多个视觉特征交互 (视觉特征作为key/value)。SVA的设计有两点要素：(1) 通过显式定义每个query token对应的视觉特征图子区域，引入空间inductive bias，便于模型在处理视觉信息时保留对空间结构的理解，更准确地定位和整合局部特征；(2) **在LLM的多层聚合视觉特征**，**让模型在不同层级特征上反复利用视觉信息**，增强模型对视觉内容的深入推理能力。**该方法可以有效减少需要的视觉token数**，例如相比于Mini-Gemini和LLaVA-NeXT，Cambrian-1的视觉token数是其20%。![image-20241107193249858](.\image-20241107193249858.png



![img](https://picx.zhimg.com/v2-20d290260aba984c5e5d91bd6540e8c1_1440w.jpg)

图中展示了Spatial Vision Aggregator (SVA) 的设计，旨在整合多个视觉编码器的特征，并防止插值引入的信息丢失。其核心思路是使用一组可学习的潜在查询通过交叉注意力层与多个视觉特征进行交互。具体来说，这种方法包含两个新的视觉中心设计原则：

1）通过明确定义查询中每个token的聚合空间，**引入空间归纳偏差**。

2）在LLM层之间**多次聚合视觉特征**，使模型能够反复访问和整合必要的视觉信息。

这种新公式灵活地适应具有不同特征分辨率的多个视觉编码器，同时在聚合过程中保持视觉数据的空间结构，并将其与LLM集成。![image-20241107165630484](.\image-20241107165630484.png)
![image-20241107165800509](.\image-20241107165800509.png)

虽然作者的方案有效地聚合了来自多个视觉编码器的特征，但对于高分辨率输入（较大的）或多个视觉编码器（较大的N NN）来说，仍然存在潜在的信息丢失问题。在这种情况下，单个token在聚合期间需要处理大量的上下文信息。为防止这种情况，**作者允许在整个LLM层中多次进行交叉注意力，从而允许持续访问未压缩的视觉信息**（见图8-右）。

为了灵活调节容量，作者引入了两个超参数D和G，它们分别表示交叉注意力层的数量和在视觉模型和LLM之间使用的可学习查询的不同组。直观上，更大的D允许更多堆叠的交叉注意力操作以促进聚合过程，而更大的G允许捕获更广泛的聚合模式。这些查询组分别并行地聚合视觉信息，然后连接形成LLM的最终视觉tokens。在LLM层内的交叉注意力层中，D和G始终设置为1。

作者通过使用四个视觉编码器的最佳组合结果和Vicuna-1.5-7B基本LLM展示了SVA模块的有效性。具体来说，作者使用了四个视觉编码器的组合：OpenAI CLIP ViT-L/14@336, SigLIP ViT-SO400M/14@384, OpenCLIP ConvNeXt-XXL@1024和DINOv2 ViT-L/14@518。作者将作者的方法与两个强大的基线进行比较：

1）基于连接的

2）重采样器

它们利用了类似的交叉注意力形式，但缺乏空间归纳偏差和多层次的视觉聚合。作者包括两个SVA模块的变体。标准的“SVA”使用D=3，G=1，并在LLM内部插入交叉注意力块，层步长为3。为了隔离空间归纳偏差的优势，作者团队还提出另一个SVA变体“SVA-no-multi-agg”，它不在LLM内部添加交叉注意力块，并将D设置为3，G设置为3。表4显示，SVA在所有基准类别中都优于两个基线，在需要高分辨率特征理解的OCR和图表类别中显著提升。相比之下，缺乏空间归纳偏差的重采样器在通过全局交叉注意力将来自各种视觉塔的连接tokens压缩到有限数量的可学习查询中时遇到了困难。

作者通过进一步的消融实验，以评估不同设置对高分辨率视觉理解的影响。研究使用了OpenAI CLIP ViT-L/14@336和OpenCLIP ConvNeXt-L/1024作为基础模型组合，重点关注OCR和图表类别。结果表明，通过增加超参数D或G的容量，可以提高性能，并且在LLM内部增加交叉注意力层、允许跨多层次的视觉聚合也能增强性能。更详细的实验设置和分析在附录F中提供。


3. 指令微调数据

作者发布了指令微调数据集Cambrian-10M，综合了OCR、通用VQA、纯语言等指令数据，还筛选了质量更高的7M版本。不同类型的视觉指令数据能赋予模型不同的能力，因此数据配比的平衡性也很关键，实验结果表明，平衡OCR、通用数据和语言数据的比例很重要。此外，在实验中作者发现，训练好的多模态LLM可能在基准测试上指标表现好，但实际对话能力弱，回复简短。因此，作者在训练期间引入了额外的系统提示，鼓励模型输出更长的回答和思维链推理，增强数学推理等任务的表现。

4. 指令微调策略

作者遵循LLaVA的两阶段训练策略，**先使用适配数据只微调中间的MLP连接层，再打开LLM和连接器微调**。结果表明，第一阶段对连接器的预训练可以提高性能，而使用更多的适配数据可以进一步增强。此外，作者对比了是否微调视觉编码器带来的性能影响，表明微调视觉编码器能增强性能，尤其对自监督预训练的视觉编码器 (如DINO v2、MoCo v3、[MAE](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=MAE&zhida_source=entity)等)，在以视觉为中心的测试上提升明显。

5. 以视觉为中心的基准CV-Bench

现有多数benchmark无法正确评估模型的[视觉感知](https://zhida.zhihu.com/search?content_id=245052246&content_type=Article&match_order=1&q=视觉感知&zhida_source=entity)定位能力，而且相应的样本数量有限。CV-Bench重新利用现有视觉benchmark中的样本，包含2638个以视觉为中心的VQA问题，涉及2D的空间位置关系和物体计数、3D的深度次序和相对距离。



多模态大型语言模型（MLLM）最近已成为一个新兴的研究热点，它将强大的大型语言模型（LLMs）作为大脑来执行多模态任务。MLLM 的惊人新能力，如基于图像撰写故事和无 OCR 的数学推理，在传统方法中很少见，这表明了通向通用人工智能的潜在路径。

通常人们会在 pair 数据上进行大规模（相对于 instruction tuning）的预训练，以促进不同模态之间的对齐。对齐数据集通常是图像文本对或自动语音识别（ASR）数据集，它们都包含文本。

更具体地说，图像文本对以自然语言句子的形式描述图像，而 ASR 数据集包含语音的转录。对齐预训练的常见方法是保持预训练模块（例如视觉编码器和 LLMs）冻结，并训练一个可学习的接口，本文调研了到近期位置不同的接口设计以及学习方法相关的文章。







![图片](https://i-blog.csdnimg.cn/blog_migrate/bb5c314d96fb53f0984510a24dedc38c.png)

#### **Flamingo**

**论文标题：**

Flamingo: a Visual Language Model for Few-Shot Learning

**论文链接：**

https://arxiv.org/abs/2204.14198



![图片](https://i-blog.csdnimg.cn/blog_migrate/fa3379cfa7955e881b5b981ecf1b612f.png)

总的来说，首先，**Perceiver Resampler** 接收来自视觉编码器的时空特征（从图像或视频获取），并输出固定数量的视觉标记。其次，这些视觉标记用于通过新初始化的交叉注意力层对冻结的语言模型进行条件化，这些层被插入到预训练的语言模型层之间。这些新层为语言模型提供了一种表达方式，以便将视觉信息纳入到下一个标记预测任务中

###### Visual processing and the Perceiver Resampler

**视觉编码器**：是一个预训练并冻结的 Normalizer-Free ResNet（NFNet），使用 Radford 等人提出的 two-term contrastive loss，在图像和文本对数据集上对视觉编码器进行对比目标的预训练。使用最终阶段的输出，即一个二维空间网格的特征，将其压平为一个一维序列。

对于视频输入，帧以 1 FPS 进行采样并独立编码，以获得一个三维时空特征网格，然后将学习到的时间嵌入添加到其中。特征然后被压平为一维，然后输入到 Perceiver Resampler 中。



![图片](https://i-blog.csdnimg.cn/blog_migrate/c9d12671fc608645cc84f2490f7a4d5f.png)

▲ Perceiver Resampler 模块将由 Vision Encoder **输出的可变大小的时空视觉特征网格映射到固定数量的输出标记**（图中为五个），与输入图像分辨率或输入视频帧数无关。这个 transformer 具有一组学习到的潜在向量作为查询，而键和值则是由时空视觉特征与学习到的潜在向量的连接组成。

**Perceiver Resampler**：从不同大小的大型特征图到少量视觉标记。这个模块将视觉编码器连接到冻结的语言模型，如上图所示。它以视觉编码器中的图像或视频特征的可变数量作为输入，并产生固定数量的视觉输出（64 个），从而降低了视觉-文本交叉注意力的计算复杂度。

类似于 Perceiver 和 DETR，本文学习了预定义数量的潜在输入查询，这些查询被输入到一个 Transformer 中，并对视觉特征进行交叉关注。消融研究中展示了使用这样一个视觉-语言重采样模块优于一个普通的 Transformer 和一个 MLP。

###### GATED XATTN-DENSE

Flamingo **将固定长度的视觉 query 注入到语言模型的方法称为 Gated xattn dense**，其详细结构示意图及伪代码如下图所示。具体来说，**在预训练好的 LM 的各层交替地插入一些随机初始化的交叉注意力层**。所谓 gated 门控，在每一新插入的层之后的残差链接之前添加一个 tanh gating，即 tanh(α) ，其中 α 是一个可学习的标量值，初始值为 0，**从而保证初始化时的输出与原 LM 一致**.使用文本信息作为query去aggregate视觉信息。其中text embedding作为query，visual embedding作为key和value。类比于transformer结构，唯一小的差别就是[cross-attention](https://zhida.zhihu.com/search?content_id=200937207&content_type=Article&match_order=1&q=cross-attention&zhida_source=entity)和FFN之后额外加了一个gate。

门控注意力单元的设计，则是在原先固定的LLM结构的每一层基础上叠加了门控单元，门控单元由交叉注意力机制和门控结构、FFW交替组成，**其中交叉注意力的k和v都是感知重采样器的输出，而q则是文本输入**。为了保证在训练初始阶段模型和原先的LLM不至于偏差太远，作者采用了门控机制，具体来说就是将新层的输出乘上一个可学习的tanh ⁡ ( α ) \tanh(\alpha)tanh(α)，将LLM的原先输入与其加和，只需要在初始化时候将α = 0 \alpha = 0α=0即可确保初始化时候和原先LLM无太大偏差。作者对在训练过程中每一LM层的α \alphaα变化进行了可视化，见Fig 3.可发现两个规律，**第一随着层数加深，门控值则更大，第二随着训练过程，门控值也逐渐变大**，这个倒是符合我们的认识，浅层提取基础特征而深层则更加富有语义信息，因此在深层中的门控更大有利于引入更多的视觉语义信息。


![fig1_framework](https://i-blog.csdnimg.cn/blog_migrate/c4333bbce76a3e16216748c2d202aee2.png#pic_center)



![fig2_pr_xattn](https://i-blog.csdnimg.cn/blog_migrate/7c9bbc616a6ecdbcb0166f06758aff2c.png#pic_center)

上图提供了一个 GATED XATTN-DENSE 块的示意图，以及它与一个冻结的 LM 块的连接方式，同时附上了伪代码。下图绘制了 Flamingo-3B 模型的 24 个 LM 层在训练过程中（从 0％ 到 100％）不同层中 tanh 门控值的绝对值的演变。**冻结的 LM 堆栈的所有层似乎都利用了视觉信息**，因为 tanh 门控的绝对值从其 0 初始化中迅速增长。

我们还注意到，绝对值似乎随着[深度](https://so.csdn.net/so/search?q=深度&spm=1001.2101.3001.7020)增加而增加。然而，从这个观察中很难得出强有力的结论：门控之前的激活的规模也可能随着深度变化。未来的工作需要更好地理解这些添加层对优化动态和模型本身的影响。



![图片](https://i-blog.csdnimg.cn/blog_migrate/a66cb585aadd43cc21e088f435b26d43.png)

###### Multi-visual input support



![图片](https://i-blog.csdnimg.cn/blog_migrate/94280c4dae214f2a6ca481a8a0b482ad.png)



▲ 首先通过在文本中的视觉数据位置插入 image 标签以及特殊标记 BOS 表示“序列开始”或 EOC 表示“块结束”）来处理文本。图像由 Vision Encoder 和 Perceiver Resampler 独立处理，以提取视觉标记。在给定的文本标记处，模型仅与最后一个前导图像/视频对应的视觉标记进行交叉关注。𝜑 指示文本标记可以关注的图像/视频，或者在没有前导图像/视频时为 0



上图说明了本文使用的 mask 方法，以限制某个文本标记看到的视觉标记数量。我们还对图像/视频和文本的交错序列的符号化进行了规范化。交错的视觉数据和文本序列。我们考虑交错的图像/视频和文本示例：每个示例包含一系列文本 𝑦，一系列图像/视频 𝑥，以及图像在文本中的位置序列。

基于视觉数据的位置，我们定义一个函数 𝜑 : [1, 𝐿] ↦ → [0, 𝑁 ]，它为每个文本位置分配最后一个出现在该位置之前的图像/视频的索引（或者如果该位置之前没有视觉数据，则为 0）。函数 𝜑 定义了我们考虑用于预测的标记 的可用视觉输入：前面的标记 .

1.4 训练细节

**1. 训练数据集**由不同格式的训练数据集混合而成。去除交错的图像文本数据集 M3W 导致性能下降超过 17%，而去除传统的配对图像文本对也会导致性能下降（下降 9.8%），这表明需要不同类型的数据集。



![图片](https://i-blog.csdnimg.cn/blog_migrate/af3bc4ed2f752ffe7cf8e968c35c5814.png)

**2. 冻结 LM 组件可以防止灾难性遗忘**。如果从头开始训练，我们观察到性能大幅下降了-12.9%。有趣的是，微调我们预训练的 LM 也导致了性能下降了-8.0％。

**3. 数据集加权**。M3W、ALIGN、LTIP 和 VTP，其权重分别为 1.0、0.2、0.2 和 0.03。这些权重是在小模型规模下经验性地获得的，并且在之后保持不变。



**消融实验**

是否采用全量数据？ 特别是对M3W图文交织数据的有无进行了消融，我们发现图文交织数据能提供大约17%的提升。
是否采用门控机制？**实验证明采用门控机制能带来月8%的提升**。
采用交叉注意力层的频率？实验证明**每一层都引入门控交叉注意力层**效果是最好的。
是否采用感知重采样单元引入视觉信息？实验证明该设计能带来约4%的提升。
视觉编码器的选择同样对结果影响巨大。
是否固定LLM的参数？**实验证明固定LLM反而能带来最好的效果**，而让LLM随着训练一起进行（会采用massive text数据集一起训练）反而效果会差8%左右，笔者估计是训练过程需要平衡多个目标导致的，如何让LLM也能训练起来可能也是一个值得关注的点。![fig6_ablation](https://i-blog.csdnimg.cn/blog_migrate/7eaa9e281c59630dd6f11c4d7399a03c.png#pic_center)





![图片](https://i-blog.csdnimg.cn/blog_migrate/63483f0fa499af079affa8beabaa4ef9.png)

### **BLIP-2**

**论文标题：**

BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models

**论文链接：**

https://arxiv.org/abs/2301.12597



![图片](https://i-blog.csdnimg.cn/blog_migrate/a158c79bc64f812de47e30e531d90d12.png)

▲ BLIP-2 框架概述。我们通过一个两阶段策略预训练轻量级的查询 Transformer，以弥合模态差距。第一阶段从冻结的图像编码器中引导视觉-语言表示学习。第二阶段从冻结的 LLM 中引导视觉到语言的生成学习，这使得零样本指导的图像到文本生成成为可能。

LLM 本质上是个语言模型，自然无法直接接受其他模态的信息。所以如何把各个模态的信息，统一到 LLM 能理解的特征空间，就是第一步要解决的问题。为此，作者提出了 Q-Former。



![图片](https://i-blog.csdnimg.cn/blog_migrate/cc47d368f8d7021818ebde56e0ce062d.png)

▲（左）Q-Former 和 BLIP-2 的第一阶段视觉-语言表示学习目标的模型架构。我们共同优化三个目标，**这些目标强制查询（一组可学习的嵌入）提取与文本最相关的视觉表示**。（右）每个目标的自注意力屏蔽策略，以控制查询-文本交互。

Learned Query 的引入在这里至关重要。可以看到这些 Query 通过 Cross-Attention 与图像的特征交互，通过 Self-Attention 与文本的特征交互。这样做的好处有两个：

1. 这些 Query 是基于两种模态信息得到的；
2. 无论多大的视觉 Backbone，最后都是 Query 长度的特征输出，大大降低了计算量。

比如在实际实验中，ViT-L/14 的模型的输出的特征是 257x1024 的大小，最后也是 32x768 的 Query 特征。针对 Q-Former 的三个训练任务分别是 Image-Text Contrastive Learning（ITC），Image-grounded Text Generation（ITG），Image-Text Matching（ITM）。

第一阶段，对于模型的训练，就是由以上三个任务组成（即训练Q-former），通过这几个任务，实现了对于特征的提取与融合。但现在模型还没见过 LLM。我们现在用传感器完成了数据的提取与融合，下一步，我们得把数据转换成处理器能识别的格式。



![图片](https://i-blog.csdnimg.cn/blog_migrate/bebb3eeda170ca7d5d3122e025bde434.png)

▲ BLIP-2 的第二阶段视觉到语言生成预训练，**从冻结的大型语言模型（LLM）中引导**。（顶部）引导基于解码器的 LLM（例如 OPT）。（底部）引导基于编码器-解码器的 LLM（例如 FlanT5）。全连接层从 Q-Former 的输出维度调整到所选 LLM 的输入维度。

通过第一阶段的训练，Query 已经浓缩了图片的精华，现在要做的，**就是把 Query 变成 LLM 认识的样子**。这里作者针对两类不同 LLM 设计了不同的任务：

Decoder 类型的 LLM（如 OPT）：以 Query 做输入，文本做目标；Encoder-Decoder 类型的 LLM（如 FlanT5）：以 Query 和一句话的前半段做输入，以后半段做目标；

为了适合各模型不同的 Embedding 维度，作者引入了一个 FC 层做维度变换。

训练细节

作为图文预训练的工作，工程问题往往是关键。BLIP2 的训练过程主要由以下几个值得关注的点：

1. 训练数据方面：包含常见的 COCO，VG，SBU，CC3M，CC12M 以及 115M的LAION400M中的图片。采用了BLIP中的CapFilt方法来 Bootstrapping 训练数据。
2. CV 模型：选择了 CLIP 的 ViT-L/14 和 ViT-G/14，特别的是，作者采用倒数第二层的特征作为输出。
3. 训练时，CV 模型和 LLM 都是冻结的状态，并且参数都转为了 FP16。这使得模型的计算量大幅度降低。主要训练的基于 BERT-base 初始化的 Q-Former 只有 188M 的参数量。







![图片](https://i-blog.csdnimg.cn/blog_migrate/5e66abe0c7544557009fa38274136085.png)

### **InstructBLIP**

**论文标题：**

InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning

**论文链接：**

https://arxiv.org/abs/2305.06500



![图片](https://i-blog.csdnimg.cn/blog_migrate/26fae4dc6fc4177b97a271c560988615.png)



▲ InstructBLIP 的模型架构。Q-Former 从冻结的图像编码器的输出嵌入中提取了指示感知的视觉特征，并将这些视觉特征作为软提示输入馈送给冻结的 LLM。我们使用语言建模损失对模型进行指令调整，以生成响应。



视觉编码器提取输入图片的特征，并喂入 Q-Former 中。此外，Q-Former 的输入还包括可学习的 Queries（BLIP-2 的做法）和 Instruction。Q-Former 的内部结构黄色部分所示，其中可学习的 Queries 通过 Self-Attention 和 Instruction 交互，可学习的 Queries 通过 Cross-Attention 和输入图片的特征交互，鼓励提取与任务相关的图像特征。

Q-Former 的输出通过一个 FC 层送入 LLM，Q-Former 的预训练过程遵循 BLIP-2 的两步：1）不用 LLM，固定视觉编码器的参数预训练 Q-Former 的参数，训练目标是视觉语言建模。2）固定 LLM 的参数，训练 Q-Former 的参数，训练目标是文本生成。

在推理的时候，对于大部分数据集，如 image captioning，open-ended VQA 等，InstructBLIP 可以直接使用 LLM 生成的文本作为输出；对于 classification 和 multi-choice VQA 这样的任务，InstructBLIP 遵循 ALBEF 的做法生成固定的几种答案，根据概率选择最后的结果作为输出。

这种做法的数据集包括有 ScienceQA、IconQA、A-OKVQA（多项选择）、HatefulMemes、Visual Dialog、MSVD 和 MSRVTT 数据集。

Tricks

**数据重采样**由于训练数据集数量太大，而且每个数据集的大小存在显着差异，均匀混合它们可能会导致模型过拟合较小的数据集，并欠拟合更大的数据集。因此，作者改了一下采样数据的概率，从某个数据集里面采样数据的概率是 ，其中 是单个数据集的大小。







![图片](https://i-blog.csdnimg.cn/blog_migrate/9b87df23da6d429d19db8537b8c3cb60.png)

### **LLaVA**

**论文标题：**

Visual Instruction Tuning

**论文链接：**

https://arxiv.org/abs/2304.08485

##### 4.1 数据构造

结合 GPT-4 优异的文字能力，将原始数据构造成结构化的文本信息作为 Context，同时通过 prompt template 请求 GPT-4 得到一些结果，来生成原始的 instruction data。在训练时，则可加入 visual token，以得到 align 后的 instruction-tuned model。



![图片](https://i-blog.csdnimg.cn/blog_migrate/4cfdbec7e8091e464ec64b216d1e7054.png)

训练分两步，第一步做对齐，只训 projection [layer](https://so.csdn.net/so/search?q=layer&spm=1001.2101.3001.7020)；第二步 e2e finetune，vision encoder（clip vit-L）是 freeze 的。可以看到 instruction tuning 对任务效果影响巨大，另外每个任务本身的指令数据也对各个任务都有互补作用



![图片](https://i-blog.csdnimg.cn/blog_migrate/81da88be4e6d5ed491d0545d069293e9.png)

▲ 使用不同训练数据在 LLaVA-Bench（COCO）上的消融实验。我们报告相对分数，相对于一个仅使用地面真实图像标题和边界框作为视觉输入的文本 GPT-4 模型。我们使用我们模型输出的答案和 GPT-4（仅文本）的答案来提示 GPT-4，并让它在两者之间进行比较并给出一个带有解释的评分。







![图片](https://i-blog.csdnimg.cn/blog_migrate/ef71896d96b243cde16c18239368cca2.png)

### **LLaVA-v1.5**

**论文标题：**

Improved Baselines with Visual Instruction Tuning

**论文链接：**

https://arxiv.org/abs/2310.03744

**Response formatting prompts**。我们发现，像 InstructBLIP 这样的方法无法很好地平衡短形式和长形式 VQA 的原因主要有以下几点。首先，是响应格式上的模糊提示。例如，Q: {问题} A: {答案}。这样的提示并不清楚地指示了期望的输出格式，甚至在自然的视觉对话中，**也可能使 LLM 在行为上过度拟合为短形式答案**。

其次，没有对 LLM 进行微调。第一个问题由于 InstructBLIP 只对 Qformer 进行了指导调整而进一步恶化。它需要 Qformer 的视觉输出令牌来控制 LLM 的输出长度，使其为长形式或短形式，就像前缀调整一样，但是 Qformer 可能缺乏正确执行此操作的能力，因为与 LLMa 等 LLM 相比，其容量有限。

为了解决这个问题，我们建议**使用一个单一的响应格式提示，清楚地指示输出格式**，在促进短答案时附加到 VQA 问题的末尾：用一个词或短语回答问题。我们经验证明，当 LLM 使用这样的提示进行微调时，LLaVA 能够根据用户的指示正确调整输出格式，并且不需要对 VQA 数据进行额外处理，这进一步实现了对各种数据源的扩展。

**Academic task oriented data** 我们进一步包括了额外的学术任务导向的 VQA 数据集，用于 VQA、OCR 和区域级感知，以各种方式增强模型的能力，如表 1 所示。

我们首先包括了 InstructBLIP 中使用的四个额外数据集：开放知识 VQA（OKVQA ，A-OKVQA ）和 OCR（OCRVQA ，TextCaps）。A-OKVQA 被转换为多项选择问题，并使用特定的响应格式提示：直接用给定选项的字母回答。

仅使用 InstructBLIP 使用的数据集子集，LLaVA 就在表 1 中的所有三个任务上都超过了它，表明 LLaVA 的有效设计。此外，我们发现进一步添加区域级 VQA 数据集（Visual Genome，RefCOCO）可以提高模型对细粒度视觉细节的定位能力。

**Additional scaling.** 进一步增加了输入图像的分辨率，以使 LLM 能够清晰地“看到”图像的细节，并将 GQA 数据集作为额外的视觉知识源。我们还加入了 ShareGPT 数据，并将 LLM 扩展到 13B，在 MM-Vet 上的结果显示了将 LLM 扩展到 13B 时的最显著的改进，表明了基础 LLM 能力对视觉对话的重要性。

**Limitations.\**。尽管 LLaVA-1.5 展示了令人期待的结果，但必须承认存在一些限制。首先，LLaVA 利用完整的图像补丁，可能会延长每个训练迭代的时间。虽然视觉重采样器可以减少 LLM 中的视觉补丁数量，但它们目前不能像 LLaVA 那样有效地收敛，可能是由于\**重采样器中的可训练参数更多**。

一个高效的样本重采样器的开发可以为未来扩展指导跟随多模态模型铺平道路。第二，由于缺乏这种指导跟随数据和上下文长度的限制，**LLaVA-1.5 目前还不能处理多个图像**。第三，尽管 LLaVA-1.5 在遵循复杂指令方面表现出了熟练，但其问题解决能力在某些领域仍然可能受到限制，这可以通过更有能力的语言模型和高质量、针对性的视觉指导调整数据来改善。

最后，尽管 LLaVA 的产生幻觉的倾向显著降低，但它**仍然可能产生幻觉并偶尔传播错误信息**，在关键应用（例如医学）中应谨慎使用。







![图片](https://i-blog.csdnimg.cn/blog_migrate/9d9e9e095c911dff77d524ffaaa86ac5.png)

### **LLaVA-NeXT**

**论文标题：**

LLaVA-NeXT: Improved reasoning, OCR, and world knowledge

**博客链接：**

https://llava-vl.github.io/blog/2024-01-30-llava-next/

LLaVA-NeXT，它在推理、OCR 和世界知识方面有所改进。LLaVA-NeXT 甚至在几个基准测试中超越了 Gemini Pro。

与 LLaVA-1.5 相比，LLaVA-NeXT 有几个改进：

1. 将输入图像分辨率提高了 4 倍像素。这使得它能够捕捉更多的视觉细节。它支持三种宽高比，分辨率可达 672x672、336x1344、1344x336。
2. 通过改进的视觉指导调整数据混合，提供更好的视觉推理和 OCR 能力。针对更多场景提供更好的视觉对话，涵盖不同的应用。具有更好的世界知识和逻辑推理能力。
3. 除了性能提升外，LLaVA-NeXT 还保持了 LLaVA-1.5 的简约设计和数据效率。它重用了 LLaVA-1.5 的预训练连接器，并且仍然使用不到 100 万个视觉指导调整样本。最大的 34B 变种在约 1 天内使用 32 个 A100 完成训练。

##### 6.1 Detailed Technical Improvement



![图片](https://i-blog.csdnimg.cn/blog_migrate/58bceb074739acd91379c3aafb5001dc.png)

▲ 通过将图像分割成网格并独立对其进行编码，将 LLaVA-1.5 扩展到更高分辨率。这使得模型能够适应任何分辨率，而无需为 ViTs 执行位置嵌入插值。我们还将下采样图像的特征连接起来，以为 LLM 提供全局上下文。

**Scaling to Higher Resolutions** 我们通过将图像分成原始训练视觉编码器的分辨率的较小图像块，并独立对其进行编码来克服这一问题。在获取单个块的特征图后，我们将它们合并成目标分辨率的单个大特征图，并将其馈送到 LLM 中。

为了为 LLM 提供全局上下文并减少分割-编码-合并操作的人为因素，我们还将一个降采样图像的特征连接到合并后的特征图中。这使我们能够将输入扩展到任意分辨率并保持 LLaVA-1.5 的数据效率。我们将这个结果模型称为 LLaVA-1.5-HD。

**高质量的用户指导数据**。我们对高质量的视觉指导跟随数据的定义主要有两个标准：首先，**任务指令的多样性**，确保充分代表了在真实世界场景中可能遇到的广泛用户意图，特别是在模型部署阶段。其次，响应的优越性至关重要，目标是获得良好的用户反馈。

为实现这一目标，我们考虑了两个数据来源：（1）现有的 GPT-V 数据，包括 LAION-GPT-V 和 ShareGPT-4V。（2）为了进一步促进更多场景下更好的视觉对话，我们收集了一个包含不同应用的小型 15K 视觉指导调整数据集。

这些指令和图像来自 LLaVA 演示，是真实用户的请求。我们仔细过滤可能涉及隐私问题或潜在有害的样本，并使用 GPT-4V 生成响应。

**多模态文档/图表数据：**

1. 我们从训练数据中删除了 TextCaps，因为我们意识到 TextCaps 使用与 TextVQA 相同的训练图像集。这使我们能够更好地了解在开发过程中评估 TextVQA 时我们模型的零 -shot OCR 能力。为了维持和进一步提高我们模型的 OCR 能力，我们用 DocVQA和 SynDog-EN 替换了 TextCaps；
2. 受到 Qwen-VL-7B-Chat 的启发，我们进一步添加了 ChartQA、DVQA 和 AI2D，以便更好地理解图表和图表的内容。

##### 6.2 Open Problems in LMMs

**数据效率：**在本节中，我们进行了进一步提高数据效率的实验，通过随机子采样 LLaVA-1.5 的训练数据混合，采样比例范围从 0.1 到 0.5 不等。我们在图 4 中可视化了不同采样变体的相对性能。

首先，完整的数据混合提供了最佳的知识覆盖，并允许模型实现最佳的整体性能。令我们惊讶的是，**仅使用 50% 的样本，模型仍然保持了超过 98% 的完整数据集性能**。这表明在数据效率方面还有进一步改进的空间。

其次，当将数据集缩减到 50% 时，模型在 MMBench、ScienceQA 和 POPE 上的性能完全不降低，甚至在 MMBench 上略有改善。同样，当进一步将数据从 50% 降至 30% 时，模型的性能保持稳定。这些结果显示了多模态模型也具有“少即是多”的潜在好处。

**重新思考 LMM 中的幻觉：**将模型的输入分辨率提高到 448 时，这种幻觉显著减少。这一发现很有意思，因为它表明 LMMs 可能对训练数据中的一些错误具有鲁棒性。

然而，当输入分辨率不足以使模型辨别训练数据中的所有细节，并且超出模型能力的数据量足够大时，模型会学会产生幻觉。这进一步表明，需要在提高数据注释的同时保持良好的模型处理信息的能力之间取得平衡。不平衡的扩展可能导致模型产生更多的幻觉或对视觉细节的理解能力降低。







![图片](https://i-blog.csdnimg.cn/blog_migrate/01522ea2be913c39a50fe1c8fa33a737.png)

### **Cheap and Quick**

**论文标题：**

Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models

**论文链接：**

https://arxiv.org/abs/2305.15023



![图片](https://i-blog.csdnimg.cn/blog_migrate/3935d765255bca5136b8cba6b580cd1b.png)

▲ Mixture-of-Modality Adaptation（MMA）概述及 LaVIN 的架构。在 LaVIN 中，采用了新颖的混合模态适配器来处理不同模态的指令。在指导调优过程中，LaVIN 通过端到端的模态混合训练（Mixture of Modality Training，MMT）进行优化。

本文提出了混合模态适应（Mixture-of-Modality Adaptation，MMA）：一种端到端的优化方案，通过轻量级适配器连接图像编码器和 LLM。与此同时，我们还提出了 MMA 中的一种新颖路由算法，可以帮助模型自动调整单模态和多模态指令的推理路径。

基于 MMA，我们开发了一个名为 LaVIN 的大型视觉语言指导模型，它在各种遵循指令的任务中展现出了比现有多模态 LLM 更优异的训练效率和更好的推理能力。

LaVIN 在效率上具有优越性，并且与现有的多模态 LLM 相比具有竞争力的性能，同时也确认了它作为通用聊天机器人的巨大潜力。实验结果显示，LaVIN 可以达到与先进的多模态 LLM（如 LLaVA）相当的性能，同时减少了高达 71.4% 的训练时间和 99.9% 的存储成本。

值得注意的是，将 LaVIN 在 ScienceQA 上进行微调仅需 1.4 小时，使用 8 个 A100 GPU，更新的参数仅为 3.8M。







![图片](https://i-blog.csdnimg.cn/blog_migrate/5bb26cf4a65143da4f45b778b2f1ef18.png)

### **MIMIC-IT**

**论文标题：**

MIMIC-IT: Multi-Modal In-Context Instruction Tuning

**论文链接：**

https://arxiv.org/abs/2306.05425



![图片](https://i-blog.csdnimg.cn/blog_migrate/ec577a151375f1e8a73070f2dd23ddc5.png)

▲ MIMIC-IT 数据集包括 280 万个多模态指令-回复对，涵盖了基本能力：感知、推理和规划。每个指令都伴随着多模态的对话背景，使得在 MIMIC-IT 上训练的 VLM 能够展现出在交互式指令遵循方面的强大熟练度，实现零 -shot 泛化。

数据格式比较：LLaVA-Instruct-150K vs. MIMIC-IT。（a）LLaVA-Instruct-150K 由一张图片及其对应的仅包含语言的上下文信息（黄色框）组成。（b）MIMIC-IT 包含**多个图片或视频的输入数据**，并支持**多模态上下文信息**，即考虑图片/视频和语言输入作为上下文信息。



![图片](https://i-blog.csdnimg.cn/blog_migrate/55957a3a36888e5bcb1c4f8191e008ab.png)







![图片](https://i-blog.csdnimg.cn/blog_migrate/aed7177269da78fe921caedca9973aaf.png)

### **LLaVAR**

**论文标题：**

LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding

**论文链接：**

https://arxiv.org/abs/2306.17107

本工作通过文本丰富的图像（例如电影海报、书籍封面等）增强了当前的视觉指令调整流程。具体而言，我们首先**使用公开可用的 OCR 工具在 LAION 数据集的 422K 个文本丰富的图像上收集结果**。此外，我们使用识别出的文本和图像标题提示纯文本 GPT-4 生成 16K 个对话，每个对话包含针对文本丰富图像的问答对。

通过将我们收集的数据与先前的多模态指令遵循数据相结合，我们的模型 LLaVAR 大大提高了 LLaVA 模型在基于文本的 VQA 数据集上的能力（最多提高 20% 的准确率）。



![图片](https://i-blog.csdnimg.cn/blog_migrate/6e4c135f7a5a8992c4ca71614de2012d.png)







![图片](https://i-blog.csdnimg.cn/blog_migrate/4512875f4fac9da3da9c9c2213fc232c.png)

### **SVIT**

**论文标题：**

SVIT: Scaling up Visual Instruction Tuning

**论文链接：**

https://arxiv.org/abs/2307.04087

为了推动多模态能力的边界，我们提出了规模化视觉指导调整（SVIT）方法。

SVIT 涉及构建一个包含 420 万个视觉指导调整数据点的数据集，包括 160 万个对话问答（QA）对，160 万个复杂推理 QA 对，100 万个引用 QA 对和 10.6 万个详细的图像描述。除了数量之外，所提出的数据集还具有高质量和丰富多样性。它是通过提示 GPT-4 与丰富的图像手动注释一起生成的。

此外，我们提出了一种新的数据处理方法，选择具有更好多样性和平衡性的子集，从而激发模型的优越能力。

数据集选择算法



![图片](https://i-blog.csdnimg.cn/blog_migrate/2e6a81b0f7021b5c61755abbea529805.png)

流行的基准测试评估多模态大型语言模型（MLLM）的不同能力，这需要特定的训练数据配方来激发预训练模型。因此，我们设计了一种新的数据配方，即核心集选择算法，以更好地适应这些基准测试，并在性能和训练效率之间取得平衡。

**多样性**。我们构建了一组与流行基准测试相匹配的关键概念，即 MME 和 MMBench。具体来说，我们设计了几个高级概念，然后使用 GPT-4 生成每个概念的数十个关键词。然后，我们过滤掉在 SVIT 数据集中频率较低的那些关键词。概念集在上表中。我们通过与概念集的重叠来衡量每个训练样本的信息量，并选择最具信息量的样本。

**平衡**。在 MME 基准测试中，使用“是”或“否”问题来评估模型。然而，在由 GPT-4 生成的数据中，这两个选择的比例极不平衡，这使得调整后的模型有倾向性地回答“是”。我们通过重新采样来调整比例。

通过以上两个操作，我们获得了 157,712 个样本的核心集 SVIT-core-150K，其大小与 LLaVA-Instruct-150K 相同。我们还用 SVIT-core-150K 替换了 LLaVA-v1.5-mix-665K 中的 LLaVA-Instruct-150K，从而生成了 SVIT-mix-665K。







![图片](https://i-blog.csdnimg.cn/blog_migrate/6c56c8aae90fb028d6abcb610fa9087b.png)

### **Qwen-VL**

**论文标题：**

Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond

**论文链接：**

https://arxiv.org/abs/2308.12966



![图片](https://i-blog.csdnimg.cn/blog_migrate/8374148aa6e4480db6cd456819510abb.png)

**预训练的第一阶段**，我们主要利用**大规模的、弱标记的、网络爬取**的图文对数据集。我们的预训练数据集由几个公开可访问的来源和一些内部数据组成。我们努力清理了数据集中的某些模式。原始数据集包含总共 50 亿个图文对，在清理后，仅剩 14 亿数据，其中 77.3% 是英文（文本）数据，22.7% 是中文（文本）数据。

**我们在这个阶段冻结了大型语言模型**，只优化了视觉编码器和 VL 适配器。输入图像被调整为 224×224。训练目标是最小化文本标记的交叉熵。最大学习率为 2e−4，训练过程使用了 30720 的图文对批量大小，整个预训练的第一阶段持续了 50000 个步骤，消耗了大约 15 亿个图文样本。更多的超参数详见附录 C，该阶段的收敛曲线如图所示。

**在多任务预训练的第二阶段**，我们引入了**高质量、细粒度的 VL 标注数据**，并使用**更大的输入分辨率和交替的图文数据**。同时训练了 Qwen-VL 的 7 个任务。对于文本生成，我们使用内部收集的语料库来维持 LLM 的能力。

我们将视觉编码器的输入**分辨率从 224×224 增加到 448×448**，减少了图像降采样造成的信息损失。我们解锁了大型语言模型并训练了整个模型。训练目标与预训练阶段相同。

**在监督微调阶段**，我们通过指令微调来对 Qwen-VL 预训练模型进行微调，以增强其**指令跟随和对话能力**，从而得到交互式 Qwen-VL-Chat 模型。多模态指令调整数据主要来自通过 LLM 自我指导生成的字幕数据或对话数据，这些数据通常只涉及单图对话和推理，并且仅限于图像内容理解。

我们通过手动注释、模型生成和策略串联构建了一个额外的对话数据集，以将定位和多图理解能力引入 Qwen-VL 模型。我们确认模型有效地将这些能力转移到更广泛的语言和问题类型上。

此外，我们在训练过程中混合了多模态和纯文本对话数据，以确保模型在对话能力上的普遍性。指令调整数据量为 35 万。在这个阶段，**我们冻结了视觉编码器，并优化了语言模型和适配器模块**。我们在下面展示了该阶段的数据格式。





![图片](https://i-blog.csdnimg.cn/blog_migrate/cdde60e0a18dc217980bf135aec43470.png)

### **NExT-GPT: Any-to-Any Multimodal LLM**

**论文标题：**

NExT-GPT: Any-to-Any Multimodal LLM

**论文链接：**

https://arxiv.org/abs/2309.05519



![图片](https://i-blog.csdnimg.cn/blog_migrate/d1460c86d67b7cadd91502e9e3e6af17.png)

作者提出了一个端到端通用的任意对任意 MM-LLM（Multimodal-Large Language Model）系统。NExT-GPT 将 LLM 与多模态适配器和不同的扩散解码器连接起来，使 NExT-GPT 能够感知输入并以文本、图像、视频和音频的任意组合生成输出。

NExT-GPT 基本思想是利用编码器对各种模态的输入进行编码，将其投影为 LLM 可理解的类语言表示。ExT-GPT 利用现有的开源 LLM 作为核心，处理输入信息，进行语义理解和推理。

LLM 不仅直接生成文本标记，而且还**产生独特的“模态信号”标记，这些标记作为指令来指示解码层是否要相应地输出什么模态内容**。然后，生成带有特定指令的多模态信号，经过投影后传输到不同的编码器，最终生成相应模态的内容。

**Multimodal Encoding Stage**

首先，NExT-GPT 利用现有的完善模型对各种模式的输入进行编码。对于不同的模态，有一组替代编码器，例如 Q-Former、ViT、CLIP。在本文中，NExT-GPT 采用了 ImageBind，它是跨六种模式的统一高性能编码器。然后，通过线性投影层，不同的输入表示被映射为LLM可以理解的类似语言的表示。

**LLM Understanding and Reasoning Stage**

在 LLM 方面，NExT-GPT 采用的是 Vicuna2，它是一种基于开源文本的 LLM，广泛用于现有的 MM-LLM 中。LLM 将不同模态的表示作为输入，并对输入进行语义理解和推理。它输出两项内容：1）直接文本响应；2）每种模态的信号标记，用作指示解码层是否生成多模态内容以及如果生成则生成什么内容的指令。

**Multimodal Generation Stage**

从 LLM 接收到多模态信号之后，基于 Transformer 的输出投影层会将信号标记表示映射为后续多模态解码器可以理解的信号表示。

具体来说，NExT-GPT 采用当前现成的潜在条件扩散模型（conditioned diffusion models）用于生成不同模态结果，包括用于图像合成的 Stable Diffusion 模型、用于视频合成的 Zeroscope4 模型和用于音频合成的 AudioLDM5 模型。

**Lightweight Multimodal Alignment Learning（轻量级多模态对齐学习）**



![图片](https://i-blog.csdnimg.cn/blog_migrate/154eed8a68f22f0e8244e2c9e9bafc60.png)

为了完成编码器对齐，作者从现有语料库和基准中准备了 “X-caption” 对（“X” 代表图像、音频或视频，caption 代表文字）数据，然后强制 LLM 根据标注 caption 生成每个输入模态的 caption，学习过程如上图所示。



![图片](https://i-blog.csdnimg.cn/blog_migrate/dea3381d9eda86e4d680f171c946b7ac.png)

在解码端，NExT-GPT 集成了来自外部资源的预训练条件扩散模型，对齐的主要目的是将**扩散模型与 LLM 的输出指令保持一致**。然而，在每个扩散模型和 LLM 之间执行全面的对齐过程将带来巨大的计算负担。因此，我们在这里探索一种更有效的方法，即解码端指令跟随对齐，如上图所示。

具体来说，由于各种模态的扩散模型仅以文本标记输入为条件， 这种调节与 NExT-GPT 系统中 LLM 的模态信号标记不同，这导致扩散模型对 LLM 指令的准确解释存在差距。因此，作者考虑**最小化 LLM 的模态信号标记表示与扩散模型的条件文本表示之间的距离**。

由于仅使用文本条件编码器（扩散模型的 Text Encoder 冻结），因此学习仅基于纯粹的字幕文本，即没有任何视觉或音频资源，这也确保了高度轻量级的训练。

**2.3 Modality-switching Instruction Tuning（模态转化指令调优）**

尽管编码和解码端能够与 LLM 保持一致，但距离使整个系统能够忠实地遵循和理解用户的指令并生成所需的多模态输出的目标仍然存在差距。

为了增强 LLM 的能力和可控性，进一步的指令调整（Instruction Tuning，IT）被认为有必要的。IT 使用“（输入，输出）”对整体 MM-LLM 进行额外训练，其中“输入”代表用户的指令，“输出”表示符合给定指令的所需模型输出。



![图片](https://i-blog.csdnimg.cn/blog_migrate/c72230b933b9c526a073a50d052c007e.png)

具体来说，作者利用 LoRA 使 NExT-GPT 中的一小部分参数能够在 IT 阶段与两层投影同时更新。如上图所示，当 IT 对话样本输入系统时，LLM 会重建并生成输入的文本内容（并使用多模态信号标记表示多模态内容），优化的目标是根据金标注和 LLM 的输出进行的。

除了 LLM 调优之外，作者还对 NExT-GPT 的解码端进行了微调，将输出投影编码的模态信号标记表示与扩散条件编码器编码的金多模态 caption 标注表示对齐。至此，全面的调优过程更加接近与用户忠实有效交互的目标。

为了更好地进行指令调优，作者还收集了几组数据集，其中的 “X” 可以是图像、视频、音频或其他模态的数据：

1. Text+X →Text Data：此类成熟的数据包括 LLaVA、miniGPT-4、VideoChat 等；
2. Text →Text+X Data：基于现有语料库中丰富的 “X-caption” 对，通过一些模板，作者借用 GPT-4 来生成各种文本指令来产生数据。
3. modality-switching instruction tuning（MosIT） Data：作者设计了一些“人”角色和“机器”角色之间的模板对话示例，在此基础上促使 GPT-4 在各种场景下生成更多具有 100 多个主题或关键词的对话。







![图片](https://i-blog.csdnimg.cn/blog_migrate/2847053efdd561aa816d2596221343db.png)

### **InternLM-XComposer**

**论文标题：**

InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition

**论文链接：**

https://arxiv.org/abs/2309.15112



![图片](https://i-blog.csdnimg.cn/blog_migrate/1647fa82aeb077e17bb8d8f3cee574d8.png)

▲ InternLM-XComposer 的架构和训练方案。预训练阶段对齐了视觉和语言知识，SFT 阶段激发了不同的模型能力。

模型由三个组件构成：

1. 视觉编码器：EVA-CLIP （CLIP的一个改进变种，通过掩码图像建模能力增强，以有效捕捉输入图像的视觉细微差异）。输入 224x224，以 stride 14 分为小 patch 后输入 transformer
2. 感知采样器（Perceive Sampler）：InternLM-XComposer 中的感知采样器作为一种专注的池化机制，旨在将初始的 257个 图像嵌入压缩为 64 个经过优化的嵌入。这些优化的嵌入随后会与大型语言模型理解的知识结构相匹配。与 BLIP2 类似，使用带有交叉注意力层的 BERTbase 作为感知采样器。
3. LLM：InternLM-XComposer 以 InternLM 作为其基础的大型语言模型。值得注意的是，InternLM 是一款强大的语言模型，具备多语言能力，在英语和中文方面表现出色。使用公开可用的 InternLM-Chat-7B 作为大型语言模型。







![图片](https://i-blog.csdnimg.cn/blog_migrate/34fcd47eeb1d240e3b66efa5f758f324.png)

### **CogVLM**

**论文标题：**

CogVLM: Visual Expert for Pretrained Language Models

**论文链接：**

https://arxiv.org/abs/2311.03079

**tl;nr:** 使用已经训练好的 LLM，然后给它添加图像的功能。方法上，引入 vit 做图像的 encoder 和 MLP adapter，来将图像编码到和 text 一样的 embedding 空间中，然后是在 LLM 的各层添加 visual expert，它具有独立的 QKV 和 FFN 相关的参数，并使用 LLM 中的层来做初始化，训练的时候冻结已经训练好的 LLM 部分，训练图像相关的部分。

这就是作者探讨的 deep fusion 方法。最后的效果提升很大。除了很少的任务没有超过 Pali-x 之外，其他全部 sota。

**浅层对齐的方法：**blip-2 中，把已经训练好的 image encoder 冻结，然后加一个 Q-former 或者 linear layer，把 image feature 映射到语言模型的 input embedding space 中，BLIP-2 NoCaps CIDEr 121.6。收敛很快，但是结果没有联合训练的模型效果好，e.g., PaLI-X. 用浅层对齐的方法训练的 chat-style 模型，e.g., MiniGPT-4, LLAVA, and VisualGLM，视觉理解能力弱表现为幻觉。

作者认为核心问题是，浅层对齐缺少不同模态信息的 deep fusion，这个灵感来自 p-tuning 和 LoRA 的对比，p-tuning learns a task prefix embedding in the input while **LoRA adapts the model weights in each layer via a low-rank matrix. LoRA 效果更好且更稳定**。

in the shallow alignment methods, the image features act like the prefix embedding in p-tuning. 其他细节：

- 语言模型权重冻结，这些权重是为文本训练的，文本的输入空间，图像的 embedding 在这个空间里没有很好的对应关系，每一层的输入的分布也是不断变化的，当经过几层变换之后，图像的特征分布已经和比较深的层的权重所需要的输入特征的分布不再匹配了。
- 在预训练过程中，图像字幕任务的先验，例如文字风格和字幕长度，只能在浅对齐方法中编码到视觉特征中。它削弱了视觉特征与内容之间的一致性。

CogVLM-17B包含：

1. LLM：Frozen Vicuna-7B-v1.5，此模型在所有的注意力操作中都应用了因果掩码（causal mask），包括图像特征之间的注意力。
2. ViT encoder：EVA2-CLIP-E ，负责将图像转化为特征表示。在 CogVLM-17B 中，**移除了 ViT 编码器的最后一层**，因为该层专注于整合 [CLS] 特征以用于对比学习。
3. MLP adapter：a two-layer SwiGLU MLP，用于将ViT的输出映射到与文本特征相同的空间。所有的图像特征在语言模型中共享相同的「位置编码 id」。
4. Visual expert module：在 LLM 的每一层中引入可训练的 visual expert，其包含专门处理 image feature 的「QKV [矩阵](https://so.csdn.net/so/search?q=矩阵&spm=1001.2101.3001.7020)」和「MLP 层」，以实现深度的视觉-语言特征对齐。QKV 矩阵和 MLP 的形状与预训练语言模型中的相同，并从中进行初始化。trainable visual expert 专门用于转换图像特征，功能上和 LLM QKV/MLP 一致，但是只针对 image feature，从而实现模态间的深度融合。



![图片](https://i-blog.csdnimg.cn/blog_migrate/46c5518f786368f538543469b659482f.png)

▲ CogVLM 的架构。（a）关于输入的说明，其中图像由预训练的 ViT 处理，并映射到与文本特征相同的空间中。（b）语言模型中的 Transformer 块。图像特征具有不同的 QKV 矩阵和 FFN。只有紫色部分是可训练的。

**PRETRAINING**：用了公开可用的图像文本对进行训练，为 LAION-2B 和 COYO-700M

**The first stage**：Image captioning loss, next token prediction task on 1.5B image-text pairs

**The second stage**：a mixture of image captioning and Referring Expression Comprehension（REC）。在答案的部分，只考虑了下一个标记的预测损失。REC 任务是根据 text description of an object 来预测图像中的 bounding box ，比如 “Question: Where is the [object]?” and “Answer: [x0, y0, x1, y1]” 。其中，x 和 y 坐标的取值范围从 000 到 999，表示在图像中的归一化位置。







![图片](https://i-blog.csdnimg.cn/blog_migrate/740d646181fb3080bc7a22ecf1a24de3.png)

### **OtterHD: A High-Resolution Multi-modality Model**

**论文标题：**

OtterHD: A High-Resolution Multi-modality Model

**论文链接：**

https://arxiv.org/abs/2311.04219

在本文中，我们提出了 OtterHD-8B，这是一种创新的多模态模型，是从 Fuyu-8B 演变而来，**专门设计用于以细粒度精度解释高分辨率视觉输入**。与传统模型不同，传统模型受固定大小的视觉编码器限制，OtterHD-8B 具有**处理灵活输入尺寸的能力**，确保其在各种推理需求下的多功能性。

除了这个模型，我们还引入了 MagnifierBench，这是一个评估框架，旨在审查模型对微小物体的细节和空间关系的辨别能力。我们的比较分析显示，虽然目前领先的模型在这个基准测试中表现不佳，但特别是在直接处理高分辨率输入时，OtterHD-8B 的表现优于其竞争对手很大程度上。

这些发现揭示了不同模型在视觉信息处理中的结构差异，以及**视觉编码器的预训练分辨率差异对模型在这些基准测试中有效性的影响**。我们的研究突显了大型多模态模型中灵活性和高分辨率输入能力的关键作用，并且展示了 Fuyu 架构的简洁性在处理复杂视觉数据方面所具有的潜力。







![图片](https://i-blog.csdnimg.cn/blog_migrate/932e5c315dd80ca1246cdd065f34869f.png)

### **Monkey**

**论文标题：**

Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models

**论文链接：**

https://arxiv.org/abs/2311.06607

Monkey 模型提出了一种有效地**提高输入分辨率的方法**，最高可达 896 x 1344 像素，而**无需从零开始进行预训练**。针对复杂场景描述、问答和叙述，Monkey 模型采用了一种无需预训练即可提高输入分辨率的架构和一种多层级详细描述生成方法。这两个设计确保了模型能够从生成的数据中进行更有效的学习，更高的分辨率可以更详尽地捕捉视觉特征，这反过来又提高了详细描述的有效性。

##### **1. 提高输入分辨率**



![图片](https://i-blog.csdnimg.cn/blog_migrate/002357ec1554e7f6b392e64f094a5c9b.png)

▲ Monkey 的整体架构允许通过从原始图像中捕获全局特征和从分割补丁中获取局部特征来实现高分辨率。所有补丁都通过共享的静态 ViT 编码器进行处理，例如具有 2b 参数的 ViT-BigG。

1. 给定一个 H x W 的图像，使用 x （和 LMM 分辨率一致）大小的滑动窗口将图像划分为更小的局部区域。Monkey 对于每个图片块的编码器都增加了独属它的 Lora [10] 来有效地识别和吸收每个图像区域的细节敏感特征，从而增强对空间和上下文关系的理解。训练时只训练 Lora 部分，因此无需大幅增加参数量和计算需求。
2. 原始图像大小也被调整为 x ，用于全局信息的提取。
3. 最后，通过视觉编码器和重采样器处理所有局部图像和全局图像，并将局部特征和全局特征送入 LLM。这种方法能够在不显着增加计算负载的情况下提高模型分辨率和性能。

##### **2. 多级特征整合详细描述生成**

之前的工作如 LLaVA [3]、Qwen-VL [4] 等依赖于互联网上爬取的大规模图文数据及进行模型的预训练。但这类数据标注比较简单，缺乏更丰富的图像细节。即使使用高分辨率图像进行训练， LMM 也无法在图像视觉特征和其中各个物体之间建立准确的关联，从而可能损害了视觉处理和语言理解之间的协同作用。

Monkey 使用了一种多级特征融合的详细描述生成方法（利用 BLIP-2 [5]、PP-OCR [6]、GRIT [7]、SAM [8] 和 ChatGPT [9] 等预训练系统），为 CC3M 中的 400k 图像提供更加细致的描述，来更好地将高分辨率的视觉模型和语言模型对齐。

关键发现

提高分辨率能提高模型性能（r3-r9），四个 LoRA 能够帮助模型获得图像中不同部分的独特特征（r7 vs. r9），并帮助模型建立对空间和上下文关系的理解。进一步提高输入分辨率能够提高模型在文档等更高分辨率的图像上的性能（r5,r6）。

同时，相比与直接插值扩大模型输入分辨率的方法相比（r1,r2 vs. r9），本文的方法在时间和性能上更具优势。表六中当把 llava1.5 的输入分辨率从 224 扩大为 448，性能得到显著提升，进一步展现了本文方法的有效性。



![图片](https://i-blog.csdnimg.cn/blog_migrate/4790792a18fd6403a857348bdd4eefe5.png)





![图片](https://i-blog.csdnimg.cn/blog_migrate/c8150dbc37ff921cdaddcae65bfb1719.png)

### **LLaMA-VID**

**论文标题：**

LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models

**论文链接：**

https://arxiv.org/abs/2311.17043

当前的 VLMs 在诸如图像字幕和视觉问答等任务中表现出色，但在处理长视频时面临着计算负担，因为存在过多的视觉标记。LLaMA-VID 通过用两个不同的标记表示每个帧来解决这个问题，即上下文标记和内容标记。上下文标记基于用户输入编码整体图像背景，而内容标记则封装了每个帧中的视觉线索。这种双标记策略显著减少了长视频的负担，同时又保留了关键信息。

总的来说，LLaMA-VID 赋予现有框架支持长达一小时的视频，并通过额外的上下文标记推动了它们的上限。在大多数基于视频或图像的基准测试中，它被证明超越了先前的方法。



![图片](https://i-blog.csdnimg.cn/blog_migrate/36e3ac2b135e032ae66e25f15831d2b6.png)

▲ LLaMA-VID 的框架。在用户指令下，LLaMA-VID 通过接受单个图像或视频帧作为输入，并从 LLM 生成响应。该过程始于一个视觉编码器，将输入帧转换为视觉嵌入。然后，文本解码器根据用户输入生成文本查询。在上下文注意力中，文本查询从视觉嵌入中聚合与文本相关的视觉线索。为了提高效率，提供了将视觉嵌入降采样到各种令牌大小甚至单个令牌的选项。然后，使用线性投影器制定文本引导的上下文令牌和视觉丰富的内容令牌来表示每个时间 t 的每个帧。最后，LLM 接受用户指令和所有视觉令牌作为输入并给出响应。







![图片](https://i-blog.csdnimg.cn/blog_migrate/3ceb33ac82e809874d258dfdce811816.png)

### **MoE-LLaVA**

**论文标题：**

MoE-LLaVA: Mixture of Experts for Large Vision-Language Models

**论文链接：**

https://arxiv.org/abs/2401.15947

最近的进展表明，扩展大型视觉语言模型（LVLMs）有效地提高了下游任务的性能。然而，现有的扩展方法使得所有模型参数在计算中对每个标记都是活跃的，这带来了巨大的训练和推理成本。

在这项工作中，我们提出了一种简单而有效的训练策略 MoE-Tuning 用于 LVLMs。这一策略创新地解决了多模态稀疏学习中的性能下降问题，从而构建了一个具有惊人参数数量但计算成本恒定的稀疏模型。

此外，我们提出了基于 MoE 的稀疏 LVLM 体系结构 MoE-LLaVA，它在部署过程中通过路由器唯一激活了仅排名靠前的 k 个专家，使其余的专家保持不活跃状态。

大量实验证明了 MoE-LLaVA 在各种视觉理解和物体幻觉基准测试中的显著性能。值得注意的是，仅有约 3B 个稀疏激活参数，MoE-LLaVA 在各种视觉理解数据集上表现出与 LLaVA-1.5-7B 相当的性能，甚至在物体幻觉基准测试中超过了 LLaVA-1.5-13B。通过 MoE-LLaVA，我们旨在建立稀疏 LVLMs 的基准，并为未来研究开发更高效、更有效的多模态学习系统提供宝贵的见解。



![图片](https://i-blog.csdnimg.cn/blog_migrate/62638df2bee4f5ad5a9f55e63839f6fa.png)

▲ MoE-Tuning 的示意图。MoE-Tuning 包括三个阶段。在第一阶段，只有 MLP 被训练。在第二阶段，除了视觉编码器（VE）之外，所有参数都被训练。在第三阶段，FFN 被用来初始化 MoE 中的专家，只有 MoE 层被训练。对于每个 MoE 层，每个标记只激活两个专家，而其他专家保持沉默。

**阶段一**：在这个阶段，我们的目标是使图像标记适应 LLM，使 LLM 能够理解图像中的实例。为了实现这一目标，我们使用 MLP 将图像标记投影到 LLM 的输入域中，将图像块视为伪文本标记。**在这个阶段，LLM 被训练来描述图像**。MoE 层在这个阶段不应用于 LLM。

**阶段二**：使用多模态指令数据进行调整是**增强大型模型能力和可控性**的关键技术。在这个阶段，LLM 被调整为具有多模态理解能力的 LVLM。我们使用更复杂的指令，包括**图像逻辑推理和文本识别等任务**，这些任务要求模型具有更强的多模态理解能力。

通常情况下，对于密集型模型，LVLM 训练在这个阶段被认为是完成的。然而，我们在同时将 LLM 转变为 LVLM 并稀疏化 LVLM 方面遇到了挑战。因此，MoE-LLaVA 利用第二阶段的权重作为第三阶段的初始化，以缓解稀疏模型的学习困难。

**阶段三**：作为初始化，**我们多次复制 FFN 以初始化专家**。当图像标记和文本标记被输入到 MoE 层时，路由器计算每个标记与专家之间的匹配权重。然后，每个标记都由前 k 个专家处理，并且根据路由器的权重进行加权求和。当激活前 k 个专家时，其余的专家保持沉默。这种建模方法形成了 MoE-LLaVA，具有无限可能的稀疏路径，提供了广泛的能力。



![图片](https://i-blog.csdnimg.cn/blog_migrate/4f04acd84e87b3142d1fe2a7eaaaaf56.png)







![图片](https://i-blog.csdnimg.cn/blog_migrate/124d5867e6f6061047ca1f5521d00e14.png)

### **LLaVA-UHD**

**论文标题：**

LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images

**论文链接：**

https://arxiv.org/abs/2403.11703

该文讨论了视觉编码在大型多模态模型（LMMs）中对理解视觉世界的基础作用。它突出了现有 LMMs 的局限性，如固定的图像大小和分辨率，以及最近对这一方向的探索在适应性、效率甚至正确性方面存在的不足。

为了解决这些挑战，该论文介绍了 LLaVA-UHD，一种大型多模态模型，旨在高效处理任何纵横比和高分辨率的图像。LLaVA-UHD 包括三个主要组成部分：



![图片](https://i-blog.csdnimg.cn/blog_migrate/b28fbfdc3da621aee7b346c4d2790546.png)

▲ LLaVA-UHD 框架。左图：给定一个高分辨率图像，LLaVA-UHD 首先计算理想的切片数量，然后从可能的因式分解中选择最佳分区，将高分辨率图像分割成不同大小的切片。右图：切片通过在位置嵌入上进行 2D 插值以保持原始纵横比进行编码，然后压缩并按空间结构排列以供 LLM 处理。

1. 图像模块化策略：该策略将原始分辨率的图像划分为较小的可变大小切片，以便进行高效和可扩展的编码。
2. 压缩模块：该模块进一步压缩由视觉编码器生成的图像标记，增强了效率。
3. 空间结构：一种用于组织切片标记以供 LLMs 理解空间关系的模式。

##### **模块化视觉编码**

针对具有不同纵横比的高分辨率图像，一个朴素的方法是将 ViT 的位置嵌入插值到目标形状，以整体编码。然而，这种方法由于二次计算成本和由于分布外问题导致的性能降低而不是最佳的。

为了解决这个挑战，我们提出了一种模块化的视觉编码策略。基本思想是将原始分辨率图像划分为**较小的可变大小切片**，其中每个切片的形状与 ViT 的标准预训练设置不会偏离太远。通过可变大小的切片，LLaVA-UHD 可以在不需要填充或形状扭曲的情况下实现对原始分辨率图像的完全适应性。

接下来，我们对 P 进行二维插值，以适应由分区策略给出的切片分辨率，用于视觉编码。在我们的实验中，我们表明，在预训练期间可以保持 ViT 和位置嵌入参数不变，并且在 instruction tuning 阶段更新这些参数就足以实现良好的性能。除了切片之外，我们还提供了一个以本机纵横比的低分辨率概览图像。概览图像可以提供图像的粗略信息和全局语义连接。

##### **压缩层**

高分辨率图像需要 LLMs 处理更多的视觉标记，这占据了大部分计算量。例如，一个 672×1008 的分辨率图像将为 LLaVA-1.5 生成 3456 个视觉标记。为了解决这个问题，我们使用一个**共享的感知器重新采样器层来压缩每个图像切片的视觉标记**。

具体来说，由视觉编码器输出的图像标记**通过一组查询向量通过交叉注意力被重新采样**为较少的数量（在我们的实验中从 576 个到 64 个）。与流行的基于 MLP 的视觉投影方法相比，感知器重新采样器不受图像分辨率的限制，始终保持固定且可负担得起的视觉标记数量，因此更适用于理解高分辨率图像。因此，LLaVA-UHD 可以使用比 LLaVA-1.5 在编码 336×336 分辨率图像时更低的计算成本来编码 672×1008 分辨率图像。

##### **图像切片的空间结构**

由于图像分区在不同图像之间是动态的，因此有必要向 LLM 提供图像切片的空间组织信息。受 FuYu 模型的启发，我们设计了一个空间模式来使用两个特殊标记指示图像切片的相对位置。具体地，**我们使用“,”来分隔一行中的切片表示，并使用 “\n” 来分隔不同的行**。在我们的实验中，我们发现这种简单的模式可以有效地向动态分区提供信息，从而产生良好的性能。

全面的实验证明，即使建立在分辨率为 336×336 的 LLaVA-1.5 架构上，LLaVA-UHD 支持高达 672×1088 的图像，并且在仅使用 94% 的推断计算量的情况下，在 TextVQA 上取得了 6.4% 的准确率提高。此外，该模型在学术环境中可以高效训练，在 8 个 A100 GPU 上仅需 23 小时，而 LLaVA-1.5 则需要 26 小时。







![图片](https://i-blog.csdnimg.cn/blog_migrate/c18b311251f02e6ec8f47e3aa4c4c884.png)

### **Yi-VL**

**论文标题：**

Yi: Open Foundation Models by 01.AI

**论文链接：**

https://arxiv.org/abs/2403.04652



![图片](https://i-blog.csdnimg.cn/blog_migrate/371739c82450c1ee58d7fa0b5047b8de.png)

Yi-VL 采用了 LLaVA 架构，经过全面的三阶段训练过程，以将视觉信息与 Yi LLM 的语义空间良好对齐：

第 1 阶段：**ViT 和投影模块的参数使用 224×224 的图像分辨率进行训练**。LLM 的权重被冻结。训练利用包含来自 LAION-400M 的 1 亿个图像-文本对的图像标题数据集。主要目标是增强 ViT 在指定架构内的知识获取能力，并实现 ViT 和 LLM 之间更好的对齐。

第 2 阶段：**ViT 的图像分辨率扩展到 448×448**，并训练ViT和投影模块的参数。它旨在进一步提升模型对复杂视觉细节的识别能力。此阶段使用的数据集包括约 2500 万个图像-文本对，例如 LAION-400M、CLLaVA、LLaVAR、Flickr、VQAv2、RefCOCO、Visual7w 等。

第 3 阶段：训练整个模型的参数（即 ViT、投影模块和 LLM）。主要目标是增强模型在多模态对话交互中的熟练程度，从而赋予其无缝整合和解释视觉和语言输入的能力。为此，训练数据集涵盖了各种来源，总计约 100 万个图像-文本对，包括 GQA、VizWiz VQA、TextCaps、OCR-VQA、Visual Genome、LAION GPT4V 等。为确保数据平衡，我们对**任何单个来源的最大数据贡献设定了上限，限制为不超过 5 万对。**



## 对比学习

- **原理** ：对比学习是**无监督学习**的一种，着重于学习同类实例之间的共同特征，区分非同类实例之间的不同之处。

> 举个例子，从imagenet中抽出猫、猫、狗、飞机四张图，那么猫和猫的图片肯定是相似的，和狗不相似。但是和飞机比起来，猫和狗是相似的。所以**对比学习就是对比着差异去学习，模型并不需要真的知道图片中代表的是什么，而只需要知道哪些图片是类似的，哪些图片是不一样的就可以了**。

- **训练目的**：对比学习，希望相似数据（图片）最终学到的特征是相似的，在特征空间（`embedding space` ）中，特征向量尽量靠近；反之还希望不同的数据学到的特征向量，尽量远离。

- `**pretext task`（**代理任务**）**：对比学习是不需要标签的（比如不需要知道图片是哪一类），但模型还是需要知道哪些图片是类似的，哪些是不相似的，才能训练。这就需要通过通过设计一些巧妙的代理任务，人为指定一些任务来实现。

- 应用最广的代理任务：[instance discrimination](https://link.zhihu.com/?target=https%3A//paperswithcode.com/paper/unsupervised-feature-learning-via-non-1) 。（**instdisc**）

- - 简单说就是，从一堆图片中调出任意一张图片𝑥𝑖，将其做一次转换（`transformation` ，比如随机裁剪等数据增广），得到新的图片𝑥𝑖1、𝑥𝑖2。那么**样本𝑥𝑖1叫做基准点（锚点），𝑥𝑖2被认为是正样本**（两者都是从𝑥𝑖变化得到的，虽然看起来有差异，但语义信息不应该发生变化），数据集中其它所有图片都是负样本。
  - 有了正负样本的划分，就可以将数据都输入编码器进行编码提取特征了。因为所有的正负样本都是基于锚点来说的，所以𝑥𝑖1会单独使用一个编码器𝐸11，𝑥𝑖2和其它所有负样本使用另外的编码器（可以是同一个编码器，也可以也可以使用不同的编码器。但是不同的编码器之间必须相似，这样编码的特征才有一致性，才有比较的意义）。
  - **对比学习就是要让正样本的编码特征和锚点的编码特征尽可能靠近（相似），让负样本的特征和锚点特征尽量远离**。
  - `instance discrimination`直译过来就是**个体判别**，在这个任务中，只有经过这张图片转换的样本才是正样本，其它图片都是负样本，所以每张图都自成一类。对于ImageNet来说，就不是1000类，而是**128万个类别。**



- 目标函数：确定了代理任务，知道如何定义正负样本之后，就需要用一个目标函数，来告诉模型该如何学习，比如常见的对比学习目标函数`**NCE loss**`等。
- 特性：对比学习最大的特性，是这种方法非常的灵活，可以设置各种不同的代理任务。只要找到一种方式去定义正负样本，剩下的都是一些比较标准化的流程。



如果把 近几年对比学习在视觉领域有代表性的工作做一下总结，那么对比学习的发展历程大概可以分为四个阶段：

1. 百花齐放
   这个阶段代表性工作有InstDisc（instance discrimination，）、CPC、CMC等。在这个阶段中，方法、模型、目标函数、代理任务都还没有统一，所以说是一个百花齐放的时代

- InstDisc：一个编码器+memory bank，特征一致性比较差
- Inva Spread：只使用一个编码器进行端到端训练，但是字典太小，负样本不够
- CPC：一个编码器+一个自回归模型
- CMC：有两个甚至多个编码器

2.CV双雄
代表作有MoCo v1、SimCLR v1、MoCo v2、SimCLR v2；CPC、CMC的延伸工作、SwAV等。这个阶段发展非常迅速，有的工作间隔甚至不到一个月，ImageNet上的成绩基本上每个月都在被刷新。

3.不用负样本
BYOL及其改进工作、SimSiam（CNN在对比学习中的总结性工作）

4.transformer
MoCo v3、DINO。这个阶段，无论是对比学习还是最新的掩码学习，都是用Vision Transformer做的。

<img src=".\image-20240422112836122.png" alt="image-20240422112836122" style="zoom:70%;" />

| 模型        | 创新点                                                       | 优势                           | 局限性                     |
| ----------- | ------------------------------------------------------------ | ------------------------------ | -------------------------- |
| **阶段一**  | 百花齐放                                                     |                                |                            |
| Inst Disc   | 提出了个体判别的任务，对比学习loss，使用一个 memory bank的外部数据结构去存储负样本来做对比学习 |                                | 特征一致性差               |
| Inva Spread | 只使用一个编码器而不需要额外的数据结构去存储负样本           | 可以进行端到端的对比学习       | 字典太小，对比学习效果不好 |
| CPC v1      | 提出了**infoNCE Loss**，以及预测型的代理任务，其输入可以是图像、音频、视频、文字或加强学习 | 是一个非常全能的结构           |                            |
| CMC         | 把两个视角的任务扩展到了多个视角，为以后的多视角多模态对比学习打下了基础 。 |                                |                            |
| 阶段二      |                                                              |                                |                            |
| MoCov1      | Inst Disc的延伸工作，使用队列结构代替 memory bank来存储负样本，使用动量更新编码器代替动量更新特征；把之前对比学习方法都归纳成字典查询问题；第一个让无监督预训练媲美有监督预训练的方法 | 字典大且特征一致性好，训练便宜 |                            |
| SimCLR v1   | Inva Spread延伸工作。batch-size加大到8192，引入projection head ，使用更优的数据增强（随机裁剪和随机色彩变换） | 端到端训练                     |                            |
| CPC v2      | 引入SimCLR v1的几个技术，ImageNet精度直接从40多提到70多      |                                |                            |
| MoCov2      | 相比 MoCov1，引入了projection head；使用更多数据增强、cosi调度器和更长的训练epoch |                                |                            |
| SimCLR v2   | 受noisy student影响，使用伪标签进行半监督训练。相比SimCLRv1使用了更大的backbone，动量编码器和两层的 projection head |                                |                            |
| SwAV        | 结合聚类和对比学习，使得对比学习不再需要负样本（跟聚类中心对比）；使用multi crop技术 |                                |                            |
| **阶段三**  | 不用负样本                                                   |                                |                            |
| BYOL        | 处理负样本实在是太过麻烦，所以完全舍弃负样本，自己预测自己（mse loss），也可以训练 |                                |                            |
| SimSiam     | 化繁为简，使用孪生网络，不需要动量编码器、负样本、大的batch-size就可以训练。不过一个分支必须是stop gradient，这样交替 优化，类似K-means |                                |                            |
| 阶段四      | 引入Vision Transformer                                       |                                |                            |
| MoCov3      | 冻住ViT结结构中的patch projection layer就可以稳定训练        |                                |                            |
| DINO        | teacher网络的输出先做centering归一化也可以稳定训练           |                                |                            |

总体来说，现在的对比学习都使用：

​	动量编码器（MOCO）

​	目标函数（infoNCE）

​	模型：用一个编码器+projection head（simCLR）

​	数据增强



## GNN

## AI for chemistry

开源的CPM，可用的数据集，重要论文：https://github.com/junxia97/awesome-pretrain-on-molecules

分子指纹综述：Concepts and applications of chemical ﬁngerprint for hit and lead screeni

端到端（End-to-End）在人工智能领域通常指的是一种系统或方法，**能够直接将原始输入映射到最终输出，而无需中间的人工干预或手动设计的特征提取过程**。换句话说，端到端的方法旨在让机器自己完成整个任务，从原始输入数据到最终的输出结果，而不需要人为的干预或特征工程。

### 化学预训练模型综述

**A Systematic Survey of Chemical Pre-trained Models**

链接：https://arxiv.org/abs/2210.16484

简介：纯利用有标签数据从头训练模型很贵，且这样的模型在分布外泛化能力差。解决方案受NLP领域启发，人们在化学预训练模型 （CPM）方面做出了巨大努力，即利用大规模的未标注分子数据库对 DNN 进行预训练学习通用能力，然 后针对特定的下游任务进行微调。

常用的分子数据结构：

​		1.分子指纹（FP）：分子指纹是较常用的通过遍历分子中所有原子间键连关系来进行分子表示的方法。分子指纹会通过hash以用相同长度的向量来表征不同长度的分子的要求，保证向量长度的统一，因此分子指纹往往是高维的0/1向量。

​		2.序列（如分子的smiles表示）

​		3.二维图：分子可以自然地表示为二维图，原子为节点，键为边。

​		4.三维图：三维几何图形表示分子中原子在三维空间 中的空间排列，其中每个原子都与其类型和坐标以及一 些可选的几何属性（如速度）相关联。使用三维几何的优势在于，构象信息对许多分子特性，尤其是量子特性 至关重要。此外，利用三维几何图形还可以直接利用立体化学信息，如手性。

CPM预训练策略：

![image-20240617141405437](.\image-20240617141405437.png)

​		1.AE（自编码器）：使用自编码器重构分子（图 3a）是学习具有表现力的分 子表征的天然自监督目标。分子重构中的预测结构是给定分子的（部分）结构，如原子或化学键子集 的属性。一个典型的例子是 SMILES 变换器。该研究利用**基于变换器的编码器-解码器网络，通过重建 SMILES 字符串所代表的分子来学习表征。**虽然自编码器可以学习 有意义的分子表征，**但它们只关注单个分子，无法捕捉 分子间的关系**，这限制了它们在一些下游任务中的表现

​		2.AM（自回归建模法，同GPT，预测下一个token）：自回归建模法（AM）将分子内含物因子化为一系列子 成分，然后**以序列中的前一个子成分为条件，逐一预测 这些子成分**。（如预测 SMILES 字符串中的下一 个字符串。或给定一个节点和边都 被随机屏蔽的图，GPT-GNN 每次生成一个屏蔽节点及其边，并最大化每次迭代生成的节点和边的可能性。然后，迭代生成节点和边，直到生成所有屏蔽节点。）与其他策略相比，**AM 使 CPM 在生成分子方面表现得更好**，其训练过程与分子生成过程类 似 [Bagal 等人，2021]。不过，AM 的**计算成本较高， 而且需要事先对原子或化学键进行排序**，这对于分子来 说可能并不合适，因为原子或化学键并不存在固有的 排序。

​		3.屏蔽组件建模 (MCM)（同BERT,MLM）：屏蔽成分建模 （MCM，图 3c）将 MLM 的理念推广到分子中。具体来 说，**MCM 屏蔽了分子的某些成分（如原子、键和片 段），然后根据剩余成分训练模型来预测它们**。**MCM 尤其适用于注释丰富的分子**。例如，屏蔽原子 属性可以GNNs 可以**学习简单的化学规则（如化合价）以及其他 潜在的复杂化学描述符**（如官能团的电子效应或立体效 应）。此外，与上述 AM 策略相比，**MCM 可根据周围 环境预测被遮蔽的成分**，而 AM 则仅仅依赖于按预定顺 序预提成分。因此，MCM可以**捕捉到更完整的化学语义。**然而，由于 MCM 在 BERT [Devlin 等人，2019 年] 之后的预训练过程中通常 会**掩盖每个分子的固定部分**，因此它不能对每个分子中的所有成分进行训练，从而降低了样品的 利用效率

​		45略

​		6.更换组件检测 (RCD) ：替换成分检测（RCD，图 3f）建议识别输入分子中随机 替换的成分。例如，MPG [Li 等人，2021] **将每个分子 分成两部分，通过组合两个分子的部分来改变其结构， 并训练编码器来检测组合后的部分是否属于同一分子**。

​		7.去噪 (DN）：如（Uni-mol）将噪声转化为三维分子几何的原子坐标，并预先训练编 码器预测噪声。

​		8.多模态预训练。除了第 2 节中提到的描述符之外，还可 以使用其他模式（包括图像和生化文本）来描述分子。 最近的一些研究对分子进行了多模态预训练。

应用：

​		1.分子性质预测 (MPP)

​		2.分子生成（MG）：分子生成是计算机辅助药物设计领域的一项长期挑战， 机器学习方法，尤其是生成模型，缩小了搜索空间，提 高了计算效率，使深入研究看似无限的药物化学空间成 为可能[Du 等人，2022b]。**事实证明，采用自回归预训练 方法的 CPM，如 MolGPT [Bagal 等人，2021]，有助于生 成有效、独特和新颖的分子结构。**多模态分子预训练技 术的出现[Edwards 等人，2022；Zeng 等人，2022b]通过 **将描述性文本转化为分子结构**，进一步拓展了分子生成 的可能性。CPM 展示其能力的另一个关键领域是**生成三 维分子构象**，特别是用于预测蛋白质配体结合位置。基 于分子动力学或马尔科夫链蒙特卡洛的常规方法往往受 到 计 算 能 力 的 限 制 ， 尤 其 是 对 较 大 的 分 子 而 言 [Hawkins，2017]，而基于三维几何的CPM[Zhu等人， 2022a；Zhou等人，2023]则不同，它们能在预训练过程 中捕捉二维分子与三维构象之间的一些固有关系，因此 在构象生成任务中表现出显著的优越性

​		3.药物-目标相互作用 (DTI)

​		4.药物之间的相互作用（DDI）

5 结论和未来展望  

​		1 改进编码器架构和预训练目标 虽然在分析神经架构的学习能力（如 GNN 的 WL 测 试）方面取得了长足进步，但这些分析在确定高度结构 化分子的最佳设计方面缺乏特异性。此外，迫切需要探索如何将消息传递技术无 缝集成到转换器中，作为统一的编码器，以适应大规模分子图的预训练。此外，由于 在第 3 节中讨论过，预培训目标仍有很大的改进余地， MCM 中子组件的高效屏蔽策略就是一个很好的例子。 

​		2 建立可靠和现实的基准 尽管对 CPM 进行了大量研究，但由于采用的评估设置 （如随机种子和数据集分割）不一致，其实验结果有时 并不可靠。例如，在包含多个昂贵的分子性质预测数据 集的 MoleculeNet [Wu 等人，2018]上，可能由于这些分 子数据集的规模相对较小，同一个模型的性能在不同的 随机种子下会有很大差异。

​		3 扩大化学预训练模型的影响 CPMs研究的最终目标是开发出多功能分子编码器，可应 用于与分子相关的大量下游任务。然而，与 NLP 界 PLM 的进展相比，CPM 的方法论进步与实际应用之间 仍存在巨大差距。**一方面，CPM 生成的表征尚未被广泛 用于取代化学领域的传统分子描述符，预训练模型也尚 未成为社区的标准工具。另一方面，对于这些模型如何 有利于单个分子之外的更广泛的下游任务（如化学反应 预测、虚拟筛选中的分子相似性搜索、逆合成、化学空 间探索等）的探索也很有限**。

​		4 建立理论基础 尽管 CPM 在各种下游任务中表现出令人印象深刻的性 能，但人们对这些模型的严谨理论理解却十分有限。



思考：1.没有提及GAN  GAN是否可行？比如面对生成合成反应链的任务，训练一个可以判断是否可行的D与生成反应链的G。（可能对分布外的泛化能力不强）

​		2.没有提及酶、催化剂、反应条件？（需要看看数据集）

​		3.一种提升模型分布外泛化能力表现的方案：无标签数据预训练模型+有标签数据微调  

​		4.生成式可能更适用于AM模型

​		5.难就难在泛化能力  这种泛化能力几乎等同于对真实世界的仿真？现实世界很多化学合成反应是大量试出来的  我们并不清楚内在反应机理（很多反应机理是假设？）  也就是说这其实也是黑箱  而DL也是黑箱 黑箱+黑箱=？ 可能需要一个更大的仿真平台 



### 深度学习用于化学反应与逆合成预测综述

**A Unified View of Deep Learning for Reaction and Retrosynthesis Prediction: Current Status and Future Challenges**

简介：化学家通常致力于设计合成路线，通过一系列化学反应获得目标分子。一种常见的策略是**将目标分子分解成更容易合成的简单前体结构**，这一过程被称为逆合成分析。逆合成规划涉及两个子任务：**多步骤逆合成规划和单步骤逆合成预测**。在本研究中，我们将重点讨 论后者，因为多步规划通常是作为搜索问题来处理的， 这与**反应预测和单步逆合成预测所提出的条件结构预测问题**有着本质区别。在本研究的其余部分，我们将单步 逆合成预测简称为逆合成预测。**反应预测是有机合成分 析的另一项关键任务。**一个强大的反应预测模型可以帮助我们深入了解生化反应的内在机理，并生成虚拟反应 以扩展用于逆合成规划的数据库。总之，反应预测和逆合成预测是相互关联、相互促进的。

反应预测和逆合成预测是双重的，它们也分别被称为前向反应预测和后向反应预测。 在深度学习中，这两项任务都被表述为**条件生成任务。**

![image-20240617165520216](.\image-20240617165520216.png)

**定义**：

**分子表示**：SMILES 字符串和分子图

​		1.对于 SMILES 格式，分子结构 M 被描述为字符序列 mi，使得 M ：= m1m2...mL，其中 L 表示字符串的总长度。序列表示二维分子结构的生成树，每个字符 mi 表示一个结构元素，例如原子元素、化学键、分支符号

​		2.分子图:分子也可以抽象为无向图 G = {V， E}，其中 V = {v1， ..， vn} 表示 n 个原子的集合，E = {e1， ..， em} 表示 m 个边的集合。每个节点都与一个特征向量 hi ∈ R d 相关联，其中包含原子信息，例如芳香性和电荷。然后，我们有一个包含所有原子信息的特征矩阵 H ∈ R n×d。邻接矩阵 A ∈ R n×n×c 描述了 M 的拓扑结构，其中 Aijk 表示原子 i 和原子 j 之间是否存在 k 型化学键。多个分子可以很容易地用上述两种格式表示。

对于 SMILES 格式，可以通过句号“.”将多个 SMILES 字符串连接成一个 SMILES 序列。对于分子图，一组分子被视为一个不相连的图，每个分子都是一个独立的连接组件。

**反应预测**：给定一组反应物预测所有可能的产物集合

**逆合成预测**：给定产物，推断可能的反应物集合。**在实际应用中产物种类为1，这是因为在公共基准数据集中只记录了主要产物。不幸的是，这个问题使得逆合成预测比反应预测更加困难，需要重新合成来连接新出现的原子**（如上图逆合成预测中的-BR,-Cl），**从而导致更大的组合搜 索空间。**

**反应中心和反应模板**。在反应预处理中，反应中心 C 是原子对 C = 的子集。 {(vi , vj )} ⊆ V × V，当化学反应发生时，这些键的 类型会发生改变。在逆合成预测中，反应中心 C 被定 义为现有键 C = {ei } ⊆ E 的子集，可以通过修改这 些键来获得更简单的结构。反应模板库 T 是一组从大型 化学反应数据库中提取的反应子图规则。反应模板 T∈ T是从相应的化学反应数据库中提取的子图式。 具体来说，

**原子映射**：反应预测和逆合成预测都遵循原子映射原则。 该原则规定，**反应物/生成物中的每个原子在生成物/反 应物中都有一个对应原子**。这种基本的一对一映射关系 从物理上限制了反应空间，并决定了化学反应主要是键 的断裂和键的形成。

![image-20240617170217147](.\image-20240617170217147.png)

**使用的深度学习方法**

![image-20240617170423711](.\image-20240617170423711.png)

​	**1.基于反应模板的方法**（可能类似于搜索+匹配？搜索可能的反应路线）：基于模板（TB）的方法主要是利用反应模板库来推断可能的反应中心。模板法是模仿人类专家进行化学推理的 方法。**TB 方法的推理过程旨在选出最佳模板，最关键的部分是将分子与模板进行匹配。*优点**(1) TB 方法是可靠的，因为它们使用的是提取的人 类知识，而人类知识总能为预测提供很好的解释。(2) 训练和输入过程相对简单**缺点**(1) TB 方法的性能高度依赖于模板数据库的规模。 因此，模板数据库必须经常更新，这显然非常昂贵。(2) **TB 方法对域外未见反应的普适性较差**。(3) 模板是提取局部子图规则，而忽略全局信息。因此，TB 方法无法捕捉到全局信息的相互作用。因此， TB 方法无法捕捉到全局信息的交互作用，很容易产生错 误信息。

​	**2.基于序列和基于图形的自回归模型**：

​		基于序列的自回归（SAR）模型在前向和后向预测中被广泛采用。SAR认为这两个问题都是神经机器翻译问题。对于反应预测，输 入是反应物的 SMILES 字符串 M R := m mRR ...mR ，长 度为 L1 ，输出是长度为 L 的 SMILES字符串 M P := mP mP ...m P。输入源和输出目标是翻转的，以便进行回溯预测。每个生成步骤对标记空间进行贪婪搜索，选出最佳标记。要 生成前 k 个候选者，我们只需对 贪 婪 结 果 进 行 波 束 搜 索 。

​		 基 于 图 的 自 回 归 （GAR）模型具有类似的学习机制，但生成序列定义不 同。GAR 模型首先定义一个动作空间 π，其中包括多个 编辑动作，如原子添加/删除、键添加/删除和终止。这 些动作的序列会将反应物/生成物转化为相应的产品/反应物。每个步骤 i 都会选择最优行动 π ∗ ，并将最优行动 π 应用 于每个步骤。π将反应物 G R,i−1 和生成物 G P,i−1 分别转化为下一状态 G R,i 和 G P,i 。当预测所有生成步骤都结束时，最佳操作是 " 终止"。![image-20240617172636107](.\image-20240617172636107.png)

 **基于图形的两阶段模型** ![image-20240617173559191](.\image-20240617173559191.png)

​	对于基于图的两阶段**反应预测**模型，他们将反应预测分为两个阶段，即**反应中心识别阶段**和**候选物排序阶段**。对于反应中心识别，其目的是**选择具有高反应性得分的原子对**，第二阶段 是学习如何对 G ˆ P 中的**可生成产物进行排序**（可能性排序）

​	对于基于图的两阶段逆合成预测模型，他们 将逆合成分为两个阶段，即**编辑预测阶段和合成阶段**。 在编辑预测阶段，他们选择一些预测反应性得分较高的 现有边缘进行分解。在获得预测的编辑中心 Cˆ 之后，我们通过打破预测的编 辑中心来分解目标分子，即产生一组合成子。![image-20240617174927816](.\image-20240617174927816.png)

**基于图形的非自回归模型**

​	只有反应预测在 NERF实现了基于 图的非自回归模型，目前还没有用于逆合成的非自回归 模型。



**现有局限**

​	**副产物**

​		1.缺少副产物：公开基准数据集中缺少副产品，导致监督信号不完整。

​		2.同1。逆合成直接使用 USPTO 中的反应数据，因此 所有单步分析只包含一个单一结果（因为没有副产物的数据）。结果侧的侧产物缺 失可能不会影响逆合成预处理的一般过程。不过，侧产 物仍能提供有关离去基团的重要信息。

​	**数据集**

​		1.USPTO-479K 数据集有两个主要问题。首先，**反应类型非常不平衡**。线性拓扑结构的反应是主要的反应 类型，而环状拓扑结构的反应很少。

​		2.其次，在实际应用中，**同一组反应物 在不同的物理条件下会产生不同的产物**，这就是反应预 测中的多模态。多模态可以为条件生成模式提供丰富的 信 息 ， 从 而 生 成 有 效 和 多 样 的 候 选 产 物 。 **然 而 ， UPSTO-MIT 中的大多数反应都是一对一映射**，这意味 着同一组反应物只能生成唯一的主要产物。

​		3.逆向反应的数据库规模太小。USPTO-50K 的规模不够大，只包含 50K 个逆反应。

![image-20240617175830904](.\image-20240617175830904.png)

**思考：**

1.数据集只是对真实世界的抽样，具体到化学反应这一类型数据集上，现有数据集样本少，且没那么具有代表性。

2.可以考虑分子的三维表示。与二维 分子图相比，三维欧几里得几何中的相对成对距离可能 会有很大不同。例如，原子 v1 和原子 v2 在非欧几里得 分子图中可能相距甚远，而在三维欧几里得空间中可能 相距很近。这对反应中心排序特别有用。但是这很贵，往往也很慢。而有些反应按现在的观点来看对空间构想有要求（如两个官能团间隔XXnm）



### 大型语言模型在化学中能做什么？综述

**简介：**实验结果表明，**在需要深入理解分子 SMILES 字符串的生成任务（如反应预测、名称 预测和逆合成）中，LLM 的表现竞争力较弱。**在产率预测和试剂选择等分类或排序形式的 任务中，LLMs 的表现具有竞争力。在涉及文本提示的任务（如性质预测和基于文本的分子 设计）或可解释的任务（如分子标题）中，LLMs 具有选择性竞争力。

**代码**：https://github.com/ChemFoundationModels/ChemLLMBench

**结果：**在**需要精确理解分子 SMILES 表征的任务（如名称预测、反应预测和逆合成）中，GPT  模型表现出较低的竞争力**； • GPT 模型在与文本相关的解释任务（如分子标题）中表现出强大的定性能力（图 14 中由 化学家进行的评估）和定量能力； 11 • 对于可以转换为分类任务或排序任务的化学问题，如性质预测和产量预测，GPT 模型可以 获得与使用经典 ML 模型作为分类器的基线相比具有竞争力的性能，甚至更好，如表 2 所示。

​		**性能不具竞争力（NC）的任务**。在反应预测和逆合成等任务中，GPT 模型的性能不如由 大量训练数据训练出来的现有 ML 基线，**部分原因是对分子 SMILES 字符串的理解能力 有限**。在反应预测和逆合成中，GPT 模型的输入和输出中都存在 SMILES 字符串。如果 **不能深入理解 SMILES 字符串所代表的，如果没有反应物和生成物以及将反应物转化为生成物的反应过程** ，GPT 模型将很难生成准确的反应。GPT 模型在名称预测任务中的表现也很差（见表 4） 。这进一步验证了一个观点，**即 GPT 模型在理解 SMILES、IUPAC 名称和分子式等格式 的长字符串并在它们之间进行正确转换方面存在困难。**

​		**具有竞争（C）性能的任务**。当化学任务被表述为**分类**（例如，将产 量预测格式化为 "高 "或 "低 "分类，而不是回归）或**排序**（如试剂选择）形式时，GPT 模 型可以取得令人满意的结果。这是可以理解的，**因为做出选择本身就比生成产物、反应 物或名称简单。**当要求 GPT 模型从提供的候选反应物、溶剂或配体中进行选择时，其准 确率可达 40% 至 50%。虽然 GPT-4 在产率预测方面的表现不如基线模型 UAGNN，**但 当给定更多的演示示例时，GPT-4 的表现会有所改善，**值得注意的是， UAGNN 模型是在针对这些特定反应的数千个示例中训练出来的。最后，虽然 GPT 模型 在已评估的高通量实验（HTE）数据集（特别是 Buchwald-Hartwig 数据集[1] 和 SuzukiMiyaura 数据集[50]）上表现出良好的产量预测性能，但在更具挑战性的数据集（如 USPTO-50k 数据集[53]）上，它们的表现与其他 ML 基线一样糟糕。这一观察结果表明， **GPT 模型在具有挑战性的化学数据集上的表现是未来研究和改进的潜在领域**。

​		**具有选择性竞争（SC）性能的任务。**GPT 模型在两类任务上具有选择性竞争力。 – 在某些数据集（HIV、ClinTox）的**属性预测任务**中，GPT 模型的表现明显优于基线 模型，F1 分数和准确率接近 1，如表 6 和表 7 所示。**这可能是因为要预测的属性标 签包含在提示中，而 GPT 模型的任务只是回答是或否**。例如，提示中包括抑制 HIV  复制或药物因毒性原因未能通过临床试验，我们观察到，从**提示中移除属性标签后 ，GPT 模型的性能显著下降**（参见附录 B 部分）。相比之下，采用机器学习模型的 基线在其输入中不包含这些标签的语义。这些模型的输入只包括图形式的分子表征 ，而不包括标签。 – 对于与文本相关的任务，如基于文本的分子设计和分子字幕，GPT 模型因其语言生 成 能 力 而 表 现 出 强 劲 的 性 能 。 在基 于 文 本 的 分 子 设 计任 务 中 ， 使 用 BLEU 和 Levenshtein 等 NLP 指标进行评估时，GPT 模型的表现优于基线模型。但是，如表 14 和表 15 所示，当涉及精确匹配时，准确率低于 20%。这表明，GPT 模型设计的分子 可能与地面实况不完全相同。特别是在分子设计/生成方面，精确匹配是一个重要指 标。**与自然语言生成不同，自然语言生成允许与输入有一些偏差，而分子设计则要 求精确的准确性和化学有效性。然而，与基本事实不完全一致并不会自动导致结果无 效。如果 GPT 模型生成的分子符合输入文本中列出的要求，并且大多数（超过 89% ）分子的化学成分是有效的（见表 14），那么它们仍有可能被证明是有益的，并有 可能成为基本事实的可行替代品。**不过，评估这些生成分子的真正用途（如评估其 在实际应用中的新颖性）可能是一项耗时的工作。

​		**在所有任务中，ICL(in context learning) 提示的性能都优于零镜头(zero-shot)提示。** • 在大多数任务中（如表 4、6、7、11、13、14 和 15 所示），使用脚手架相似性来检索与 问题最相似的示例作为 ICL 示例，比随机抽样取得了更好的效果。 • 在大多数任务中（表 4、6、7、10、11、14、15），使用较大的 k（更多的 ICL 示例）通 常比使用较小的 k（较少的 ICL 示例）性能更好。 这些观察结果表明，ICL 示例的质量和数量对 ICL 提示的性能起着重要作用[23, 36]。这可 能启示我们，有必要设计更多针对化学的 ICL 方法，以建立高质量的 ICL 示例，从而进一 步提高 ICL 提示性能。



​		**局限性：**LLMs 的**一个重要局限是缺乏对 SMILES 字符串中分子表示法的理解**，这在很多情况下会 导致不准确或不一致的结果，如 A 节中不同分子命名方式的翻译所示。SMILES（简化分子 输入行输入系统）[60, 61]是一种广泛使用的化学结构文本表示法。例如，乙醇（一种简单 的酒精）的 SMILES 字符串为 "CCO"。该字符串表示一个分子，其中两个碳原子（C）通 过单键连接，一个氧原子（O）与第二个碳原子连接。SMILES 字符串可以与其他自然语言 文本一起作为 LLM 的输入和输出。**然而，有几个问题使得 LLM 难以准确理解和解释 SMILES 字符串**：

​		1）SMILES 字符串中没有明确表示氢原子，因为氢原子可以根据标准成 键规则推断出来。**LLM 经常难以推断出这些隐含的氢原子**，甚至可能在计算分子中的原子 数等简单任务中失败[27, 6]。

​		2.**一个给定的分子可能有多种有效的 SMILES 表示法**，如果 处理不当或标准化不当，可能会导致歧义。因此，LLM 可能无法一致地识别和比较由不同 SMILES 字符串表示的分子结构。

​		3.LLM 对 SMILES 字符串没有固有的理解，将其视为字 符或子词序列。在处理较长的 SMILES 字符串时，LLM 依赖于字节对编码标记化技术，**这 种技术会将字符串分解成较小的片段或子词**，而这些片段或子词并不代表 SMILES 字符串 所代表的分子结构和分子特性。由于化学信息学中的许多任务都依赖于 SMILES 字符串对 分子的准确表述，因此 GPT 模型在将结构转换为 SMILES 字符串（反之亦然）方面的非竞 争性表现会影响下游任务，如逆向合成、反应和名称预测。**因此，有必要加强 LLM 处理分 子结构及其特定属性的能力，或将其与 RDKit [35] 等现有工具进行耦合。**



思路：

​		虽然在大多数任务中，LLMs 的表现都不如基线，但值得注意的是，LLMs 只利 用了少量示例来解决化学问题，而基线则是在大量特定任务数据集上训练出来的，因此受到了限制，**是不是可以将LLM在化学数据集上fine-tune，以增强LLM对SMILES、对任务的理解**

​		采用先进的提示技术，如**思维链（CoT）[59]、分解提示（Decomposed Prompting）**[31]，有 可能提高 LLMs 执行复杂推理的能力。思维链作为提示能够进一步提高性能，在大模型（如5400亿参数的PaLM）上表现出最大增益，但是当模型参数规模小于1000亿时，思维链并不能产生性能增益，甚至可能会导致性能下降。因此，思维链引发的可能是模型规模上的“涌现”能力，即一种**仅存在于大规模模型中的能力。**

​		gpt-4在分子设计上有很大潜力，可以设计从文字设计分子并给出合成路线的一条龙服务？



### 逆合成预测的现代AI算法 综述

简介：介绍了常用数据集和分子表征，并分别比较了基于模板的模型、无模板模型和基于半模板的模型。

一旦完成了精确的递归单步逆合成预测，**多步逆合成预测的重点就是规划最优的反应序列**，使合成步骤的数量、起始分子的成本、产生的废 料等最小化。因此，**单步逆合成预测模型的性能是逆合成任务的基础**



**模型分类：**

​	第一类是基于模板的模型，它整合了领域知识和基于先前化学知识的形式规则 ，如基于模板的算法。反应模板是一组规则，决定了反应物如何通过键解转化为生 成物。模板和规则这两个术语经常互换使用。这些模型通常具有较高的可解释性和 准确性，但在大多数情况下，它们很难在其知识库之外做出准确的预测。 

​		**基于模板的模型**通常涉及目标分子与整个模板库的匹配。然后，解决子图同构 问题，获得候选反应物。基于模板的系统的核心在于使用逆合成模板。从目标分子开始，按照预定义的规则选择模板，并将其 应用于目标分子以确定反应物。虽然与无模板方法相比，基于模板的方法具有更好 的可解释性和准确性，但它们的计算要求很高，而且在模板库之外的通用性有限。**对于给定的反应，我们可以将改变键连接性的原子集合确定为反应中 心。然后通过算法提取反应中心和相邻原子，并进行归纳，形成相应的逆合成模板**

![image-20240716113827547](.\image-20240716113827547.png)

![image-20240716114232157](.\image-20240716114232157.png)



​	**第二类是无模板模型**，通常不包含化学知识，被视为黑盒模型，如深度神经网 络。这些黑箱模型通常表现出较低的可解释性和较高的计算复杂性。它们容易生成 违反化学知识的解决方案。尽管如此，它们在发现不受现有知识库限制的新反应途 径方面显示出巨大潜力。随着计算数据处理能力的指数级增长，纯数据驱动模型的 性能有了大幅提高。避免了计算密集型的子图 匹配问题**。这些方法利用分子的文本表示（SMILES 或 InChI)将逆合成任务转化为 翻译任务，而翻译任务可通过使用深度学习中的强大方法来解决。**这一过程**不再需 要**原子到原子映射来识别反应中心**



 	**第三类是基于半模板的模型**，包括两个步骤：(1) 它们首先确定反应中心，并利 用反应中心将产物转化为合成物（中间分子）；然后 (2) 它们将合成物完整地转化为反应物。



**不同的模型架构**

SEQ2SEQ:基于 Transformer 的模型已成为纯数据驱动逆合成预测领域的主导力量

GNN 是一种很有前途的参数高效工具，可用于学习图的结构信息，从而预测反 应中的分子转换[21]。GNN 是对图的所有属性进行的可优化变换，它保留了图的对 称性（置换不变性）。Sperduti 是将神经网络应用于有向无环图的先驱[22]。这种方 法也适用于化学分子的无向图表示。无向图是分子的一种图 表示，以原子为节点，化学键为边，**天生适合捕捉化学分子结构**。

强化学习

搜索算法：搜索算法检索数据结构中存储的信息或在搜索空间中计算的信息，**为规划合成路线的多步逆合成预测奠定基础。**一般来说，这些算法分为两类：无信息搜索和有 信息搜索。无信息搜索不利用有关状态转换成本的信息；典型的例子包括深度优先 搜索和广度优先搜索。与此相反，有信息搜索采用启发式函数来评估当前状态与目 标状态之间的距离，从而指导搜索进度。虽然这种方法不一定是最优的，但它能确 保在合理的搜索时间内找到有利的解决方案。最佳优先搜索是采用优先队列概念的 典型启发式搜索。打开列表包含当前可遍历的节点，而关闭列表则存储已遍历的节 点。**波束搜索**（Beam search）通过扩展有限集合中最有希望的节点来增强最佳优先搜 索[45]。A* 搜索综合了均匀成本搜索和最佳优先搜索的优点，确保了解决方案的最 优性[46]。在这种情况下，每个状态的成本包括从起始状态到当前状态的实际成本和 从当前状态到目标状态的启发式成本。**蒙特卡洛树搜索**（Monte Carlo Tree Search， MCTS）[47] 完善了从当前状态到目标状态的价值估计。AlphaGo[48] 是 MCTS 最著 名的应用之一，它在围棋搜索树中探索潜在的棋步并追踪结果。MCTS 包括四个阶 段：选择、扩展、模拟和反向传播



**数据集**：在 CASP 任务中，无论是通过符号 AI 还是纯粹的数据驱动建模，计算机能够解 析的数据集都是前提条件。数据集的质量决定了模型的上限。毫不夸张地说，**数据 集的质量比模型本身更重要**

​	期刊和出版社根据许可协议，通过算法自动提取和专家人工编码，以计算机可 读格式提供数据集。其中包括爱思唯尔（Elsevier）出版的 Reaxys 数据库，截至 2023 年，该数据库涵盖了 7,300 多万个反应。来自 16,000 种期刊和 105 个专利局的 全面、最新的期刊和专利信息。它汇编了来自 16,000 种期刊和 105 个专利局的全面 、最新的期刊和专利信息。为了从化学专利中提取信息，爱思唯尔和澳大利亚墨尔 本大学发起了一个基于 NLP 模型的项目，名为 ChEMU[50]。化学文摘社（CAS）收 录了从 1840 年到 2023 年的约 1.5 亿个反应，包括有机、无机、天然产物全合成和生 物转化反应，是反应数据的最大提供者。其数据源来自期刊、专利、学位论文和重 要参考文献。此外，规模较小的数据集包括 InfoChem 开发的 SPRESI，该数据集涵 盖 1974 年至 2014 年期间的 460 万个反应。另一个著名的数据集是由 NextMove 软件 公司创建的 Pistachio 数据集，包含从 1976 年到 2023 年的专利数据，涵盖超过 13,118,970 个反应的庞大语料库。在研究人员中，使用最广泛的数据集是 Lowe 在 1976 年至 2016 年期间提取的专利数据子集，其中包含 330 万个反应。该数据集是目 前唯一可公开访问的反应数据存储库，通常称为 USPTO[51]。此外，USPTO 50K 是 USPTO 化学反应的子集和预处理迭代，由随机挑选的 50,000 个反应组成，涵盖十种 不同的反应类型[52]。USPTO-MIT[53] 也是一个常用的子集，与 USPTO-50K 相比， 它包含更多试剂和可能的催化剂

​	虽然上述数据集包括分子结构、反应条件（溶剂、催化剂、试剂）和产率等详 细信息，但它们也难免出现错误。此外，大多数专利和文献中正面数据的普遍存在 也导致了产品表述的分布不均[54,55]。这种数据分布的不平衡会对模型性能产生不 利影响。此外，在 CASP 框架内，失败反应的实例也起着重要作用，尤其是在涉及 到区域选择性和化学选择性的情况下。为了克服这些挑战，已经发布了 THE 数据， 以生成更加一致的数据[56]。IBM 发布了一种采用自然语言处理（NLP）的方法，从专利和科学文献中提取实验程序，从而创建结构化、自动化的友好格式[57]。Pistoia 联盟与爱思唯尔合作，定义了用于交换反应信息的统一数据模型 (UDM)。电子实验 室笔记（ELN），一种新型数据集从一家大型制药公司的电子实验笔记本中提取的数据，不受发表偏向高产反应的影 响[58,59]。值得一提的是，通过比较各种数据源，包括专利（美国专利商标局和 Pistachio ）、文献和专利（Reaxys）以及工业数据（阿斯利康 ELN），尽管它们的模板集规 模相似，但在反应空间的覆盖范围上却有所不同。Reaxys 因其广泛而独特的反应模 板多样性而脱颖而出，提供了更广阔的反应空间



**分子表示方法**

​		SMILES 是最广泛采用的分子结构字符串表示系统。SMILES 系统结合了特定的语法规则和化学原理，能够严格地表示分子结构。SMILES 的优势之一是能够将反应预测任务转化为机器翻译任务。SMARTS 作 为 SMILES 语言的扩展 ， 是一种描述分子模式和性质的语 言 。 SMARTS 可用于创建查询。SMARTS 的一个显著特点是允许使用通配符来表示原子 和化学键。因此，SMARTS 被广泛应用于化合物数据库结构的计算机化搜索，从而 实现高效灵活的化学结构搜索。

​		自参照嵌入字符串（SELFIES）[65] 是一种既 100%稳健又可由人类阅读的分子 结构表示方法，它的提出是为了克服 SMILES 的局限性。InChI[66] 是另一种基于字 符串的化学结构表示法，与 SMILES 相比，它具有唯一性和可逆性的优势。**这些方 法不再需要通过原子原子映射来识别反应中心**

​		分子指纹背后的核心思想是将分子映射到长度为 l 的比特串或数字数组中，其中每个比特编码分子是否 包含特定的亚结构特征，分子指纹具有计算效率高、易于检索等优点，是分子相似 性评估的理想选择。主要方法包括基于亚结构键的指纹、基于路径的指纹和环形指 纹

​		随着图神经网络的飞速发展，分子图也引起了 CASP 领域研究人员的极大关注 。无向图是图论中的一种基本数据结构，由带有相关权重的节点和边组成。无向图 中的边没有明确的方向，允许节点 A 和节点 B 之间存在双向边。它的每个元素都代 表节点之间是否有边相连。邻接矩阵的大小就是图 中 顶点的数量。如果邻接矩阵的行𝑖 和列𝑗 的元素为 1，则节点𝑖 和节点𝑗 由一条边连接。然而，邻接 矩阵的空间复杂度为 𝑂(𝑛2 ) ，其中 𝑛 是图中的顶点数。因此，为了提高计算效率， **如果邻接矩阵的大小较大，可以将邻接矩阵转换为节点、边和全局的特征向量，这 些特征向量一般用作输入特征**与 SMILES 和分子指纹相比，分子图可以表示更多的化学结构信息，包括原子 类型、键类型、拓扑结构等。在图的节点和边上还可以添加三维信息，如键长、键 角等。此外，图表示法不受原子顺序的影响。然而，从分子结构中提取图表示的高 效算法是分子图实际应用的先决条件。**对于表示反应图，从预先训练的模型中提取反应是一种很有前途的方法**



**根据文本序列训练的无模板模型可能会忽略键断开背后的重要化学含义 ，这有时会导致建议不可行**

**从专业文献中爬一点数据集**





### 利用局部反应性和全局注意力进行深度逆合成反应预测（LocalRetro基于模板+图神经网络+自注意力机制）

模型代码：https://github.com/kaist-amsg/LocalRetro

简介：先前方法依照分子全局结构推荐反应物，但**分子变化往往发生在化学反应的局部**，故设计名为LocalRetro的局部逆合成框架。且因为远端官能团也会对整体反应产生次要影响，故加入**全局注意力机制（GRA，即transformer中的多头注意力机制）**。同时，**将逆合成任务的两个阶段（识别和完成）合并**。

​		预测化学反应一般涉及两个映射方向， 要么是正向 （从给定的反应物预测产物），要么是逆向（从目标产 物设计适当的反应物），而术语 "逆合成 "指的就是后 一种合成路径规划。**正向预测一般更为直接**、 因为所需的任务是**一对一的映射**，即对于一组给定的反 应物，反应产物通常是唯一确定的（在实验条件变化的 范围内）。另一方面，**逆合成是一种一对多的映射**，而 且更具挑战性，因为可能有几种不同的反应途径来合成 一种目标化合物。



**差别：**

​	**先前**

​		逆合成任务通常分为两部分：反应中心识别和变异图翻译。在反应中心识别过程中，分子中的每个键都要经过预测被切割或保留。在识别和编辑反应中心后，由 GNN 完成生成分子（合成物），以生成相应的反应物。这种识别和完成概念类似于有经验的化学家如何设计给定分子的合成途径。

​		上述几乎所 有现有的逆合成方法都采用目标分子的全局结构来进行 预测。例 如 ，在大多数现有的基于图的逆合成方法 中， 全局特征是通过对 所有原子特征求和或求平均得 到的，并用于预测反应物。在基于分子相似性的逆合成 中，也会使用分子间的全局相似性。然而，使用全局特 征进行合成途径预测可能会产生对与目标反 应 **无直接 关系的细节的关注**，这是不可取的。

​	**本实验**

​		我们认为，**识别步骤和完成步骤是高度相关的**。换句话说，由此产生的反应物 高度依赖于已识别的反应中心，因此这两个步骤应合并 并**同时进行**。

​		在大多数基本化学反应中，分子式和结构因键的断裂 或形成而发生的变化大多是局部的。**所以本实验更关注局部，并使用GRA关注远端官能团。**

​		即：在本地推导反应模板，并评估这些本地模板在目标分子的所有列举的可能反应中心的适用性。所有化学变化信息都包含在局部反应模板中，因此，一旦预测出所选反应中心的 正确模板，只需应用推导出的模板，就能立即得到反应物。换句话说，我们的本地方法将识别和完成两步流程合并为一步。此外，我们使所有反应中心都能通过注意机制交换信息，以考虑全局情况。这与化学反应的非局部效应相对应，其中反应 性有时会因为远处的化学变化而改变。

**模型方法：**我们的方法**只关注分子结构（原子或键）的变化**，以完成逆合成。也就是说，我们不是从头开始寻找合适 的反应物，而是重点推断在键的形成或断裂和/或原子的添加 或移除方面发生了哪些局部变化以形成给定的产物。 接下来，我们开发了一个模型，**通过学习每个原子和化学键的局部环境来预测产生给定产物的正确局部反应模板。**由于化学反应并不总是局部的，有些反应可能会因为分子内存在某些偏远的化学环境而受到影响，为了考虑反应的这 种全局依赖性和非局部性，我们通过应用**全局注意力机制**来 更新所有原子和化学键的特征。为了捕捉原子和化学键之间 不 同 的 反 应 关 系 ， 我 们 应 用 了 **多 头 自 注 意 机 制 ，** 即 Transformer中应用的注意机制，通过学习给定特征的键、查 询和值来学习不同的上下文。原子和化学键的特征由分子中 的所有原子和化学键共同更新。我们将非局部注意力操作称 为**全局反应注意力（GRA**）



​		模型学局部：通过学习每个原子和键的局部环境（见代码unbatch_mask、unbatch_feats），训消息传递神经网络

​				学整体：GRA

​		将局部反应模板分类：**原子反应模板A、键反应模板B、混合反应模板C**![image-20240618000945551](.\image-20240618000945551.png)



**模型流程**



![image-20240618000404241](.\image-20240618000404241.png)

​	

​		首先，根据原子和化学键的属性初始化给定**分子的分子 图（G）、原子特征（va ，点）和化学键特征（eab，边 ）**。然后，通过**消息传递神经网络**（MPNN）、**化学键特征编码层**和**全局反应注意 层（GRA）** 更新原子和 化学键 特 征 ， 以编码分子中原子和化学键的局部环境和非局部反应依赖性。最后，通过原子反应模板分类器和键反应模板分类器（两个FC）预测每个局部反应模板 在每个原子和键上的得分（即使用FC分类头）。将预测的局部反应模板应用到预测的原子和化学键上，就得到了预测的反应物，并按预测得分排序。



**模型优越性**：

​		1.即使不使用GRA也表现优于其他模型

​		2.GRA的好处：提升1-2个点。虽然从数字上看， 1-2% 的改进似乎很小，但这却是一个重要的改进。这是**因为数据集中并非所有反应都需要非局部效应来描述**。事实上，大多数化学反应都是局部的，这就是为什么我们的基线局部编码模型在 没有非局部效应的情况下已经取得了很好的结果。然 而，在许多化学反应中，**非局部效应仍是化学家考虑用来解释选择性的一个重要因素**。因此，考虑到数据集中非局部效应确实起重要作用的一小部分反应，我们认为 1-2%（额外获得 5000-10000 个反应的正确性）是统计 意义上的显著改进，在化学上也具有重要意义。此外， **当逆向合成任务包括不止一种产物时，GRA 也会发挥 重要作用**。在测试集中，由于认识到了其他分子中存在的其他原子， 在包含多个产物的反应中，有 GRA 的 LocalRetro 的 top- 1 精确匹配准确率比没有 GRA 的模型高出 12.3% （表 S2）。我们注意到，USPTO-MIT 数据集中总共 有 471 个反应（1.2%）含有多个产物。应可能不会发生。**因此，GRA 允许算法对反应中心 进行优先排序，并找到正确的反应中心，从而防止在没 有全局关注机制的情况下可能形成的不想要的产物。**

​		3.模型成功针对多个实际合成规划问题给出了多个多步骤逆合成路线，与文献所给路径几乎相同。![image-20240618001532909](D:\Mengzhou\新建文件夹\神经网络\image-20240618001532909.png)

**缺点：**但它们仅限于现有模板数据库的范围，但它们（基于模板的模型）都存在覆盖范围不完整的问题，并且不能很好地扩展。

**展望：**未来的工作应着眼于**更大的反应数据库**，以进一步推广我们的模型。这些未来的反应数据库 还可能包括**反应条件**，如试剂、温度和 pH 值，这些都 是我们在本研究中使用的当前数据集中缺乏的关键信 息。随着反应映射方法的进步，31 ，我们希望将来能 使用更大的高质量数据集来训练我们的模型。我们还希 望这里提出的局部/非局部反应性概念可以用于正向反应产物预测模型，我们的研究小组目前正在开发这种模 型 。





1.图注意力机制一定程度上类似于transformer，（本文即运用了多头自注意力机制，但不是用在消息传递上，用在了全局信息上），是否可以将本文的消息传递机制也换为多头自注意力机制？

2.是否可以利用在NLP中train好的BERT等模型，迁移到逆向合成预测中（李宏毅实验室将这样的BERT用于DNA分子序列，效果不错）

3.没有经过无标记数据预训练





### Retroformer:(无模板逆合成，transformer +图神经网络)

代码：https://github.com/yuewan2/Retroformer

简介：Retroformer 是一种基于 Transformer  的新型架构，它引入了局部注意力头，支持局部重要反应区域与全局反应背景之 间的高效信息交换，它的生成程序对精确的局部区 域也很敏感，不借助任何模板由模型直接生成反应物和产物

模型架构：整体架构包括一个编码器、一个解 码 器 和 两 个 反 应 中 心 标 识 符 。

![image-20240619093909589](.\image-20240619093909589.png)

![image-20240619094005069](.\image-20240619094005069.png)

![image-20240619094106936](.\image-20240619094106936.png)![image-20240619094141286](.\image-20240619094141286.png)

​		**ENCODER:**与在整个模块内计算图形自我注意力的现有图形转 换器（Ying 等人，2021 年；Łukasz Maziarka 等人， 2020 年；2021 年）不同，我们的模型在头部层面对图形信息进行编码。我们指定了两种类型的注意力头： **全局头和局部头**。全局注意头与虚构自我注意头相同 ，其感受野是整个 SMILES 序列。而局部注意头则考 虑分子的拓扑结构。

​		我们在关键向量和边缘特征之间执行元素乘法,然后， 来自全局和局部头部的计算结果沿着隐藏维度进行连 接，并传递到线性层,线性层表示更新后的标记特征 h l+1 ,同时，边缘更新模块是一个全连接层（FFN）， 它将接收和发送标记的更新特征的连接作为输入:![image-20240619094757500](.\image-20240619094757500.png)

​		**反应中心检测：**然后，Retroformer 会预测每个 原子和化学键的反应概率 Prc (.) 并将 S 的反应区域转 换为解码器的注意力接收区域。简而言之，检测到的 反应中心 Src 是 S 的子集。图 2b 显示了预测反应概率的热图可视化。它由两个**全连接层**完成，分别名为 Atom RC Identifier 和 Bond RC  Identifier。![image-20240619095740806](.\image-20240619095740806.png)

​		然后，我们通过以下两种策略之一将原子和键反应概率转换为 Src 中token的反应指标：

​		1.朴素：如果一个token存在于**具有反应活性的边**（即 Prc（e） > 0.5）并且**本身是反应式的**（即 Prc（s） > 0.5），我们朴素的地将它设置为反应性。请注意，**特殊令牌（special token）必须保证是非反应性的**。此策略用于训练和推理阶段。

​		2.搜索：我们对分子图进行**子图搜索，并按其反应中心分数对子图进行排名**：P si∈Src log Prc（si）+P si，sj∈Src log Prc（eij ）。在搜索中，仅考虑具有 Prc（s） > αatom 的原子和具有 Prc（e） > α 键的键，以减少计算时间。详细算法在附录 E.4 中描述。然后，选择前 n 个子图作为反应中心候选。然后，该模型为每个反应中心生成 k/n 反应物，其中 k 是预测反应物的总数。最终结果按反应中心分数和生成分数的总和进行排名。此策略仅在推理阶段使用。

​		**解码器：**解码器将上一步的生成结果、编码器输出 h 和反应中 心 Src 作为输入。与编码器类似，我们也在其交叉注 意模块中引入了两个不同的头。全局头与香草头相同 。相反，局部头部只对检测到的反应中心 Src 可见。它计算的是稀疏交叉注意力，而不是全局交叉关注。与编码器一样，从全局和局部磁头计算出的表征也会 沿着隐藏维度串联起来，并传递到线性层。

训练：我 们 还 建 议 使 用 SMILES 对齐和即时数据增强作为两种额外的训练 策略。

![image-20240619100601552](.\image-20240619100601552.png)

​		**SMILES 对齐**：SMILES 对齐是 Retro- former 的附加学习任务。与机 器翻译类似，源分子和目标分子的 SMILES 序列通常 是部分对齐的。在反应过程中，大部分分子保持不变 。图 4 显示了图和 SMILES 表示法中的这种对齐关系 。图之间的节点配准（即原子映射）可以很容易地转 换为 SMILES 之间的标记配准。这种引导式关注可以促使模型更有效地 理解化学反应。

​		**数据扩充:**我们沿用（Seo 等人，2021 年；Tetko 等人，2020 年 ）在 SMILES 生成模型中使用的相同数据增强技巧， 即生成物的 SMILES 排列和反应物的顺序排列。不过 ，我们并没有扩展现成的训练数据集，而是选择即时 进行扩展。每次迭代时，产品 SMILES 和反应物排序 排列的概率为 50%。这种动态排列允许 l+1 i 本地 Σ sj∈Src q kij T σ( √ d )vj (6) 该模型更加关注标准 SMILES，并使用经过处理的 SMILES 进行正则化。

![image-20240619100912694](.\image-20240619100912694.png)

缺点：由于我们的模型是通过学习源 SMILES 和目标 SMILES 之间的标记排列来训练的，因此预测的注意力可以很容 易地转换为原子映射。

![image-20240619101359174](.\image-20240619101359174.png)

​	该赋值错误地将 [O:11] 与 [N:11] 对齐。这个错误是可以解释的，因为反应物 中的 [HN:8] 和 [O:11] 正是发生化学变化的位置。此外，由于第二种反应物具有对称性，这种天真的原子映射也 无法实现一对一的映射。



暴论：实际上是数据集太小的问题？因为无论怎么改，理论上全局多头自注意力机制都能通过不断地学习、迭代达到相同的效果（如全局注意力机制也可以只关注局部反应，但是这需要大量时间训练）。但如果一昧的增加数据集/增大模型，可能又回到了老路。





逆合成的难点：**逆合成是一种一对多的映射**，而 且更具挑战性，因为可能有几种不同的反应途径来合成 一种目标化合物。

​						**在实际应用中产物种类为1，这是因为在公共基准数据集中只记录了主要产物。不幸的是，这个问题使得逆合成预测比反应预测更加困难，需要重新合成来连接新出现的原子**（如上图逆合成预测中的-BR,-Cl），**从而导致更大的组合搜 索空间



### OGNN（环+自注意力）

**代码**：https://github.com/O-GNN/O-GNN    模型架构：见dual graph

**简介**：在这项工作中，我们 设计了一种新的图神经网络变体--环增强图神经网络（O-GNN）。我们 明确地将环表示纳入 GNN，并与原子和键表示共同更新**除了对化合物中的原子和键进行建模外，还对环进行了明确建模**(具体到代码中应为面特征（face features）)。在 O-GNN 中，**每个环由一个潜在向量表示**，该潜在向量对原子和化学键的表示有贡献，并通过原子和化学键的表示进行迭代更新。**我们主要使用自注意层来进行自适应信息传递，并使用MLP来引入非线性表征。**在逆反应合成方面，我们将 -GNN 应用于LocalRetro（用OGNN替换掉MPNN层），这是一种基于 GNN 的强大回溯合成方法。**通过利用环信息，无论层数多少，性能都得到了提升**。此外，我们还发现，6 层的 -GNN  与 12 层的 -GNN （无环）性能相当，这显示了在 GNN 中建立环模型的巨大威力。**但是更大的模型并不总能带来更好的验证结果**，14 层O - GNN 的验证 MAE 与 12 层O -GNN 相比略有下降。**总体而言，随着环数、最大环大小和环上 原子数的增加，O-GNN 与不对环建模的变体相比取得了更大的改进**

​		在图神经网络（GNN）中，面特征（face features）通常指的是图结构中更高层次的结构特征。具体来说，面特征可以理解为图中由多个节点和边组成的子结构，例如在分子图中，面特征可以表示由多个原子和化学键组成的环结构。

​		在分子图或其他复杂的图结构中，面特征可以帮助模型捕捉到更复杂的局部结构信息，从而提高模型的表达能力和预测性能。例如，在分子图中，面特征可以表示不同的化学环结构，这些环结构对于分子的性质和反应活性具有重要影响。

在代码中，面特征的处理通常包括以下几个步骤：

1. **初始化面特征**：根据是否需要初始化面特征，可以选择将面特征初始化为零或通过某种方式进行初始化。
2. **聚合边特征**：通过聚合与面相关的边特征来生成面特征。
3. **编码面特征**：使用多层感知器（MLP）或其他编码器对面特征进行编码。
4. **消息传递**：在GNN的消息传递过程中，面特征会与节点特征和边特征一起进行更新和传递。

**通过引入面特征，模型可以更好地捕捉到图结构中的复杂模式和局部结构信息，从而提高模型的性能。**

​		环状化合物是指体系中至少有一个环的分子，自然存在于化学空间中。根据我们从广泛使用的化学库 PubChem（Kim 等人 ，2019 年）中对 1.09 亿个化合物的统计，**90% 以上的化合物至少有一个环**。



**优点**：

​		1.理论分析表明，O-GNN 只需一层就能区分位于不同环上的两个同构子图，而传统的图卷积神经网络需要多层才能区分，这表明 -GNN 更具表现力。

​		2.当网络宽度和高度的乘积不够大时，现有的基于消息传递的 GNN 无法正确捕捉环信息

**模型架构：**![image-20240626163936093](.\image-20240626163936093.png)

		我们的模型由具有不同参数的 L 个相同层组成。各层的结构如上图所示
		E:边  V：点  R：环  U：整个化合物（全局向量？）

**初始化：**

​	我们通过可学习嵌入层初始化 h(i) ，该嵌入层表示其**原子**类型、手性、度数、形式电荷、杂化类型等。同样，我们通过 可学习嵌入层对 h(ij) 进行初始化，可学习嵌入层表示其**键**类型、立体异构类型以及键是否 共轭。 然后，我们通过连接节点嵌入和边缘嵌入来初始化 h(r) ，再用非线性层对其进行转换(即**环**)。最 后，我们用可学习的嵌入来初始化**化合物**（U）表示。在每一层中，我们依次更新节点、键、环 和化合物的表示。我们将经常使用 MLP( )（一种具有一个隐藏层的多层感知网络）来构建 我们的模型。MLP 的输入被串联成一个长向量，并由网络进行处理。

​		(1) 更新键的表示：通过连接的原子、键所属的环和上一层的化合物表示更新键的表示

![image-20240626170713698](.\image-20240626170713698.png)

​		(2) 更新原子表征：我们使用注意力模型将键代表聚合到集中原子中

![image-20240626170729842](.\image-20240626170729842.png)

​		(3) 更新环状表征：使用 MLP 网络更新环状表征

![image-20240626170741002](.\image-20240626170741002.png)

​		(4) 更新化合物表示：![image-20240626170858561](.\image-20240626170858561.png)

**堆叠 O-GNN层后，我们通过一个简单的平均池化层得到图表示，可用于图分类任务。对于节点分类任务 我们可以在 h(L) 上添加一个分类头。**



**展望**：O-GNN 具有巨大的潜力，我们将在未来把它与预训练方法结合起来



### NAG2G（节点对齐+自回归）

代码：https://github.com/dptech-corp/NAG2G

简介：**节点对齐**图到图（NAG2G）模型--一种**基于图的无模板 SSR**（单步逆合成） 方法。该模型采用 Transformer **编码器-解码器框架**，**以自回归的方式生成反应**。NAG2G **将2 维分子图和三维构象结合**起来，保留了全面的分子细节，并通过节点对齐将产物反应物原子映射纳入其中，从而以自动回归的方式确定**逐节点图输出过程**的顺序 。

**先前的问题**：虽然无模板 DL 模型在逆合成预测方面具有灵活性和前景 ，但它们往往忽略了重要的二维分子信息（二维分子图囊括了原子 环境的大量信息，如相邻原子及其连接)，并且在节点生成的原子对齐方面存在困难，导致其性能低于基于模板和半模板的方法。而基于模板的方法的局限性在于其所依赖的库。库可能无法涵盖所有潜在的反应，而且错综复杂的产品和模板结构之间可能存 在不正确的关联。

​	基于一维序列的 模型有几个局限性：

1) 序列忽略了大量的分子拓扑信息；
2) 合法的 SMILES 遵 循复杂的语法规则，这增加了生成有效 SMILES 的难度；
3) 有效利用生成物和反应物之间的原子映射信息对一维表示法来说具有挑战性。如果不对齐，模型性能可能会因生成物和反应物之间原子相关性的丢失而下降。
4) **由于一个分子可以有多个 SMILES式子**当为一个产品生成多个候选反应物时，有可能生成代表同一反应的多个反应物 SMILES，这可能会减少候选反应物的多样性（也就是说模型可能生成很多SMILES式，但其实是一个东西）

无模板学习的理论基础：在没有科学家提供任何先决知识（包括字典、模板、合成物、中间体和编辑策略 ）的情况下，深度学习模型能否学习化学？答案是肯定的。其基本思想认为，**可以用类似于自然语言处理（NLP） 任务的方式来分析分子**



模型架构：UniMol的encoder+自己训的decoder

	Encoder:编码器扮演着学习分子表示形式的关键角色.input见下式
			Oenc = fenc (X, Penc , E, R; θenc )、 X 表示原子特征；Penc 表示一维空间编码(位置编码)，是原子嵌入的补充； E 表示二维图结构固有的边特征；R 对应三维构象中的原子坐标；θenc 封装编码器的 可学习参数，Oenc 是编码器得出的分子表示结果。
			
	Decoder:解码器主要是通过自动回归法逐个节点生成反应物图
			在第i 个时间步，也就是 第i 个生成节点（原子）时，解码器会收到三个不同的输入：
	        1) 编码器的输出，包括有助于编码器和解码器之间交互的键和值。 
	        2) 解码器的输出来自之前的步骤（从 1 到 i-1），这是典型的自回归模型，因为新值的预测是基于之前的值。在迭代过程中，增加了一维位置编码，这对 NAG2G 对齐 编码器（产物）输入和解码器（反应物）输出之间的原子顺序至关重要。
	        3) 当前输出图的图层特征，如节点的度和节点间的最短 路径。将这些图层特征直接整合到模型中会带来效率上的挑战，因为图层特征会随时间步长而变化。为了解决这个问题，我们提出了一种整合这些图层特征的高效方法（见原文图5）。
	        根据上述输入，解码器在第i 个时间步自动生成一个新节点，从原子类型开始然后是相关的形式电荷、相连氢原子的数量，最后是与先前节点（原子）相连的边（ 键的类型）。每个节点的信息都是根据上述预测依次生成的

![image-20240703145203468](.\image-20240703145203468.png)

**节点对齐**：分子图与句子不同，缺乏固有顺序，因为分子中的原子在分配之前没有自然顺序。无序节点不仅给图形生成带 来了挑战，也给编码器的输入数据扩增带来了挑战。为了应对输入和输出数据扩充方面的挑战，并实现灵活的逐节点自动回归生成， 我们提出了一种基于产品-反应物节点对齐的新方法。

​		我们的方法首先由 RDKit 随机 生成产品的 SMILES 序列、37 如图 3 所示。按照 SMILES 序列中的新顺序，我们得到 了**数据增量**输入图的节点序列顺序。随后，使用位置嵌入标记图节点顺序。如图 3 和 图 4 所示，对于已确定顺序的产品图，我们建立了一条唯一且明确的规则，该规则与 逐节点输出的反应物节点顺序相对应。在反应物中，原子可分为两类：与生成物共享 的原子和反应物独有的原子。原子顺序的分配应考虑这两个方面。首先，在生成顺序 时，我们规定反应物中共享原 子 的顺 序 应优先于非共享原子的顺序。为了确保对于 特定的有序产品输入，存在唯一对应的有序反应物，反应物中共享原子的顺序应遵守产品中的顺序。随后，使用 RDKit 将反应物 SMILES 与生成物 SMILES 对齐，以获 得最相似的 SMILES。最后，从对齐的反应物 SMILES 中提取非共享原子的顺序，确保非 共享原子顺序的唯一性。这种方法利用了产品-反应物对齐信息，确保节点生成顺序 与训练过程中的输入图顺序一致，并允许在输入和输出中进行一致的等变量数据扩增 ，从而提高了生成过程的整体稳健性和准确性。<img src=".\image-20240703151552968.png" alt="image-20240703151552968" style="zoom: 80%;" />

<img src=".\image-20240703151626965.png" alt="image-20240703151626965" style="zoom:80%;" />

可以将SMILES换成SELFIES

但是科学语言真的能像自然语言一样通过语境学习吗？



### LORA（用于LLM的低秩适应）

代码：https://github.com/microsoft/LoRA

解读：[LoRA: Low-Rank Adaptation of Large Language Models 全文解读 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/681019425)

https://www.bilibili.com/video/BV1MH4y1g77V/?spm_id_from=333.1007.tianma.2-1-4.click&vd_source=898cb0d612cdcdea138eb015775eb7cb

- **挑战背景**：大型预训练语言模型在全参数微调时存在灵活性不足和昂贵的独立部署成本等挑战。
- **LoRA方法引入**：作者引入了LoRA方法，该方法旨在在大型语言模型上进行**适配**，以降低训练参数数量。与之相比，微调的最大缺点是新模型包含的参数与原始模型一样多。
- **低秩矩阵注入**：LoRA的独特之处在于它在每个Transformer结构层中注入了**可训练的秩分解矩阵**，而不会修改预训练模型的权重。
- **优化显存和训练时间**：相对于传统微调方法，LoRA显著减少了所需的显存和训练时间，降低了资源成本。
- **通用性验证**：LoRA方法在多个任务和多个大型语言模型上进行了广泛验证，包括RoBERTa、DeBERTa、GPT-2和GPT-3，表现出相当甚至更好的性能。

**当前研究动态**

- **部分参数适配**: 针对这一问题，部分研究尝试只对模型的部分参数进行适配，或者为新任务学习外部模块，以此减少模型的总参数数量。

- **带来的挑战**: 这些方法通常会导致推理速度延迟（例如通过加深模型结构）或减少模型可处理的序列长度。

- **性能与效率的平衡**: 虽然这些方法提高了操作效率，但往往未能达到全参数微调的性能水平，从而形成了效率与模型质量的权衡

  ![img](https://pic2.zhimg.com/80/v2-2965bfe36b6c33d4afea39625108fd01_720w.webp)

  ​																	LORA：预训练权重W不受影响，只优化A和B的参数

- **设计灵感**：LoRA方法的设计启发来自于这样一个简单而深刻的观点：**即便是最复杂的模型，在其核心结构上往往隐藏着简单性**（其他人的研究表明 ，学习到的过参数化模型实际上**驻留在较低的内在维度**上。）。这就像是在一个充满书籍的巨大图书馆中，尽管书籍众多，但我们真正需要的知识往往只蕴含在其中的少数几本重要书籍里。**因此，一个超参数化的模型其实依赖于较低的内在维度，这意味着在模型适配过程中，权重的变化也倾向于展示出低“内在秩”。**
- **工作方式**：LoRA的方法就像是在这庞大的图书馆里精准地找到那些关键的书籍，并且专注于对这些书籍进行研究和更新。换句话说，LoRA通过改变模型的一些核心部分（即“关键书籍”），而不是对整个图书馆（即整个模型）进行调整。如`图1`所示，**密集层权重 𝑊 的变化可用较低的 "内在秩"的矩阵，变化矩阵的分解矩阵 和𝐴和𝐵 也是低秩矩阵，通过在适配过程中优化低秩分解矩阵 和𝐴和𝐵 来实现的，同时保持预训练的权重 𝑊 不受影响。**
- **主要优点**：这种方法的优势非常明显。首先，它极大地节省了空间，因为我们只需要专注于那些关键的几本书，而不是整个图书馆的所有书籍。其次，它也在时间和资源上更加高效，因为我们仅仅专注于最重要的部分。LoRA在存储和计算效率上都表现卓越，显著降低了存储需求和任务切换的成本，并减少了对硬件的依赖。此外，LoRA还能与多种现有方法（例如前缀调整）相结合使用，进一步增强其实用性和灵活性。

**方法**：

**低秩参数化更新矩阵**：神经网络由许多执行矩阵乘法的层组成，**通常这些层的权重矩阵是满秩的**。但研究发现，**预训练语言模型的 "本征维度 "较低 ，即使随机投影到较小的子空间，仍能高效学习**。受此启发，作者认为在适配其它任务过程中，**权重的更新** ∆𝑊∈𝑅𝑑×𝑘 **也具有低“内在秩**”。因此，采用低秩分解来约束权重矩阵的更新，预训练权重矩阵 𝑊0∈𝑅𝑑×𝑘 , 其中，，𝑊0+∆𝑊=𝑊0+𝐵𝐴,其中𝐵∈𝑅𝑑×𝑟，𝐴∈𝑅𝑟×𝑘，𝑟≪𝑚𝑖𝑛(𝑑,𝑘) 。

 **训练训练过程：**训练过程中， 𝑊0 保持不变，不接收梯度更新，而 𝐴 和 𝐵 包含可训练参数，通过调整它们可以改变 ∆𝑊 ，从而适应新任务。由于 𝑊0 和 Δ𝑊=𝐵𝐴 的输入相同，对于 ℎ=𝑊0𝑥 ，训练过程修改前向传递为： **ℎ=𝑊0𝑥+Δ𝑊𝑥=𝑊0𝑥+𝐵𝐴𝑥** ，如在`图1`中展示了的重参数化。对 𝐴 使用随机高斯初始化，对 𝐵 初始化为零，因此在训练开始时 ∆𝑊=𝐵𝐴 为零。为了调参时，改变秩 𝑟 对其它训练参数影响较小，我们将 ∆𝑊𝑥×𝛼𝑟 ，简单地将 𝛼 做为常数，并设置为我们尝试的第一个 𝑟 。这是因为使用 Adam 优化时，调整 𝛼 大致相当于适当缩放初始化后调整学习率。

**全微调的泛化：**更一般的微调形式允许训练预训练参数的一个子集。LoRA更进一步，不要求在适应期间对权重矩阵的累积梯度更新具有满秩。这意味着，当将LoRA应用于所有权重矩阵并训练所有偏差时，我们通过将LoRA秩 r 设置为预训练权重矩阵的秩，大致恢复了全面微调的表现力。换句话说，随着我们增加可训练参数的数量（增加LoRA秩r），训练LoRA大致收敛到训练原始模型，而基于适配器的方法收敛到一个MLP，基于前缀的方法收敛到一个不能接受长输入序列的模型。

 **无额外推理延迟：**在生产环境中，我们可以计算并存储，其中和𝑊0+𝐵𝐴，其中𝑊0和𝐵𝐴 ∈ 𝑅𝑑×𝑘 ，然后进行推理。当需要切换到另一个下游任务时，我们可以通过减去 𝐵𝐴 来恢复 𝑊0 ，然后添加不同的 𝐵′𝐴′ ，这是一个非常快速的操作，内存开销很小。这确保了与微调模型相比，在构建上不会引入额外的推理延迟。

**应用方法**: LoRA技术可以应用于神经网络中任何权重矩阵的子集，以减少模型的可训练参数。这对于Transformer架构特别有用，其中自注意力模块包含四个权重( 𝑊𝑞,𝑊𝑘,𝑊𝑣,𝐾𝑜 )矩阵，MLP模块包含两个群权重矩阵。在实践中，**LoRA主要用于优化注意力权重**( 𝑊𝑞,𝑊𝑘,𝑊𝑣 )，并冻结MLP模块，以简化模型并提高参数效率。并将MLP层、LayerNorm层和偏置的实证研究留作未来工作



**我们应该对transformer中的哪些权重矩阵应用LoRA？**

 在有限的参数预算下，研究探讨了在下游任务中获得最佳性能时应该使用LoRA来适应哪些权重类型。研究考虑了GPT-3 175B模型的96层，设置了18M的参数预算，对于适应一种类型的注意权重，秩r为8，对于适应两种类型，秩r为4。结果表明，将所有参数放入 Δ𝑊𝑞 或 Δ𝑊𝑘 中会导致性能显著下降，而**同时适应 𝑊𝑞 和 𝑊𝑣 会产生最佳结果**。因此，**适应更多的权重矩阵比适应具有更大秩的单一类型的权重更为有利**。这一结果强调了在有限参数预算下的权重适应策略。

![img](https://pic1.zhimg.com/80/v2-a3156110f3274b8e8f410d6a11788148_720w.webp)

表5：在GPT-3中将LoRA应用于不同类型的注意权重后，在相同数量的可训练参数下，对WikiSQL和MultiNLI进行的验证准确性。适应Wq和Wv两者在整体性能上表现最佳。我们发现在给定数据集中，不同随机种子之间的标准差是一致的，我们在第一列中进行了报告。

**LORA的最佳秩是什么？**

 该研究关注了在自注意模块中使用不同秩( 𝑟 )的适应矩阵对模型性能的影响。如`表6`所示，研究比较了适应不同类型的权重矩阵组合，包括 、{𝑊𝑞,𝑊𝑣}、{𝑊𝑞,𝑊𝑘,𝑊𝑣,𝑊𝑐} 和仅适应 𝑊𝑞 。令人意外的是，**结果表明LoRA在非常小的 𝑟 值下已经具有竞争力，特别是对于 {𝑊𝑞,𝑊𝑣} 的情况。这表明适应矩阵的更新矩阵∆W可能具有非常小的“固有秩”**。**并且∆Wq 似乎比 ∆Wv 具有更高的 "内在秩"**

 为了支持这一观点，研究还分析了不同r和不同随机种子学习的子空间之间的重叠性。发现增加r并不会覆盖更有意义的子空间，而适应矩阵的前奇异向量方向在模型性能中起着关键作用。这些方向在不同r和不同随机种子下都表现出明显的重叠，说明适应矩阵的秩可以非常低。**增加 r 并不能覆盖更有意义的子空间，这表明低秩适应矩阵就足够了**

 总的来说，研究发现低秩的适应矩阵已经足够有效，前奇异向量方向对于模型性能具有重要意义。这一发现对于理解和优化模型适应过程具有重要意义。

![img](https://pic1.zhimg.com/80/v2-bf0be63f74d785a3a3132cf75ec0b0c0_720w.webp)

表6：在WikiSQL和MultiNLI上用不同的秩r进行验证的准确性。令我们惊讶的是，在这些数据集上，**r小到1就足以适应Wq和Wv**，而单独训练Wq需要更大的r。

**7.3 适应矩阵 Δ𝑊 与 𝑊 相比如何？**

 研究调查了适应矩阵 Δ𝑊 与权重矩阵 𝑊 之间的关系，并解答了几个关键问题。首先，研究发现 Δ𝑊 与 𝑊 之间存在较强的相关性，表明 Δ𝑊 在某种程度上放大了 𝑊 中已有的一些特征。其次， Δ𝑊 并不是简单地重复 𝑊 的前奇异向量方向，而是主要放大了那些在 𝑊 中没有强调的方向。第三， Δ𝑊 的放大因子相当大，这意味着它在特定情况下具有显著的影响力，尤其是对于 𝑟=4 的情况。值得注意的是，当 𝑟=64 时，放大因子较小，如表7所示。具体原因在`附录H.4`节有进一步讨论。

 Δ𝑠 这些发现暗示着低秩的适应矩阵可能会在下游任务中放大通用预训练模型中已学到但未强调的关键特征，为适应预训练语言模型提供了深入的理解。

![img](https://pic3.zhimg.com/80/v2-f6d50e67862e34c48314a721f105069a_720w.webp)

表7：U&gt;WqV&gt;的Frobenius范数，其中U和V分别是（1）∆Wq、（2）Wq或（3）随机矩阵的左/右前r个奇异向量方向。这些权重矩阵来自GPT-3的第48层。



### X-LORA（混合LORA）

代码：X-LoRA codes and examples are available at https://github.com/EricLBuehler/xlora. Model weights and data associated with the X-LoRA discussed in this paper, along with additional examples, are available at https: //huggingface.co/lamm-mit/x-lora. The X-LoRA-Gemma model is available at https://huggingface.co/lamm-mit/x-lora-gemma-7b. The Mistral.rs inference engine is available at https://github.com/EricLBuehler/mistral.rs.

简介：这是一个用于大模型微调的插件。X-LoRA 模型可以轻松地应用于任何现有 的大型语言模型（LLM），而**无需修改底层结构**。这项工作的影响包括获得**具有强大领域知识和跨领域知识整合能 力的可随时扩展和适应的模型**

​		LORA:大型语言模型（LLMs）已经获得了极大的普及，然而，训练这类模型的成本可能很高，尤其是当需要不同的能力集时。低阶适配器（Low-rank adapters， LoRA）等方法已被提出作为一种更有效的替代方法，**但适配器通常侧重于较窄的知识领域**。LoRA 建模的基本概念是使用低秩矩阵添加到原始全矩阵中，并选择这些低秩矩阵作为模型中唯一可训练的部分。由 于只有适配器层是可训练的，这些模型通常**保留了基础模型预训练时获取的知识**，同时使模型更适用于特定 任务，而且在计算需求方面也很高效。由于 LoRA 适配器的训练效率很高，因此有可能利用这种策略开发出 **大量专用模型**。

​		X-lora:在这里，我们提出了一种计算高效的专家**混合**方法，使用一组不同的 LoRA 适配器，这些适配器可以很容易地进行单独训练。



思考：假设开发出大模型 可以参考一下用 









### 用于化学反应预测的自我反馈知识激发方法(prompting+反应类型，用于LLM)

代码：https://github.com/AI-HPC-Research-Team/SLM4CRP

简介：**CRP 的关键在于准确识别各种条件下涉及键断裂和重构的机理和结果**。本文开发出一套**动态提示模板**（prompting）并**通过聚类提取了化学反应可能的反应类型**。然后采用**自适应提示学习**（提示学习包括设计输入 "提示"，引导 LLM 执行特定任务或生成特定类型的反应），将上述先验知识（知识先验指的是预先存在的、特定领域的知识，这些知识可以集成到人工智能模型中，以增强模型的理解和预测能力）注入大语言模型（LLM），提升其表现。

![image-20240628172515175](.\image-20240628172515175.png)



先前问题：静态模板提示会导致 LLMs 中的指导模式僵化，从而可能影响其可遗传性。动态 提示可以解决静态模板的 局限性，将先验知识注入 LLM。数据集通常缺乏 RT 标签（反应类型），只包括反应物和 产物，而识别 RT 可以缩小待探索的化学空间。

优点：我们的方法通过自适应提示学习和自反馈知识激发技术，**将 RT 知识先验与 LLM 相结合**。**它解决了真实世界数据集中 RT 信息稀缺的问 题**，而且动态提示避免了 LLM 中僵化的模式引导，为提高预测准确性提供了一种解决方案。



**数据集**：在本研究中，我们利用了 Mol-Instructions [15] 数据集 中的 "面向分子的指令"，其中包括三个化学反应任务。

**模型方法**：三个阶段：知识提取阶段、数据整理阶段、微调阶段

![image-20240628182514595](.\image-20240628182514595.png)

​	**知识提取阶段**：（在训练集上）

​		1.反复推敲最优聚类方法（即反应类型RT）的选择。

​		首先要嵌入（embedding）训练数据集 Dtrain 的输入和输出，然后将其聚类（使用K-means）为相关群组，并将其注释为 RT。**即通过无监督聚类根据输入输入自动地将训练集划分为N类反应。**经试验，将 输 入 和 输 出 连 接 成 一 个 向 量 （ **concat**(inputvec , outputvec ）效果最好，N=10效果最好。

​		常见的基本反应类型有几种，包 括但不限于合成反应、分解反应、单置换反应和双置换 反应。虽然可以对这些基本类型进行细分，**但过细的分 类可能不利于有效预测模型的推广和效率**。更重要的是 ，随着聚类数量的增加，无监督标记的准确性可能会降 低。此外，聚类数量过多可能会使模型的可解释性复杂 化，从而使用户更难理解和信任模型的预测结果。考虑 到这些因素，**建议簇的数量在 3 到 12 个之间**[45]。

​		2.训练 LLM-RT，从而形成自我反馈聚类机制。

​		![image-20240628174647166](.\image-20240628174647166.png)![image-20240628175128114](.\image-20240628175128114.png)![image-20240628175212312](.\image-20240628175212312.png)

​	**数据整理阶段**:(测试集、验证集)

​		3.冻结的 LLM-RT 在输入的同时使用提示，在验证集和测试集上执行 RT 注释。

​		利用 **经过训练和冻结**的 LLM-RT，我们为验证数据集和测试 数据集注释 RT

​	**微调阶段**:

​		4.结合提示增强功能对 LLM 进行微调

​		为了解决静态模板的限制并提高模型 的泛化能力，我们为模板选择引入了**自适应选择器**。 我们建立了一个指令模板库，为每个任务分配 12 个不 同的模板，共计 36 个模板。在嵌入过程中，数据集 D 中的输入i 和指令模板库中特定任务列表中的所有相应模板都通过 LLM-CRP 模型嵌入。然后，自适应选择 器通过输入嵌入和库中每个模板嵌入之间的向量差异（**欧氏距离**）来评估适应性。对于每个批次，这一过程需要将单个 输入与多个指令进行匹配，以根据适应性分数确定最 适合的指令。**适应性选择对输入适应性最强的模板**，以促进精确的知识注入

​		选定的自适应指令与当前输入的相应 RT 相结合， 形成增强提示。



思路：1.可以利用其他模型（LLM）中获取数据（比如化学反应，比如其他模型预测的错误反应作为难负例），以扩充数据集

​			2.划分反应类型的想法不仅可以用在大模型，这是一种数据增强的方法，在小模型训练时也可以用。

​			3.可以借鉴Mol-Instructions数据集+USPTO划分反应类型。



### T5 BioT5 BioT5+

代码：

​		1.T5(HUG，可直接调用):https://hf-mirror.com/google-t5/t5-base   GIT: https://github.com/google-research/text-to-text-transfer-transformer



T5:LLM三大流派之一（**Encoder-Decoder**）,LLM的三大流派的两个起始模型：GPT-1（Decoder only）、BERT（Encoder only），但是这两个模型针对不同下游不同的NLP任务时还需要进行一定的修改（如添加一些线性层），Google经过庞大的预训练，最终提出了一个通用框架T5模型（Encoder-Decoder）， **将所有NLP任务转化为text to text任务**，**微调时无需再修改模型**，直接在原模型上微调即可。

​	**T5最核心的理念**是：使用前缀任务声明及文本答案生成，统一所有自然语言处理任务的输入和输出。在此之前的几乎所有预训练语言模型，在下游任务微调过程中都需要添加非线性层，将模型的输出转化为任务指定的输出格式。将**每个**文本处理问题都视为 "文本到文本 "问题，即以文本为输入，以新文本为输出

下图所示为T5的输入格式和输出格式。绿色部分表示翻译任务，红色和黄色部分分别表示CoLA（单句分类）和STS-B（文本语义相似度）任务，蓝色部分表示摘要生成任务，左侧的框表示T5的输入样例，右侧的框则是对应的输出结果。

![c29e931cc09049feb2bff69377c94d56](.\c29e931cc09049feb2bff69377c94d56.png)

**T5唯一需要做的就是在输入数据前加上任务声明前缀**，如：

英德翻译：translate English to German：That is good.
情感分类：sentiment：This movie is terrible!

**此时就获得了完整的 T5 模型及其训练方法：**

- Transformer Encoder-Decoder 模型；
- BERT-style 式的文本破坏方法（MLM）；
- Replace Spans 的文本破坏策略（若干个连续词一起替换，只预测被替换的词。例如：Thank you`<X>`me to your party`<Y>`week.|`<X>`for inviting`<Y>`last`<Z>`，用“|”分割T5的输入和输出）；
- 15 %的文本破坏比；
- Replace Spans破坏时小段长度为3。



启发：

​		1.理想情况下， 这种（基于大规模数据集的无监督）预训练能使模型发展出通用能力和知识，然后将其迁移到下游任务中，数据集越大，模型规模越大，效果也就越好。

​		2.迁移学习的一个有益用途是在**低资源任务中获得良好性能**的可能性



**BioT5,BioT5+**:将T5模型用于生物数据集。

亮点：解决了无效分子微笑的产生，上下文信息的利用不足，以及结构化和非结构化知识的平等对待等问题。

​		(1)我们主要集中在 两个生物学模态 ——分子和蛋白质——**以文本作为知识库和桥梁来丰富 分子和蛋白质领域的基础关系 和性质**； 然而，分子、蛋白质和文本代 表了完全不同的语言。这三种不同情态中的同一 个标记具有不同的语义（记 号“C”在自然语言中表示字符 C，在分子中 表示碳原子，在蛋白质中表示半胱氨酸(20 种 氨基酸 之 一 )）。		

​		**我们对分子、蛋白质和文本使用不同的词汇**。在 BioT5 中，**分 子由 SELFIES 字符串表示**，其中每个化学上有意义 的原子团都 包 含 在括号中 (原始 T5 字典是 使用句子片断 (Kudoand Richardson,2018).然 而，直接使用这个分子微笑词典是次优的，因为 一些化学上有意义的标记，如官能团或完整的原 子，将被不准确地识别。如Br被拆分为B和 r)， 并标记 为 SELFIES 标 记 。比如 [C][=C][Br] [C] ， [=C]，[Br]。**对于蛋白质**，为了区分文本中大 写字母的氨基酸，我们为每个氨基酸引入一个 特殊的前缀< p>。 **对于文本**，我们使用与原始 T5 相同的字典。![image-20240702155406040](.\image-20240702155406040.png)

​		(2)我 们使用**多任务预训练**以更全面的方式对这三种 模态之间的联系进行建模。

​		(3) **使用 SELFIES** 提供了更健壮和容错 的分子表示替换SMILES，消除了 SMILES 经常遇到的非法结构的问题。



**BioT5+**，是 BioT5 框架的扩展，专为加强生物研究和药物发现而量身定制。**BioT5+ 融合了几项新功能**：整合 IUPAC 名称以促进分子理 解、纳入来自 bioRxiv 和 PubChem 等来源的大量生物文本和分子数据、多任务结构 调整以实现跨任务的通用性，以及数字标 记化技术以提高数字数据的处理能力。这 些改进使 BioT5+ 能够弥合分子表征与其文 本描述之间的差距，提供对生物实体更全 面的理解，并在很大程度上改进了生物文本和生物序列的基础推理

​		生 物 文 献 中有大量关于分子 和蛋白质的信息。当这些文献中提到一个生 物实体时，其上下文主要围绕着对该实体某 些特征的描述。因此，越来越多的工作致力 于**文本和生物实体的联合建模**。

​		简而言之，BioT5+ 包含 以下重大改进：

​		 (1) 增强对分子的理解：**通过将 IUPAC 名称 整合到 BioT5+ 框架中，该模型可以更深入地 理解分子结构**。这种整合使 BioT5+ 能够解释 通常出现在科学文献中的化学名称，缩小了正 式分子表征（如 SELFIES）与文本描述之间 的差距。因此，这增强了对分子的理解，有助 于对分子特性和活性进行更准确的预测和分析 。

​		 (2) 扩展的生物文本和分子数据：与 BioT5 相比，BioT5+ 包含来自 bioRxiv（Sever 等人 ， 2019 年 ） 和 PubMed （ Canese 和 Weis ， 2013 年；White，2020 年）等来源的大量生物 文本数据，以及来自 PubChem（Kim 等人， 2019 年）的高质量分子数据。这种扩展不仅扩 大了模型的知识基础，还丰富了对生物实体的 文字理解。

​		(3) 多任务指令调整：BioT5+ 针对下游任务 采用多任务指令调整策略，而不是针对每个任 务单独进行专门的模型训练。通过利用统一的 多任务训练框架，BioT5+ 可以无缝整合来自不 同任务的知识，增强其在不同生物和化学领域 的预测能力和泛化能力。

​		(4) 高级数字标记化：鉴于数值表示法的局限性，BioT5+ 从 Llama （Touvron 等人，2023a）模型中汲取灵感， 整合了先进的基于字符的数值标记化策略。 这种技术可以更细致、更一致地表示数值。



数据集：详见BioT5(3.1节)，BioT5+(3.2节)化学可用的有分子-文本数据、小分子数据

启发：

​		1.多模态建模时要区分不同模态下相同字符表示的不同语义r

​		2.采用多模态数据集大有脾益（如文本描述的化学性质、分子-文本数据），分子数据和文本数据联合预训练的有效性。 可以利用chatgpt等模型生成对USPTO数据集中反应的反应原理描述？

​		3.分子-文本的更多工作见BioT5+（2.1节）



### PMSR（无模板逆合成，transformer+smiles，首个针对化学目标预训练任务的反应序列到序列模型，SOTA）

简介：逆合成预测更有可能是**条件生成**，而不是翻译，因为 X（反应物） 和 Y（产物） 共享相同的词汇，而且产物中 的所有原子都出现在反应的前体中。针对无模板方法面临的三个挑 战：1) 生成的分子无效。2）生成的反应物 违反原子守恒定律。3) 生成的反应物不反应或不生成 目标产物，设计三种解决方案：**即屏蔽元素恢复 、屏蔽碎片恢复和反应分类**，并定制三种预训练任务：**自动回归、分子复原和对比逆合成分类**，并运用指针生成器以提高稀有原子的生成。采用transformer架构达到无模板的SOTA

​		基于模 板和半模板的方法在很大程度上依赖于模板或原子-原 子-映射，众所周知，无模板逆合成较少受到 模板通用化问题和原子映射难题的困扰。但在无模板方法 的几个遗留问题中，**不符合化学规则的问题比较突出**。我们通过分子重构预训练任务来执行原子守恒规则，通过反应类型 引导对比预训练任务来执行决定反应中心的反应规则。

​		除了自动回归外，我们还针对前两个挑战提出了一 个分子恢复任务，该任务具有区域掩码。被掩蔽的元 素会被周围其他可见元素复原，这有助于模型生成有 效的分子。这些被掩蔽的元素也有望由给定的产物预 测出来，从而促使模型遵循原子守恒定律。此外，人 们普遍认为，反应类型作为先验知识可以大大提高回 溯分析的性能。因此，我们提出了 PMSR 中的监督对比任务



**逆合成预测的三大问题及解决方法：**

![image-20240718144029608](.\image-20240718144029608.png)

​		1.**分子的基本化学规则** 所有分子都必须遵守**基本的价键理论**。例如，一个氟原子决不能与其他三个原子相连 ，如图 1(a) 所示的无效分子。作为一个生成问题，有必 要学习化学规则，以便生成的分子是有效的。然而， 这些化学规则是隐含的，模型并不能完全按照规则生 成有效的分子。 

​		解决方法：我们不希望模型在分子的某个位置生成错误的原子或键。因此，我们设计了一个**屏蔽元素恢复任务**（类似于MLM）。给定一个分子，我们屏蔽其中的一些原子和 化学键，然后由模型恢复这些被屏蔽的元素。这项任 务有助于模型学习分子的基本规则，避免 "非法 "原子 或化学键。例如，如图 2(a)所示，模型不会恢复带有氟原子的被遮蔽元素。此外，更多的训练数据也能提高生 成分子的质量

​		2.**反应中的守恒** 我们注意到，许多由单步逆合成预测模 型，特别是无模板模型产生的不正确结果，并不符合**原子守恒定律**（Law of Conservation of Atoms）。也 就是说，**反应物中除反应中心外的其他部分在反应中 发生了变化**。例如，在图 1（b）中，甲基被错误地连 接到了羧基的正交位置，与生成物完全不同。 

​		解决方法：我们认为，在生成的前体中，产品中的功能基团是错位的。因此，我们设计了一个**掩蔽片段恢复任务**，在 该任务中，模型根据相应的产品恢复前体中被掩蔽的 连续元素。每个被掩蔽的片段都包含多个原子甚至官 能团，模型需要根据给定的产物将它们按照正确的顺 序排列。与屏蔽元素复原任务不同，屏蔽片段复原任 务鼓励模型保持产品和前体之间的一致性。例如，在 图 2（b）中，甲基可以附着在羧基的正位或偏位上， 但根据产物，只有偏位才是合理的。 此外，**对于稀有原子，我们建议使用指针生成器**（ See、Liu 和 Manning，2017 年；Nishida 等，2019 年 ），它提供了直接从产物中复制元素的机会。复制机 制有助于生成一些出现在产物中的稀有原子，而不是 错过它们。

​		3.**反应中心的选择性** 最具挑战性的问题在于选择正确的 反应中心，即产物中的一小块区域。给定一个目标产 物，通常会有几个候选反应中心，但其中一些不可行 ，违背了化学反应的机理。在图 1(c) 中的苯环上，乙 酰基的对位不能连接甲氧基。因此，模型也有必要学 习这些化学知识，以避免此类错误。 

​		解决方法：之前所有工作（Dai 等人，2019 年；Coley 等人，2017 年）的实证结果都表明，加入 反应类型信息后，性能有了显著提高。**这是因为按反 应中心模式分类的反应类型**（Schneider 等人，2016 年 ）**提供了对反应中心的宝贵见解，而逆合成预测的性 能在很大程度上取决于所识别的反应中心的正确性。** 因此，反应分类任务足以教会模型更加关注反应中心-只有模型关注反应中心，才能正确预判反应类型。在 图 2（c）中，最可能的反应类型是 C-C 键的形成，因 此甲氧基几乎不可能作为反应中心。

​	单步逆合成预测面临的这些挑战都与化学规则有关 。为了学习通用和正确的化学规则，**在大型数据集上 进行化学预训练是一种可行的解决方案**。为此，我们 提出了一种用于单步逆合成的预训练模型。



**数据增强：**

​		单步逆合成预测通常使用经典的 SMILES 字符串（ Schwaller 等人，2019 年）来限制生成的随机性。不过 ，Tetko 等人（2020 年）提出，分子的随机表示可能 是逆合成的一种增强方法。**更多的训练数据也有助于 模型学习化学规则**。此外，我们发现规范化的 SMILES 表示法不能保持不同分子中相同子图的一致 性。换句话说，规范化 SMILES 有利于生成独特的结 果，但不利于从 SMILES 字符串中学习结构信息。因 此，**在预训练期间，我们提供相同结构的不同 SMILES 字符串，这有助于模型学习不同字符串之间 的等价性**。我们采用了 Tetko 等人（2020 年）提出的 增强方法，将产品、前体、反向前体和产品的 SMILES 表示法纳入其中。



**PMSR模型概览**：PMSR 是一个序列到序列的化学反应模 型，具有一个基于transformer的编码器和一个基于transformer的解码器,并采用了指针生成器（pointer-generator）。一个 6 层编码器和一 个 6 层解码器组成，嵌入大小为 768，前馈滤波器大小 为 2048，注意头为 8

<img src=".\image-20240718145853341.png" alt="image-20240718145853341" style="zoom:100%;" />![image-20240718152250900](.\image-20240718152250900.png)

​	**预训练任务**：我们设计了三种预训练任务，即**分子复原（MR）、 自动回归（AR）和对比分类（CC）**。MR 和 AR 对编 码器和解码器都进行了预训练；CC 是在一批数据中学 习的，这就像是对模型进行正则化。这三个预训练任 务同时进行优化。

​		**AR**：我们的预训练数据是纯粹的反应数据，因此 我们首先设计了一个有监督的自动回归任务，该任务 与我们的下游回溯任务相同。

![image-20240718150628581](.\image-20240718150628581.png)

​		**MR**：我们**将分子复原中的掩码元素复原和掩码片段复原任务结合起来考虑**。具体来说，我们使用跨度掩码（Joshi等人，2020）为每个分子生成长度为[1，10]的MASK。MASK后来被应用于SMILES分子串。一个MASK屏蔽覆盖一个原子或一个键，目标是恢复被屏蔽的元素。多token mask覆盖SMILES字符串的一部分，其中隐藏了分子的至少一个片段。解码器预测来自给定产品和前体未掩蔽部分的masked token。在图4中，6-token面具可以通过产物和反应物的双环的其他部分回收。这样，解码器往往会根据化学规则产生前体。与自回归一起，**解码器**的损失被定义为![image-20240718152004709](.\image-20240718152004709.png)为了提高**编码器**的通用性，我们还在编码器上添加了MR任务。编码器通过在预训练期间了解源序列的上下文和语法来恢复被屏蔽的令牌。编码器的损失函数是![image-20240718152143790](.\image-20240718152143790.png)



​		**CC**:以往基于transformer的方法，如 MT（Schwaller 等，2020b）、SCROP（Zheng 等，2019）和 DMP（ Zhu 等，2021），都没有考虑反应中心，而反应中心是 逆向论文预测的关键。如前所述，我们设计了一个**强制分类任务**来帮助模型学习不同反应中心的特征。我 们没有直接为每个反应制定分类任务，因 为 我们认为 对比学习更能抵御破坏；许多反应被标记为未识别， 这与 NameRXN（NextMoveSoft-ware 2022）的所有类 型模板不匹配。**对比损失强制要求同一类型中的所有 反应都具有相似的嵌入**，在对比分类中，**产品的特征由编码器提取,前体的特征也由解码器提取,我们通过连接将这两部分合并,按 照 Khosla 等人（2020）的方法，我们添加了一个全 连接层作为投影层，即 z = FC(r)**。因此，对比分类 损失为![image-20240718153309784](.\image-20240718153309784.png)



​	**损失函数：**在编码器中，我们使用**掩码标记的交叉熵 损失**。此外，在解码器的自动回归和分子恢复任务中 ，我们使用**标签平滑交叉熵损失，标签平滑系数为 0.1** 。在对比分类中，投影层有 2048 个隐藏单元，我们通 过 0.1 的权重对对比损失进行正则化。我们使用 Adam 优 化 器 （ Kingma 和 Ba ， 2015 年 ） ， 并 用 Noam （ Vaswani 等，2017 年）计划改变学习率，预热步数为 8000 步。

**下游任务的微调** 在微调过程中，我们共享编码器和解码器的所有参数，只保留自动回归的投影头。在微调过程中，分子重组和对比分类将被删除



**数据集：**

​	预训练数据集：Pistachio（Mayfield、Lowe 和 Sayle，2017 年），我们在预训练时没有移除试剂， 这样我们的模型就有机会在有试剂或无试剂的情况下 对 逆 合 成 预 测 进 行 微 调

​	微调数据集：微调数据集来自美国、欧洲和 WIPO 专利

![image-20240718174952913](.\image-20240718174952913.png)

其中USPTO系列与Pistachio反应数据集其实都是利用Lowe于2012年开发的Java程序（patent-reaction-extraction，https://github.com/dan2097/patent-reaction-extraction）自动从美国专利数据库中提取的。



启发：1.假如采用图神经网络，是否可以借鉴这几种预训练任务？图神经网络可以MLM吗

不足：1.采用的是smiles，可能还是不如图  2.没有引入多模态信息 3.仍然没有考虑环（序列可以考虑环吗）4.分子的基本化学规则可以通过已有数据集“告知”模型（比如某些描述了分子性质的数据集）    是否可以通过聚类“告诉”模型反应类型

数据集：chebi-20：检索自然语言描述的相关分子（可以用于从自然语言生成分子等）



### RetroKNN(LocalRetro的进一步工作，引入KNN辅助预测)

简介：在LocalRetro基础上额外引入KNN检索模板，最终GNN 预测模板和 KNN 检索模板会以不同的权重合并（利用一种轻量级适配 器，用于在结合神经网络和 KNN 预测时根据隐藏表示 和检索模板调整权重），形成最终输出。

![image-20240718180535206](.\image-20240718180535206.png)

方法：

​		首先 利用训练有素的图神经网络（GNN）进行逆合成任务 ，然后离线构建包含反应模板的原子模板和键模板存 储（第 2.1 节）。在存储构建阶段，我们遍历训练数 据中的所有目标分子，并将每个原子和每个键的模板 添加到相应的存储中。这些模板是由 GNN 提取的隐 藏表示法生成的。在推理过程中，对于给定的新目标 分子，我们首先使用原始 GNN 提取隐藏表示以及原 始 GNN 预测模板。然后，我们使用隐藏表征搜索这 两个存储空间，以检索与查询相似的本地模板。GNN 预测模板和 KNN 检索模板会以不同的权重合并，形 成最终输出。

​	存储建构阶段：（在训练集中）作者用训练好的LocalRetro模型的MPNN部分（图1红色框内GRA前的部分）作为Feature Extraction GNN**计算分子图节点和边隐藏表示hv和he**，并与分子对应的atom/bond template中的反应中心进行比对，**将属于反应中心的隐藏表示归类为对应的LocalTemplate，不属于反应中心的隐藏表示标记为0，即不对应任何LocalTemplate**(对于每个节点v，如果它是反应中心，我们将由隐藏表示hv索引的模板t添加到SA。否则，我们添加一个特殊的标记0来指示此处未应用模板。类似地，对于每个边e，我们将（he，t）或（he，0）添加到键存储SB。最后，我们得到推断期间使用的原子存储SA和键存储SB。)这样就构建了用kNN对节点和边所属LocalTemplate进行分类的数据集atom/bond store(SA、SB)。**kNN可以通过输入的新分子图上节点和边的隐藏表示ha和hb预测其所属的LocalTemplate**，预测的概率与ha和hb到数据集中其他隐藏表示hi的距离d有关，如公式(1)(2)所示。公式中Na和Nb分别表示atom store和bond store数据集；ta和tb分别表示atom template和bond,I 是指标函数，只有当条件（即 ta = ti 或 tb = ti ）为 1 时才输出 1。![image-20240718181615665](.\image-20240718181615665.png)

​	最终，预测的概率由LocalRetro与kNN的预测概率经过公式(3)(4)加权计算得到。公式(1)(2)中的两个温度参数TA、TB以及公式(3)(4)中LocalRetro预测概率的权重λA、λB通过一个神经网络（图2中Adapter）由输入分子图节点和边的隐藏表示hv、he和hv、he到kNN数据集中数据点的距离计算得到。

![image-20240718181742266](.\image-20240718181742266.png)



![image-20240718182323940](.\image-20240718182323940.png)

**适配器架构**:GIN+MLP



### ChemDFM-X:首个化学跨模态对话基础模型

简介：将2024上半年大模型的工作（LLM 解码器 + 模态编码器）搬到化学领域。考虑到化学本质上是一门**多模态学科**，纯文本 LLM 所能实现的功能十分有限。对于每种模态编码器，要么使用已有的模型，要么重新训一个。对于数据，容易获得GT的直接用，难以获得GT的就使用近似计算（如分子的3D构象，红外光谱数据）

![](.\image-20241114103826237.png)

架构：

​	大模型:ChemDFM

​	投影层：两层MLP，中间夹着GELU激活层

​	模态编码器：

​		结构模态：

​			分子图：Molecular Net

​			分子构象:Unimol

​			图像：CLIP（将原始分辨率的图像裁剪成符合 CLIP 输入大小的多个子图像，然后利用 H-Reducer的特定模块将 CLIP 的输出序列长度减少 n 倍）

​		表征模态：

​			质谱：自己训的Transformer,训好后用encoder部分

​			红外光谱：同质谱



![image-20241114103918387](.\image-20241114103918387.png)

训练：在每个模态的训练过程中，我们会**冻结**预先训练好的ChemDFM的参数，以保持其已经获得的高级自然语言和SMILES处理能力。对模态编码器和投影模块都进行了训练，以使编码器输出与 ChemDFM 输入更加一致（与大模型类似）。

数据：实现这一目标的关键挑战之一是缺乏足够的**模态对齐数据**。为了解决这个问题，我们建议**通过 SMILES 转换来补充其他模态的数据**。我们注意到，由于实验和量子化学计算耗费过多，大 规模数据难以获得，尤其是串联质谱（MS2）和红外光谱（IR）等表征模式的数据，因此我们利用**简化近似计算和自然模型预测**来获得次优但接近的结果。通过这种方法，我们最终生成了 一个多模态指令调整数据集，其中包含来自 130 万个分子 SMILES 的 760 万个跨模态数据。

​	总的来说，就是找几个主要数据库里的smiles，并将其转为相应模态

​	分子图数据集：![image-20241114104639812](.\image-20241114104639812.png)

​	分子构象数据集：![image-20241114104729698](.\image-20241114104729698.png)

​	不过，值得注意的是，从 SMILES 中获取分子构象比从 SMILES 中获取分子图形要复杂得多。理想情况下，需要基于量子化学进行严格计算，以获得分子势能最低的 "最佳 "构象。然而，量子化学计算的成本很高，尤其是对于原子数量较多的分子。为了解决这个问题，我们采用了与 Uni-Mol相同的近似计算过程，以获得优化算法下的近似构象。具体来说，我们**首先使用 RDKit生成原始分子构象**。然后，使用**默克分子力场（MMFF） 算法对分子构象进行优化**，得到分子的**伪最佳构象**。

​	图像数据集：![image-20241114104953554](.\image-20241114104953554.png)

​	为了使图像风格多样化，我们采用了三种不同的方法生成分子图像。首先，我们直接使用两种不同的工具包，即 **RDKit和 Indigo**。我们还对生成的图像进行了传统的图像增强方法，如**随机灰度转换和色彩抖动**。此外，为了让 ChemDFM-X 熟悉手写体图像，我们利用中提出的管道生成**伪手写体图像![image-20241114105105858](.\image-20241114105105858.png)**



​	质谱数据集：![image-20241114105147247](.\image-20241114105147247.png)

​	分子图、分子构象和分子图像提供了有关已知分子结构的信息，然而，自然界中存在大量**未知结构**的分子，或在实验过程中可能出现**未知结构**的分子。在这种情况下，光谱学成为确定分子身份的重要工具之一。目前还没有大规模的 MS2 光谱实验数据。为了解决这个问题，我们采用 了一种**基于预测的方法**（CFM-ID 4.0）来大批量生成质谱数据



​	红外光谱数据集：![image-20241114105757641](.\image-20241114105757641.png)

​	与 MS2 光谱类似，红外光谱也存在实验数据成本高昂的问题。更糟糕的是，红外光谱计算的费 用也超出了可接受范围。因为分子红外光谱的模拟需要经过耗时的 DFT 过程，而大容量的红外计 算对算力提出了更高的要求。基于上述考虑，我们采用了一种**基于神经网络预测的方法** ChempropIR [42] 来生成红外数据。



启发：逆合成：与仅使用 SMILES 输入的结果相比，ChemDFM-X 在反应相关任务中 的性能有了显著提高。这一结果符合化学直觉，**即分子图和构象适合并常用于化学反应推断。**![image-20241114110035510](.\image-20241114110035510.png)

​	S - SMILES, M - MS2 Spectra, R - IR Spectra



### Syntheseus：一个具有广泛基准框架的合成规划库——重新评估逆合成算法

**简介**：提出了一套新标准，用于评估逆合成算法

**单步逆合成**：模型给出一种分子，并输出一步生成该分子的反应

**多步逆合成**：使用单步模型、一组可购买的分子和一些 可选的函数来寻找合成路线。

​			给定一个目标，采用了蒙特卡洛树搜索 (MCTS)、强化学习 (Reinforcement Learning)或启发式引导搜索算法，从起始分子开始，选择性地探索**一棵可能发生反应的树**。

单步逆合成中的陷阱：

​	1.只衡量召回率，不考虑精确度

​		top-k 准确度（召回率）实质上测试了**模型回忆数据集**的能力。特别是在使用较大的 k 时 ，这是一个有用的直观指标，可用于**评估模型是否能恢复数据集的全部多样性，而不仅仅是涵盖最常用的反应**。然而，除非 k = 1且top-1的准确率接近 100%，否则使用这种单步模型的多步搜索算法几乎肯定会建议使用数据集中未包含的 反应进行规划（**多表现为生成数据集中不存在的分子进行反应**）。如果 这些反应的质量或可行性较低，那么使用这些反应的路线就不会 有用。另一方面，在许多情况下，制造一种特定分子有多种可能 的方法，**而现有的数据集却很稀少**，大多数分子通常只有一种或几种可能的合成方法被报道过。因此，正如 Schwaller 等人之前所指出的，**单步模型的 top-k 精确度（前 k 个反应中有多少是可行的）对于多步搜索来说，可以说与召回率同等重要，甚至更为重要**

​		最佳实践：**除了召回率之外，作者还应努力评估其模型的精确度** ，至少应通过对几个示例的目视检查来进行评估。一些先前的工作使用前向反应模型（也称为反向翻译）的往返精度来衡量反应 的可行性，而这些反应并不一定是地面实况。总之，**我们鼓励使用前向模型作为往返准确性的替代指标**

​	2.后期处理：

​		我们认为最佳做法应该是在计算 top-k 精确度时**只考虑有效分子**。我们认为最佳做法是**在重复数据删除后再测量准确性**。立体化学经常未标注或标注错误，这促使即使预测结果的立体化学与数据集不同，也可被视为正确。我们认为最佳做法是报告**标准的精确匹配结果**， 并可能额外报告去除立体化学的结果，以提供更多有价值 的见解。

​	3.推理时间：

​		我们认为最好的做法是**在报告准确性的同时报告推理时间**

​	4.重点预测未知反应类型

​		我们建议研究人员将重点放在 **"反应类型 未知 "设置上**，因为这是**最直接适用于多步骤搜索**的设置。

​	5.通过原子-原子映射避免泄漏

​		大多数反应数据集都有原子-原子映射（AAM）注释，它将反应物分子中的原子映射到生成物中的相应原子上，为避免这一问题，在向模型提供输入分子时**应删除原子映射信息**，并在删除后重新对其进行规范化处理。



多步逆合成中的陷阱：

​	1.不受控制地改变单步模型

​		**许多算法不仅使用单步反应模型来定义搜索环境，而且还使用单 步模型中的排名或概率作为策略、 成本函数，或以其他方式指 导搜索 。当然，这也促使一些研究对单步模型进行修改 ，以提高搜索性能 。这些修改不仅改变了相对排名，也改变了所产生的反应集。**我们发现在实践中使用这种方法有**两个缺陷**。首先，除非对单步模型进行单独验证，否则不清楚该模 型是否仍能输出符合实际的反应：例如，溶解速率的变化可能 只是模型输出新的不符合实际的反应的结果。其次，即使不考 虑模型质量，使用不同的单步模型对搜索算法进行比较，本质 上也是对不同环境下的两种算法进行比较，这可能会使比较失 效。我们认为，这方面更好的做法**是训练策略模型，在不改变 可行反应集的情况下，对固定单步模型的前 k 个输出重新排序** 。这样既能实现有意义的改进，又能保持与使用原始单步模型 相同的精度保证和可比性。

​	2.仅使用搜索成功率来比较单步模型

​		虽然我们同意单步模型应作为搜索的一部分进行基准测试，但 仅仅因为一个模型允许找到更多的路径就推断它更好，可能会导致**错误的结论**：一个过于放任的单步模型可能仅仅因为让搜索进行了不现实的逆合成步骤就产生了许多路径，正如对比实验所证明的那样。相反，**成功率应被视为一个初始指标**；只有对**找到的路径质量进行适当评估**，才能最终确定一种端到端逆合 成管道是否优于另一种端到端逆合成管道

​	3.如果改变单步模型，请仔细选择搜索实验的上限方式

​		现有研究在如何限制搜索实验方面存在**差异**：一些研究使用反 应模型的调用次数 ，而另一些研究则将其与壁钟时间限制相 结合 。如果单步模型保持固定，对模型调用次数设置上限是 一种可靠的选择；但是，改变单步模型可能导致某些模型比其 他模型分配到更多的资源（如时间） 。如果我们相信模型速 度会发生变化，而且所有比较过的模型都可以优化到最终每次调用耗时相近，那么这样做也许是合理的，但如果没有这样的信念，**我们建议使用一种将算法视为黑盒子的测量方法（如壁钟时间或内存消耗）来限制搜索**，因为这种方法也能更直接地 反映 CASP 系统的下游使用情况。

​	4.利用缓存

​		如果在搜索过程中两次遇到相同的分子，简单的实现方法会调用两次反应模型。由于调用反应模型的成本很高，设计完善的 CASP 系统显然会缓存反应模型的输出结果，以避免重复计算 。因此，我们认为在评估多步骤算法时，**最好对单步模型使用缓存**。这听起来可能只是一个微不足道的实施细节，但实际上 却对评估产生了重大影响：在搜索过程中，大型子树往往会出 现在多个地方；如果没有缓存，扩展这些子树的每次出现 都将计入算法的时间预算，而有了缓存，这些扩展实际上是免 费的。

​	5.评估路线的多样性

​		以往的研究强调快速找到一条合成路线，但由于 CASP 程序的 输出结果可能无法在湿实验室中使用，因此**最好能返回多条路线，而且这些路线应是多样化的。换句话说，一旦算法能够找 到单一路线，就应该评估其找到与已找到路线不同的其他路线 的能力**

## 分子表征学习

### 分子表征学习综述（JMC）



![image-20240802094813160](.\image-20240802094813160.png)



分子结构决定其生物功能。准确的预测依赖于分子特征（也称为 分子描述符）的选择，所以**特征工程**很重要。好的表征能让学习任务变得更容易，因为识别关键的结构特征对于揭示生物活性和性质关 系至关重要。正如狗的视觉表征可以从简单的边缘 、形状和局部空间信息分层构建一样，**分子表征也可以 通 过 深 度 学 习 从局部原子环 境 和 子 结 构 分 层 构 建**。在设计分子表征时，有必要从实践和理论 两方面进行考虑，**其中包括原子编号的不变性和明确的、可计算的定义。**

​	**分子表征应具备以下条 件 。** 

​		1.富有表现力	化学空间是广阔的，然而单原子 对分子结构的扰动会导致理化性质和生物活性的巨大差 异。例如，质子化状态和同分异构的细微差别会导致分 子功能的急剧变化，这仍然是化学信息学的一个持续挑 战。表征既要忠实地捕捉化学空间的丰富性和多样性 ，又要区分分子之间的细微差别。 

​		2.简约的	规模实验的成本限制了 和化学数据集的多样性。为了确保模型能够学习到重要 的模式而不是噪音，保持机器学习任务输入特征空间 的 简约性至关重要。正如爱因斯坦常 说 的 ："一切都 应尽 可 能 简单，但不能更简单"。本着同样的精神，表 征应该紧凑，并在不删 除 关键信息的情况下保持表达 力。 

​		3.不变性	因为相同的分子输入应该 要始终生成相同的输出，分子表 征 必须与原子编号等 方面保持不变。其他类型的不变性可以通过限制可学习 函数的空间来促进学习，从而更好地适应特定应用领域 。例如，扩展连通性指纹（ECFP）等圆形指纹使用的 内部参考框架仅编码结构拓扑，因此对旋转和分子保形 不变因此，这些指纹在分子相似性分析中取得了 广泛的成功，因为它们能提供足够的信息，而且计算和 比较速度很快。

​		4.可解释。适用于机器学习的科学应用、 确保模型的性能来自于学习相关模式，而不是利用混杂 变量、实验噪音或其他可能的人工痕迹，这一点至关重 要。可以追溯到结构解释的表征可以让人类专家评估模 型学习到的模式，并根据领域知识进行合理性检查。54最重 要的是，将这些可解释性研究整合到化学和生物发现中，对 于机器学习在科学领域的持续发展非常必要。



领域专业知识和对产生观测数据的物理过程的理解可以激发简化学习的特征转换。从直角坐标转换到极坐标允许线性决策边界将两个类别区分开来（图 2B） 。另外，添加新的相关特征也有助于在新的维度上区分 类别（图 2C）。图 2D 展示了一个简单的神经网络如 何**自动学习新的可线性分离的内部表示**，而无 需 额外 的工程设计。这一范式转变是计算机视觉和自然语言 处理领域深度学习复兴的核心。**深度架构的一个关 键方面是表征的分层学习概念**。神经网络的最底 层学习相对简单的特征，当这些特征在网络中传 播时，它们会非线性地组合成高阶概念

![image-20240802094857138](.\image-20240802094857138.png)

没有任何一种描述符能在所有任务中普遍表现良 好。选择特定的表示方法在很大程度上 取决于其精确的应用。

​		**3D建模其效果往往比二维指纹方法要差**。分子的 生物活性构象通常并不是先验已知的，而且在庞大的构象组合中只编码一个能量最低的构象可能 会带来不必要的偏差。

​		**SMILES 表示法虽然方便，但在学习方面有几个关键 缺陷：**（1）两个相似的分子可以产生两个截然不同的 SMILES 表示法，因为多个有效但不同的 SMILES 可以 描述同一个分子。(2) SMILES 表示法很脆；单个字符的 变化就能产生无效的分子。(3) 大多数分子本身是非线 性的，而 SMILES 却能将复杂的结构压缩成单一的线性 序列。这些缺陷使得 SMILES 语法在经验上很难用标准 的卷积和递归架构来学习，因 此 需要复杂的模型架构 和大量数据才能有效克服这些线性表示的语法依赖性。 最近的研究表明，对 SMILES 语法的修改、 新的采样 方法（如 SMILES-augmentation）和专门的架构（如 stack-RNNs99 ）可以解决这些缺陷。此外，自参照嵌入 字符串（SELFIES）等更强大的词汇表示法可增强学习 效果，并持续产生有效的分子。

​		**GNN需要大量数据**

​		**化学信息学的相似属性原 则指出，相似的化合物应具有相似的属性** 虽然这一原则符合专家的直觉，但分子相似性的 定义仍然不明确。塔尼莫托系数（Tc）等计算方 法主要反映了所选分子表征的相似性，然而这些 表征的相似性对于从分子结构确定功能这一最终 目标来说，只是一种相关代理。2与活性悬崖相对 应的匹配分子对从一个极端说明了这一概念 ：**尽管两个分子在结构上可能相似，但活性上的 明显差异表明它们在功能上并不相似**。在另一个 极端，具有不同支架的两种活性分子则说明了 相 反的概念：**根据分子指纹图谱，这两种分子的结 构和拓扑相似性很低，但作为针对相同蛋白质靶 点的活性分子，它们在功能上可以被认为是相似 的**。因此，以富有表现力而又简洁的方式忠实捕 捉相似性方面的分子表征是非常有意义的。



**小分子药物发现打破了许多机器学习 技术应用中的标准假设**。大多数机器学习算法都基于这 样的假设：**训练数据和测试数据是独立且相同的分布（ 即 i.i.d. 假设）**。19例如，我们会期望一个专门用来区分 猫和狗的标准图像分类器能够泛化到新的猫和狗的图像 中。如果要求该模型对人类图片进行评估，则很可能会 产生不合理的分类。与此形成鲜明对比的是，**现实世界 中的药物发现打破了这一标准 i.i.d. 假设。小分子药物的 优化和设计必须从化学空间的有意新区域中探索结构变 化。要成为先导药物，通常需要对小分子新药进行较大 的结构改变。要使模型对药物化学家有用，就必须将其 推广到分布外实例。**尽管分布发生了根本性转变，但从 专家定义特征到学习特征的过渡仍将使化学信息学和药 物化学受益匪浅。下面，我们将概述小分子学习表征的 主要进展和发展，并讨论这些方法将如何继续提高药物发现 预测建模的先进水平。



一般来说，**多任务**网络共 享内部分层表示，可以利用不同任务之间的 相似性和 细微差别，从而提高学习效率和模型性能。在药物 化学的背景下，从一个蛋白质靶点收集到的 生物活性 数据往往能为我们提供另一个靶点的信息；例如，学习 了小分子抑制一种蛋白激酶的重要方面的模型应该有助 于学习其他蛋白激酶的重要特征，因为许多抑制剂都以 高度保守的 ATP 结合位点为靶点。**根据经验，多任务学习策略可以提高模型的性能和鲁 棒性。**例如，Dahl 等人为上述默克分子活动挑战赛开 发的多任务模型获胜，其表现优于同时针对单个目标训 练的模型。78Ramsundar 等人证明，多任务网络可同时 应用于数百个不同的蛋白质靶标，而且性能提高不大。 尽管这些早期报告激发了人们对多任务网络潜力的热 情，但系统性研究已经划定了这种方法的一些限制，并 开始解读实际用例。Kearnes 等人对真实世界工业数据 集建模的调查证实，通过多任务方法可以获得适度的性 能优势；然而，与更简单的多任务模型相比，添加大量辅助 任务并不能保证提高模型性能�139�Xu 等人进一步研究了多 任务学习在活动预测中的有效性，结果表明，**当任务活动相 互关联时，多任务学习通常是有益的**



启发：1.构建分子表征的要点（加上要保证生成分子的有效性）

​			2.可以采取多任务学习

​			3.小分子药物发现打破了许多机器学习 技术应用中的标准假设



### MOLR：**基于化学反应辅助的分子表征学习(化学反应辅助+适用于所有GNN)**

代码：https://github.com/hwwang55/MolR

简介：**分子表征学习（MRL）方法旨在将分子嵌入真实的向量空间。**，**我们使用 GNN 作为分子编码器，并通过强制反应物嵌入之和等于生成物 嵌入之和，利用化学反应来辅助学习分子表征**。关键思路是**在嵌入空间中保持分子与化学反应的等价性，即强制每个化学方程式的反应物嵌入 总和与生成物嵌入总和相等**。事实证明，这种约束可以有效地：**1）保持 嵌入空间的有序性；2）提高分子嵌入的泛化能力**。此外，**我们的模型可 以使用任何 GNN 作为分子编码器**，因此与 GNN 架构无关。



之前的不足：现有 的基于 SMILES（简化分子输入线-输入系统）或 GNN（图神经网络）的 分子表征学习方法要么以 SMILES 字符串为输入，**难以编码分子结构信息** ；要么过分强调 GNN 架构的重要性，**却忽略了其泛化能力**。这些表示法**最初是为人类读者而不是计 算机设计的**。 

​		SMILES 是分子结构的一维线性化，高度依赖于分子图的遍历顺序**这使得在 SMILES 中距离很近的两个原子实际上可能相距很远，因而不相关语言模型很难仅仅根据 "纤细 "的字符串来学习分子的原 始结构信息**（更多讨论见第 4 节）。第二类是基于结构的方法，可进一步分为传统的基于指纹的方法（Rogers & Hahn，2010；Jaeger 等人，2018）和最新的基于 GNN 的方法（Jin 等人，2017；Gilmer 等 人，2017；Ishida 等人，2021）。例如，Mol2vec（Jaeger 等人，2018 年）**将分子子结构（ 指纹）视为单词，将分子视为句子，然后**使用类似 Word2vec 的方法计算分子嵌入。然而 ，**它们无法识别不同子结构的重要性，也无法以端到端的方式进行训练**。基于 **GNN** 的方 法克服了这些缺点。不过，**由于其架构复杂，通常需要大量的训练数据，这可能会在数据 稀少时限制其泛化能力**。另虽然基于 GNN 的方法在学习分子结构方面理论上优于基于 SMILES 的方 法，**但它们仅限于设计新鲜精致的 GNN 架构，而忽略了 MRL 的本质，即泛化能力**。事实 上，我们稍后将证明，**没有一种特定的 GNN 能够在 MRL 的所有下游任务中普遍表现最佳**



本文：为了便于机器学习算法理解和利用分子，有人提出了分子表征学习（MRL） ，**将分子映射到低维实数空间，并以密集向量的形式表示**。学习到的分子向量（又称嵌入 ）可以使一系列下游任务受益。

​		我们在本文中提出利用化学反应来帮助学习分子表征并提高其泛化能力。例如，醋酸和乙醇的费歇尔酯化化学方程式可以写为CH 3COO + C2 H5 OH → CH 3COO 2 H5 + H2O。**化学反应通常表明其反应物和产物之间特定的当量关系（例如，就质量和电荷保守而言），我们的想法是在分子嵌入空间中保持这种等效性。**具体来说，鉴于上述费舍尔酯化的化学反应，我们希望方程式hCH 3COO + hC2 H5 OH = hCH 3COO 2 H5 + hH2 O也成立，其中h（·）代表分子嵌入函数。这个简单的约束条件赋予了分子内嵌非常好的特性：（1）分子内嵌在化学 反应方面是可组合的，**这使得内嵌空间变得井井有条**（见命题 1）；（2）更重要的是，我 们将在后面证明，当分子编码器是一个以求和作为读出函数的 GNN 时，**我们的模型可以 自动隐式地学习反应模板**，这些模板可以概括同一类别中的一组化学反应（见命题 2）。 **学习反应模板的能力是提高分子表征泛化能力的关键**，因为模型可以很容易地将所学知识 泛化到未见过但与已知分子属于同一类别或具有相似结构的分子上。

​		与同样学习反应模板的（Jin 等，2017）相比，我们的模型**不需要**复杂的网络来计算 注意力分数，**也不需要额外的反应物和生成物之间的原子映射信息作为输入**。此外，从理 论上讲，我们的模型甚至可以**只根据一个反应实例来学习反应模板**



方法：

​	1.结构分子编码器	

​		分子图表示为 G = (V，E)，其中 V = {a1 , - - - } 为非氢原子集合，E = {b1 , - - - - } 为键集合 。**每个原子 ai 都有一个初始特征向量 xi ，对其属性进行编码**。在本研究中，**我们使用了四 种原子属性：元素类型、电荷、是否为氢原子、原子是芳香环，以及相连氢原子的数量。**每种类型的原子属性都用一个**独热向量**来表示， **我们还为每个独热向量添加了一个 "未知 "条目**，以便在推理过程中处理未知值。**四个独热 向量连接起来作为初始原子特征**。此外，**每个键 bi 都有一个键类型（如单键、双键）**。由 于键类型通常可以通过其两个相关原子的特征推断出来，而且根据我们的实验，**键类型并 不能持续提高模型的性能**，因此我们没有明确将键类型作为输入。

​		为了学习分子的结构表征，我们选择了 GNN 作为基础模型。GNN 利用分子结构和原子特 征来学习每个原子和整个分子的表征向量。



​	2.保持化学反应等效性![image-20240802170415576](.\image-20240802170415576.png)

​		化学反应定义了反应物集 R = {r1 , r2 , - - - } 和生成物集 P = {p1 , p2 , - - - } 之间的特定关 系"→"：				![image-20240802150041644](.\image-20240802150041644.png)

​	**化学反应通常代表一个封闭系统，反应前后系统的几个物理量保持不变，如质量、能量、 电荷等。因此，它描述了化学反应空间中反应物和生成物之间的某种等价关系。我们的主 要想法是在分子嵌入空间中保留这种等价性：**![image-20240802150853352](.\image-20240802150853352.png)

**上述简单约束对于提高分子嵌入的质量至关重要。**我们首先通过下面的命题证明，在公式 （4）的约束下，化学反应关系"→"是等价关系式。**（4）构成了一个线性方程组，其中化学反应等价对碱基分子的嵌入 有很强的约束**。因此，分子嵌入的可行解将更加稳健，整个嵌入空间也将更加有序。

​	**我们还可以进一步证明，公式（4）中的约束条件还能提高分子嵌入的泛化能力。**为了说明 这一点，我们首先定义化学反应的反应中心。R → P 的反应中心定义为反应物 R 的诱导子 图，其中每个原子都有 至少有一个键的类型从 R 到 P 不同（"无键 "也被视为一种键类型）。在其他 换句话说，**反应中心是将反应物转化为生成物所需的最小图形编辑集**。鉴于反应中心的概 念，我们有如下命题：**命题 2 表明，反应物嵌入和生成物嵌入之间的残差将完全且仅取 决于距离反应中心小于 K 跳的原子**例如，如图 1b 所示，假设我们使用 3 层 GNN 来处理 丙酸和丙醇的 Fischer 酯化反应，那么**反应物嵌入和产物嵌入之间的残差将完全取决于反应 中心（橙色）以及与反 应中心距离为 1 或 2 的原子（浅橙色**）（也就是说反应只与这些原子有关，可以与官能团概念相结合？）



![image-20240802152521276](.\image-20240802152521276.png)



![image-20240802170649238](.\image-20240802170649238.png)这意味着，如果 GNN 编 码 器 对 该 化 学 方 程 式 进 行 了 优 化 并 输 出 完 美 的 嵌 入 ， 即 hCH3CH2COOH + hCH3CH2CH2OH = hCH3CH2COOCH2CH2CH3 + hH2O、 **那 么 对 于 任 何 官 能 团 R1 和 R2 ，** 方 程 式 hR1-CH2COOH + hR2-CH2CH2OH = hR1CH2COOCH2CH2-R2 + hH2O 也将成立，因为方程式两边的残差并不取决于距离反应中心超过 2 跳的 R1 或 R2 。诱导的一般化学反应 R -CH12 COOH + R -CH22 CH2 OH → R -CH12 COOCH2 CH R22 + H2 O **被称为反应模板，它抽象了同一类别中的一组化学反应。** 学习到的反应模板对提高模型的**泛化能力**至关重要，因为模型可以很容易地将这些知识应 用于训练数据中未见但符合已知反应模板的反应（如乙酸加丙醇、丁酸加丁醇)

​		根据命题 2，**GNN 层数 K 会对学习到的反应模板产生很大影响：**K 越小可能不足以 代表一个有意义的反应模板（例如，在费歇尔酯化反应中，如果 K < 3，羧酸中必要的羰基 "C=O "就不会出现在反应模板中、例如，在费歇尔酯化反应中，如果 K < 3，羧酸中必要的 羰基 "C=O "将不会出现在反应模板中），而 K 过大则可能会为反应模板包含不必要的原子 ，从而降低其覆盖范围（例如，图 1b 所示的反应模板就没有覆盖甲酸 HCOOH 和甲醇 CH3 OH 的费歇尔酯化反应）。**K 的经验影响见附录 D，一般选K=2**![image-20240802153433757](.\image-20240802153433757.png)

​		最后，**我们的模型不会明确输出所学反应模板**，而是将这些信息隐含在模型参数和分子嵌 入中。挖掘显式反应模板的一种简单方法是剔除每个化学反应中反应物和生成物的共同部 分，然后应用聚类算法。

​	3.训练模型

![image-20240802153945137](.\image-20240802153945137.png)解决这一问题的常见方法 这些方法包括引入**负抽样策略**（Goldberg & Levy，2014 年)或**对比学习**。这里，我们使用了一个与（Radford 等人，2021 年）类似的基于小批量的对 比学习框架，因为它更节省时间和内存。对于一个小批数据 B = {R1 → P1 , R2 → P2 , - - - }⊆ D，我们首先使用 GNN 编码器处理 该迷你批次中的所有反应物 Ri 和产物 Pi ，并得到它们的嵌入。**匹配的 反应物-生成物对（Ri , Pi ）被视为正对，其嵌入差异将最小化，而不匹配的反应物-生成物 对（Ri , Pj ）（i /= j)被视为负对，其嵌入差异将最大化。**为了避免优化目标被大量负对主导，使用边距作为正则项，优化目标函数minLb如下：![image-20240802154233599](.\image-20240802154233599.png)


```
其中，γ是边距的超参数，我们可以使用随机梯度下降SGD最小化上述目标训练模型。
式 (5) 可以看作是一种特殊的负采样策略，它将小批量中所有未匹配的产物作为给定反应 物的负采样。不过要注意的是，与传统的负采样（Mikolov 等人，2013 年）相比，它有两 个优点：（1）无需额外内存来存储负采样；（2）由于训练实例会被洗牌，负采样会在新 历时开始时自动更新，从而节省了手动重新采样负采样的时间。
```



分子性质预测
作者在5种数据集中测试MolR，每个数据集包含数千个SMILES分子。预测的AUC结果如下表所示，MolR在4个数据集中表现最好，作者将MolR在分子性质预测方面的优异性能归因于，MolR在USPTO-479k上预训练，因此具备对于命题2对反应中心的感知。注意，反应中心通常由化学活性官能团组成，这些官能团对确定分子性质至关重要。
————————————————



原文链接：https://blog.csdn.net/qq_40943760/article/details/121873051

启发：1.加入附加信息（如分子的文字描述）来帮 助学习分子表征也是一个很有前景的方向。

​			2.直接参考本文



数据集：USPT0-479K



### KGPT:改善分子表征学习的知识指导预训练框架（自监督+知识指导）

简介：我们提出了知识指导下的图转换器预训练（KPGT），这 是一种**自我监督**学习框架，可缓解上述问题，并提供可通用且稳健的分子 表征。KPGT 框架集成了专为分子图设计的**图转换器**（线图转换器（LiGhT）的高容量模型（特别设计用 于精确建立分子图结构模型））和**知识引导的预训练** 策略，以充分捕捉分子的结构和语义知识。



先前的问题：目前基于自我监督学习的方法存在两个主要障碍：**缺乏定义明确的自我监督学习策略**和**图神经网络的容量有限**。并且，然而，**标记分子的有限可用性**和**化学空间的广阔性**限制了各类模型的预测 性能，尤其是在处理**分布外数据 样本**时。

​			当前的自监督学习方法，以 GraphLoG36、GROVER38和 GEM42 通常需要**通过节点或子图掩蔽**来修改分子图，然后预测被掩蔽的成分或**利用对比学习目标**，将修改后的图与潜在空间中相应的原始图 对齐.**分子本身具有与其结构紧密相关的特征，这意味着即使对分子 图进行微小的修改，也会导致其语义信息的丢失。分子的这一天然 特性可能会限制目前基于自监督学习的分子图方法捕捉分子间的结 构相似性，而无法捕捉其化学结构中编码的与分子特性相关的丰富语 义信息**（补图 1）。43.此外，**在分子图缺乏语义信息的情况下**，被 掩蔽的节点与其相邻节点之间的唯一依赖关系就是化合价规则，而 化合价规则往往无法指导模型对被掩蔽的节点进行准确预测（补图 2）。因此，**这种局限性有可能导致模型只是简单地记忆数据集**。

​		GNN 只能提供有限的模型容量，因为当层数 增加时，它们会出现**过度平滑**的问题.此外，GNN 可能难以捕 捉原子间的长程相互作用，



本文：我 们假设，在**自监督学习框架中引入定量描述分子特征的额外知识**， 可以有效解决这些难题。分子的许多定量特征，如前述的分子描述 符和指纹，都可以通过现有的计算工具轻易获得.**整合这些额外 的知识可以为自监督学习引入丰富的分子语义信息，从而大大提高 语义丰富的分子表征的获取能力**。



模型架构：A：KPGT的示意图。一种基于屏蔽图模型并通过附加知识增强的知识制导的预训练策略。**分子被表示为分子折线图，表示原始分子图的边之间的邻接关系**。B一种基于经典变压器结构的折线图变压器。C转移学习用于精调设置下的下游分子性质预测，其中预先训练的光的参数是可训练的。在此设置中引入了各种微调策略，如分层学习速率衰减、重新初始化、FLAG和L2-SP。D转移学习用于特征提取设置下的下游分子性质预测，其中预先训练的光的参数是固定的。神经指纹代表由预先训练的光产生的分子特征表征，作为分子的信息性和区分性表征。多层感知器(MLP)、线性层(Line)、矩阵乘法(MatMul)、距离编码(DE)模块、路径编码(PE)模块。![image-20240802180048023](.\image-20240802180048023.png)我们提出的 KPGT 框架（图 1）由两个主要部分组成：一个称为线图 转换器（LiGhT）的骨干模型和一个知识引导的预训练策略。LiGhT 特别 设计用于全面捕捉分子图结构中的复杂模式（图 1b）。该**模型基 于经典的变压器编码器50由多层多头注意力模块和前馈网络组成。** LiGhT 将分子线图作为输入，分子线图代表原始分子图边缘之间的 邻接关系（附图 3）。将**分子表示为线图可以让 LiGhT 充分利用化 学键的固有特征**，而这些特征在之前定义的转换器架构中一般都被忽 视了。**此外，为了精确模拟分子的结构信息，我们在多头注意力模 块中引入了两个位置编码模块，即距离编码模块和路径编码模块**。 我们提出的知识引导预训练策略是基于一个目标掩蔽图模型。59该 模型最初会随机屏蔽分子图中的一个节点子集，然后学习预测这些 屏蔽节点（图 1a）。**我们的策略最显著的特点是纳入了额外的知识。 每个分子图都增加了一个知识节点（K 节点），与图中的原始节点 相连。每个 K 节点的原始特征嵌入使用相应的附加知识进行初始 化。**在预训练过程中，K 节点会与每个转换层的多头注意力模块中 的其他节点进行交互，从而为预测屏蔽节点提供指导。**这种机制使 主干模型能够有效捕捉分子图中的结构和语义信息。**

​		 我们利用 ChEMBL29 数据集中的约 200 万个分子60利用知识 引导的预训练策略对 LiGhT 进行预训练。然后，我们将迁移学习应 用于预训练的 LiGhT 模型，以执行下游分子特性预测任务。在 LiGhT 模型之上集成了一个多层感知器作为预测器。根据预训练 LiGhT 模型 的参数是否可训练，迁移学习方法可分为两种设置：微调（图 1c）和 特征提取（图 1d)。在微调设置中，我们引入了几种微调策略，例 如分层学习率衰减61重新初始化61、FLAG62和 L -SP263从而充分利用预训 练模型所获得的知识。有关 KPGT 框架的更多详情，请参阅 "方法 "部分。



### 分子屏蔽图建模的tokenizer和encoder（提出SimSGT，达到SOTA）

代码：https://github.com/syr-cn/SimSGT

简介：**屏蔽图建模（MGM）在分子图的自我监督表征学习中表现出色**。存在由三个关键部分组成的共同方案：(1) **图标记化器**（ graph tokenizer），将分子图分解成更小的片段（即子图）并转换成标记； (2) **图掩码**（graph masking），用掩码破坏图；(3) **图自动编码器**（graph autoencoder），首先在掩码图上应用编码器生成表示，然后在表示上使用 解码器恢复原始图的标记。下游任务只用encoder（利用其生成的良好表征）。我们的研究结果表明，**子图层标记器和具有足够表现力的解码器与重任务解码对编码器的表征学习有很大影响**。最后，我们提出了一种新颖的 MGM 方法 SimSGT，其特点 是基于简单 GNN 的标记器（SGT）和有效的解码策略。



MGM依赖于三个关键组件--**图标记器、图屏蔽和图自动编码器**：![image-20240807181715251](.\image-20240807181715251.png)

​		**图标记器**：给定一个图 g，图标记器利用**图分割函数**将 g 分割成更小的子图 ， 如节点和图案。然后这些片段被映射成固定长度的token，作为以后重建的目标。显然，**图标记的粒度决定了屏蔽建模中表征的抽象程度**。这一点与分子尤其相关，因为**分子的性质主要由子图粒度的模式决定**。例如，图 2 所示的分子包含一个苯环子图。苯环赋予分子芳香性，使其比只有单键的饱和化合物更稳定。因此，**应用能生成子图级标记的图标记化器可能会提高下游性能**。

​		**图屏蔽**:在输入自动编码器之前，g 会被添加随机噪音破坏，通常是通过随机屏蔽节点或丢弃边。图形掩码对于防止自动编码器仅仅复制输入内容以及引导自动编码器 学习共现图形模式之间的关系至关重要。 **这里我们不使用边缘删除,边缘删除很容易导致下游任务的性能下降**

​		**图形自动编码器**:图形自动编码器由图形编码器和图形解码器组成。**图编码器生成被破坏图的隐藏表示，图解码器根据这些表示尝试恢复被破坏的信息**。编码器和解码器通 过最小化解码器输出与重建目标（即图标记器诱导的图标记）之间的距离来共同优化。 考虑到目标是复杂的子图级标记，有效的重建可能需要一个具有足够表现力的图解码器.图自动编码器是通过**最小化预测与目标之间的距离**来训练的

​	

​		我们的分析表明，**在 MGM 中重建子图层标记比重建节点标记更有效**。此外，我们 还发现，**一个具有足够表现力的解码器结合 remask 解码可以提高编码器的再现质量**。值 得注意的是，remask 将编码器和解码器 "**解耦**"，**将编码器的重点从分子重构转向 MRL**，从而提高下游性能。总之，我们发现，将子图级标记器和具有足够表现力的解码器与 remask 解码相结合，可以提高 MGM 性能。



​		**分子标记符**：![image-20240807182556648](.\image-20240807182556648.png)



​			**节点、边标记符**：图的节点和边可以直接用作图标记，因其简单。然而，原子序数和键类型属于**低级特征**。对于需要**对图形语义有高层次理解的下游任务来说，重建它们可能不是最佳选择**

​			**基于 Motif 的标记符**：Motif是图结构中**具有统计意义的子图**模式。对于分子来说，**官能团（FGs）**是由专家根据 FGs 的生化特征人工策划的主题图。例如，含有苯环的分 子具有芳香性。**考虑到人工策划的 FGs 数量有限，不能完全覆盖所有分子**，以往的研究采用了**化学启发**的片段函数来发现主题。在此，我们总结了常用的破碎函数： 

​				• FG。FG 是分子的子图，在不同化合物中表现出一致的化学行为(**即官能团**)。**在化学工具包RDKIT中，FG 的子结构模式由 SMARTS 语言描述**。

​				• 循环。由于**分子中的循环具有潜在的化学意义**，因此经常被提取为主题。图 3b 描述了将一个五节点循环分割为一个母题的过程。如果两个循环有两个以上的原子重叠 ，它们可以合并，因为它们构成了一个桥式化合物。

​				• BRICS 在潜在的裂解位点上分裂分子，在特定的环境条件或催化剂作用下 ，化学键可在这些位点上断裂。BRICS 的关键步骤是识别分子 g 的潜在裂解位点，要做到这一点，需要找到两边都符合 BRICS 中预先定义的 "类 FG "亚结构模式 S1 的化学键，接下来，g 被分割成最大的子图，这些子图不包含裂 解位点中的任何键：为简单起见 ，我们在此只介绍关键步骤。 

​				• 剩余节点和边：给定另一个分片函数 f0 ，**未包含在任何 f0 的输出中的节点和边将 被视为单独的子图**。这就提高了 f0 对原始图的覆盖率。图 3b 显示了在 fcycle 之后对剩余 节点进行分片的示例：不在任何循环中的节点被视为独立子图。 

​			为了得到更细粒度的子图，以往的研究 [1, 32, 18] 通常通过**联合**（如f1 (g) f2 (g)）或组合（ 如f2 (t) t f1 (g) ） 的 方式**将多个分片函数组合在一起**。让 fmotif 成为组合后的最终破碎函数 。我们通过 fmotif 分割数据集中的每个分子，并收集一个主题词词汇表，通过阈值过滤掉不 常见的主题词。然后，给定一个新分子 g′ ，我们可以通过对其主题词 fmotif (g′ ) 进行单次编 码来生成其标记： tokmotif (g′ ) = {yt = one-hot(t, M)|t∈ fmotif (g′ )} 。

​				基于预训练 GNN 的标记器 [10]。预训练的 GNN 可以作为图标记符。以 k 层图同构网络（ GIN）[37] 为例。它的节点嵌入总结了节点 k 跳根子树的结构信息，使其成为子图层的图标 记（图 3c）。GIN 同时执行分片和映射。它可以写成 tokGIN (g) = {yi = SG(h(k) )|i∈ V}、 (10) h(k) = COMBINE(k) (h(k−1) , AGGREGATE(k) ({hk−1 , j ∈ N (i)}))、 (11) 其中，AGGREGATE( ) 从节点 i 的邻居收集信息，COMBINE( ) 据此更新 i 的表示。SG( )表 示停止梯度，用于在 MGM 预训练期间停止梯度流向标记化器。除了 GIN，其他 GNN 也可 以作为标记化器。为了获得有意义的图标记，我们会在使用 GNN 作为标记化器之前对其进 行预训练 [10]。预训练完成后，该 GNN 将被冻结并用于后续的 MGM 预训练。在第 4 节中 ，我们将对标记化器常用的图预训练策略进行评估。鉴于基于 GNN 的标记化器提供节点标 记，我们将直接最小化图标记与自动编码器标记之间的距离。 输出 {z }i |iV=|1 的屏蔽节点 Vm ，即L0 = 1Σ ∈V ℓ(y ˆ i = zi , y ).i



![image-20240807183228628](.\image-20240807183228628.png)



### Unimol:

### MoleBert:使用VQ-VAE架构进行掩蔽原子建模+三重对比学习

简介：通常情况下，针对分子的预训练图神经网络作为节点属性的原子类型会被随机屏蔽，然后 GNNs 会被训练以预测屏蔽类型，如 AttrMask，这与 BERT 的屏蔽语言建模（MLM）任务如出一辙。然而，与拥有庞大词汇量的 MLM 不同，**由于原子 "词汇量 "小且不平衡**，AttrMask 的预训练无法学习到信息丰富的分子表征。我们提出了一种 **VQ-VAE 变体**，**作为一种上下文感知标记器，将原子属性编码为具有化学意义的离散代码。这可以扩 大原子 "词汇量"，并减轻主要原子（如碳）和稀有原子（如磷）之间的数量差异**。 有了扩大的原子 "词汇量"，我们提出了"**掩蔽原子建模**"（MAM），即随机掩蔽一些离散代码，然后预训练 GNN 来预测它们。 **MAM 还能缓解 AttrMask 的另一个问题，即负转移**。此外，我们还提出了**用于图级预训练的三重掩码对比学习**（TMCL），以模拟分子之间的异质语义相似性，从而实现有效的分子检索。MAM 和 TMCL 构成了一个新颖的预训练框架 Mole-BERT。

**我们强调以下贡献**：

​	(i) 我们发现 AttrMask 的负迁移问题可归因于原子词汇量极小且不平衡。

​	(ii) 作为 补救措施，我们使用 VQ-VAE 的变体，为分子图贡献了一种上下文感知标记器。此外，该标记器还 可作为现成工具重新用于 NLP 社区的后续工作。	(iii) 有了新词汇，我们提出了量身定制的预训练任务 MAM，以缓解负迁移问题。MAM 作为基本的预训练任务，可与各种预训练任务相结合，以提高其性 能。

​	(iv) 我们提出了一种新颖的图级预训练任务 TMCL，以模拟分子间的异质相似性，这对分子检索 特别有效。 

​	(v) 我们将 MAM 和 TMCL 结合起来，作为一个联合预训练框架（Mole-BERT），与需要昂贵的领域 知识作为指导的最先进模型相媲美，甚至更胜一筹。



先前方法缺陷：

​	1.仅使用 AttrMask（节点级预训练任务）进行预训练有时会产生**负迁移问题**（ 即预训练模型落后于无预训练模型）。**这表明对于较小的原子词汇量来说 ，AttrMask 的任务（118 路分类，118 是自然界中常见化学元素的数量）是非常简单的**。原子词库是自然界中常见化学元素的独特原子类型集合。相比之下，BERT 中的 MLM 任务（∼ 30k-way classification）的训练准确率只能提高到 70%，而且在文本词汇量较大（∼ 30k to kens）时几乎不 收敛。其次，**不同原子之间的定量差异非常显著**（见图 1(b)），**这会使模型的预测偏向于优势原子**（如碳原子），从而导致快速收敛。以前 的研究（Clark 等人，2020 年；Robinson 等人，2021 年）表明，**简单的预训练任务捕获的可迁移知识 较少，会影响对新任务的泛化或适应**。对于 GNN 的预训练，之前的工作采用原子类型作为标记，这将导致原子词汇量过小且不平衡。 我们认为，**不同语境下的原子即使属于同一类型，也应标记为不同的离散值**

​	2.对于分子图级预训练，图对比学习是一种可行的预训练策略。然而，**对比法会将不同的分子等量推开，而不管它们真正的相似度如何。**

​	

解决方法：

​	1.因此，我们**引入了一种上下文感知标记符，将原子编码为有意义 的离散值**。具体来说，这些离散值是图 VQ-VAE 变体的潜在代码。 标记化器是上下文感知的，因为图 VQ-VAE 的编码器是一个 GNN 模型。通过这种方式，我们可以**考虑原子的上下文，将主要原子（如碳原子）分为几个有化学意义的子类（如醛碳原子、酯碳原子等） ，从而扩大原子词汇量，减少主要原子和稀有原子之间的数量差异。**为了支持上述观点，我们在图 1 （c）中提供了通过所提议的标记器学习到的碳表征的 t-SNE 可视化图。可以看出，碳的表示是根据 官能团的类型进行聚类的，这表明我们的标记化器可以将原子编码为有化学意义的值。有了新词汇， 我们提出了节点级预训练任务，称为 "**屏蔽原子建模（MAM）"，以随机屏蔽离散值**，并预训练 GNN 来预测它们。

​	VQ-VAE作为原子标记器，它以一种上下文感知的方式将连续语义空间离散为离散代码。此外，我们还发现， 在 vanilla VQ-VAE 中，不同类型的原子可能会被分配到相同的标记 ID 中。作为补救措施，我们引入了**组 VQ-VAE** 来解决这个问题。

​	(1)我们的标记器以上下文感知的方式将原子标记为离散代码；

​	(2)我们使用标记而不是自回归生成进行预训练；

​	(3)我们观察到不同类型的原子可能会在虚VQ-VAE中被分配为相同的标记id。作为 补救措施，我们引入了分组 VQ-VAE 来解决这一问题。具体来说，我们将编码本嵌入分为几组，每 组对应特定的原子类型。例如，碳、氮和氧的量化编码分别限制在 [1，128]、[129，256] 和 [257， 384]。左侧的稀有原子被限制在 [385, 512]，因为它们相互冲突的可能性较小

​	(4)我们提出用 MAM 对 GNN 进行预训练。具体来说，**给定一个输入分子图 G，我们随机屏蔽其 15%原子的标记**，然 后预训练 GNN 来预测它们

​	2.为了弥补这一不足，我们提出了三重掩蔽对比学习（TMCL），通过不同的掩蔽比例来模拟不同程度 的分子相似性.图形对比学习（You 等人，2020 年）是解决上述问题的一种可行方 法。对于每个分子（锚），他们会最大限度地提高成对分子图增强（正对）之间的一致性，并将批次 中的其他分子作为负对（异类分子）无差别地推开。然而，我们认为这一框架无法反映锚和其他分子之间的异质相似性。例如，甲酸（负分子）和醋酸（锚分子）之间的相似性应该是比乙醇（负）和乙酸（锚）之间的对比更重要。因此，我们引入了三重掩蔽对比学习（TMCL）来 缓解这一关键缺陷。

![image-20241116114217933](.\image-20241116114217933.png)

![image-20241116114255988](.\image-20241116114255988.png)

**主要结果和分析**。

​	我们在表 1 和表 2 中记录了分子特性预测的主要结果。我们的系统研究表明了以下 趋势： 

​	观察结果 I：AttrMask 的预训练任务在某些数据集（HIV 和 BBBP）上产生了负迁移问题。相比之下 ，虽然 MAM 只通过节点级任务对 GNN 进行预训练，但与 AttrMask 和 "无预训练 "相比，MAM 实现 了一致且显著的改进。这一观察结果验证了只有节点级的预训练任务也能减轻负迁移，从而推翻了之 前认为在单个节点级预训练 GNN 可能会带来有限改进的观点。**AttrMask 失败的原因在于原子词汇量 极小且不平衡。** 

​	观察结果 II：在相同的实验方案下，Mole-BERT 的性能可与之前的预训练策略相媲美，甚至更好。更 具体地说，Mole-BERT 比 "无预训练 "模型高出 6.89%，比目前最先进的方法 GraphMVP 高出近 1.40% ，尽管 GraphMVP 在另一个具有 3D 几何结构的分子数据集上对 GNN 进行了预训练。为了进一步证 明上述说法，我们在附录 G 中绘制了训练和测试准确率曲线。

​	结论三：如表 2 所示，MAM 可以像 AttrMask 一样作为基本的预训练子任务。此外，在作为多任务 GNN 的预训练子任务时，MAM 比 AttrMask 显示出明显的优势。 

​	观察结果 IV（消融研究）：我们替换或删除了建议方法中的某些组件，以研究它们的有效性。从表 1 中可以看出，组 VQ-VAE 在 MAM 中优于 vanilla VQ-VAE，因为它可以防止不同类型的原子被分配 到相同的标记 ID 中。此外，当我们去除 TMCL 中的三元组损失 Ltri 或对比损失 Lcon 时，我们观察到 性能明显下降，这表明这两种损失都是必要且有效的。



### FG-Bert:基于官能团的分子表征学习框架，用于性质预测（TensorFlow）

简介:将分子视为由官能团（FG）组成的整体，然后训练策略和Bert差不多。达到了SOTA。问题是用的TensorFlow

官能团列表：

![image-20241115160729356](.\image-20241115160729356.png)







![image-20240803162208137](.\image-20240803162208137.png)**模型架构**：transformer encoder-decoder架构

**亮点**：1.多模态，引入科学语言帮助模型学习分子性质  2.在逆合成领域引入基于官能团的分子切片+表征学习   3.可解释性可能较强？

​	一言以蔽之，在纯化学反应数据集较少的情况下，我们将工作重心从设计精巧却针对性强的模型转移到常被忽略却十分重要的通过化学领域的domain knowledge设计更好的分子表征上来，使模型从单纯依靠自身复杂架构通过较少的纯逆合成反应的数据集学习化学反应转变到使用通用架构，学习化学知识和学习化学反应并重，进而更好的学习到反应的内部机理。这一学习过程也符合人类直觉。

**前人不足**：

​	1.多模态多应用在通用的大模型上（BioT5+,2024-5），**未见前人将多模态运用在逆合成分析上的工作**（综述LLMs-in-Chemistry(2023-4-11)明确指出“这些模型（逆合成模型）不利用文本信息”，综述Artificial Intelligence Methods and Models for Retro-Biosynthesis: A Scoping Review（ACS，2024-7-24）也未提及任何有关工作）。事实上**纯逆合成反应的数据集较少，且收集较为困难，而大量可用的化学信息以文本的形式存在。**

​	2.**前人逆合成分析代表性的工作多在于设计精巧的模型，而忽略了最基础的分子表示方法**。前人在逆合成中广泛采用的分子表示方法有两个方向：一维序列与图。缺陷如下：

​		一是模型很难仅仅根据 "纤细 "的字符串来学习分子的原始结构信息，而使用最多的smiles式**表示法虽然方便，但在学习方面有几个关键缺陷：**

​		(1)两个相似的分子可以产生两个截然不同的 SMILES 表示法，因为多个有效但不同的 SMILES 可以描述同一个分子。

​		(2) SMILES 表示法很脆弱；单个字符的变化就能产生无效的分子。

​		(3) 大多数分子本身是非线性的，而 SMILES 却能将复杂的结构压缩成单一的线性序列。这些缺陷使得 SMILES 语法在经验上很难用标准 的卷积和递归架构来学习，因此需要复杂的模型架构 和大量数据才能有效克服这些线性表示的语法依赖性。（CHEMICAL-REACTION-AWARE MOLECULE REPRESENTATION LEARNING，ICLR2022）

​		二是GNN需要大量数据（Learning Molecular Representations for Medicinal Chemistry，JMC,2020），并且GNN 只能提供有限的模型容量，因为当层数增加时，它们会出现**过度平滑**的问题（在图神经网络的多层信息传递过程中，节点的表示逐渐变得相似，最终导致不同节点的特征难以区分）.此外，GNN 可能**难以捕捉原子间的长程相互作用**（A knowledge-guided pre-training framework for improving molecular representation learning,Nature Communications，2023-11）。

​	综上，**前人的常用的分子表示表现出明显的局限性**，需要更有效的分子表示方法。

![image-20240803153825402](.\image-20240803153825402.png)![image-20240803154424562](.\image-20240803154424562.png)

​																						（常见的分子表示方法）

**可行性分析**：

​		1.多模态小模型：逆合成预测的关键之一在于模型对参与反应的分子性质的“理解”，而利用**文本信息**促进模型对分子的理解在生物领域大模型（BioT5,BioT5+,2024-5），在化学领域中的分子编辑（Multi-modal Molecule Structure-text Model for Text-based Retrieval and Editing，2022-12）已被证明**有效**。且可用的**数据集多**：如mol-instruction(面向分子的指令)、分子的IUPAC名称、PubChemSTM（化学结构-文本对）、ChEBI- 20数据集（描述引导的分子设计任务）等。而**小模型训练的代价不高**，且因为**任务单一**（只做逆合成），所以更容易在单一任务取得较好效果。且大模型不容易发文章？

​			疑难：（1）如何学好多模态信息，目前的思路是借鉴CLIP，采用对比学习对齐多模态信息。（2）我们不希望做大模型，但是小模型理解科学语言的能力是否足够？是否仍然需要自然语言的数据集训练而非上述纯科学语言的数据集？【参考：BERT-base包含110M参数，12层Transformer编码器，每层768维向量，12个自注意力头，训练时的最大序列长度为512。在16个TPU核心上训练，约使用了4天的时间，完成了整整1,000,000步的训练。数据集：Wikipedia：使用了英语Wikipedia的全部内容，约2.5亿个词 BooksCorpus：包含了1,000本书籍，约8.4亿个词。】（3）没有明确的消融实验表明文本信息到底能提点多少。



​		2.tokenizer+表征学习：逆合成领域几乎所有的代表性工作（无论基于模板还是无模板）都重视反应中心的识别，事实上化学反应也往往发生在局部（LocalRetro,2022），且往往是官能团之间的反应。CHEMICAL-REACTION-AWARE MOLECULE REPRESENTATION LEARNING（ICLR，2022）这篇工作进一步证明了**反应物嵌入和产物嵌入之间的残差将完全取决于反应中心（橙色）以及与反 应中心距离为 1 或 2 的原子（浅橙色**），也就是事实上发生反应的官能团，推而广之即为反应模板（见下图）。同时，这篇工作也证明了反应模板可以**隐含**在模型参数和分子嵌入中，从而在模型设计上可以避免基于模板的模型弊端（泛化能力差），但保留基于模板其优点。（即我们设计的模型本质上也是希望模型学到模板信息，但是不同于以往的在模板库中检索的基于反应模板的模型，而是将对反应模板的利用隐含在分子表征中）。同时，**使用指纹（如ECFP）可以很好地预测属性和活动，并且在逆合成过程的许多部分都使用了相同的指纹图谱**（Artificial Intelligence Methods and Models for Retro-Biosynthesis: A Scoping Review，ACS，2024-7-24），而分子指纹正是通常以二进制格式表示分子的结构信息。分子指纹通过将分子拆解为其组成部分（如原子、键、基团等），并利用特定的算法将这些信息编码成一串数字或字符。但是分子指纹构成的化学空间内不包含上述的反应模板信息。

​	我们的想法正是基于上述两种而**融合他们的优点**，通过tokenizer（借鉴分子指纹）将分子拆解为其组成部分（如原子、键、基团等），再通过分子表征学习将这些组成部分**映射到包括了反应模板等反应信息的化学空间**内。

![image-20240803172141247](.\image-20240803172141247.png)

​		疑难：（1）难就难在融合上。之前的分子表征工作正如其名，都是将整个分子嵌入到化学空间内，分子本身是一个单独的个体，不同的分子性质不同，所以在化学空间内自然具有不同的嵌入。但是如果将分子拆解为其组成部分（如原子、键、基团等）后，这些组成部分虽然本身是独立个体（如单个苯环），但是其差异性不体现在其本身（如两个羟基其本身没有任何化学性质差异），而体现在它们之间不同的组合（即组合成的不同分子上，如一个羟基连接某个环的一号位点,一个苯环连接某个环的二号位点,则这两种组合就具有了性质上的差异）。那么，我们构造的化学空间要如何体现这种差异性？【我的想法是对于相同的结构也要根据不同的结合位点区分开来，比如1-羟基与2-羟基虽然结构式相同，但是其嵌入应该有差异】

​					（2）如何让构成的化学空间带有反应模板信息？【想法是在训练表征学习时要引入化学反应作为辅助，借鉴PMSR这篇工作】

​					（3）代码实现问题



​	3.模型架构：transformer encoder+decoder：在多模态领域该架构表现亮眼（BLIP、CoCa、BEiTv3、T5等模型），在逆合成领域该架构也取得了无模板逆合成的SOTA（PMSR模型,AAAI2023），证明其在多模态、逆合成方向均可行。且该架构简单、通用，并可以无限叠层。

​						Node-Aligned Graph-to-Graph (NAG2G)（2024-3）这篇工作指出encoder扮演着学习分子表示形式的关键角色.同时Decoder架构时生成式模型的关键。

​		疑难：（1）主要是细节问题，比如层数

​					（2）采取什么样的评分机制（评分函数、合成可及性评分）来确定模型的最终输出【可以参考RetroKNN,虽然我们不基于模板，但可以通过分子表征学习框架建立基于训练集的反应模板库，而后设计一个简单MLP以在确定最终模型输出时以不同的权重综合考虑Decoder的输出与反应模板库的输出】



​	4.LOSS函数：为实现encoder对分子性质的理解，针对单模态数据（如带有 IUPAC 名称的分子 SELFIES，ZINC20 的分子 SELFIES等）采用MLM（借鉴BioT5）让模型预测被掩盖的某些embedding，；针对多模态数据（如分子-文本对），采用对比学习（借鉴CLIP）。

​							为实现decoder对逆合成反应的理解，采用分子复原损失MR（借鉴PMSR），预测来自给定产品和前体未掩蔽部分的masked token；同时采用有监督的自回归AR损失（即预测产物），以保持与下游任务的一致性。【为增强模型对错误反应的规避，可以适当利用其他模型输出的错误反应】

​							为强化模型对反应中心（反应模板）的认识，共同在encoder、decoder上引入对比分类CC损失（强制分类任务，借鉴PMSR），强制要求同一类型中的所有反应都具有相似的嵌入【这一损失函数也可以用在表征学习时】

​		疑难：（1）如何实现，有些损失函数是现成的，有些似乎没有公开

​					（2）这么多损失函数，模型训的动吗（PMSR用了4个损失函数）

​						(3)先前老师说设计一个统一的LOSS函数

​					（4）怎样从错误反应中学到有价值的东西（对比学习，learning from nosiy label）【初步想法是设计一个非常简单的二分类任务，让模型区分对错】



​	5.训练时模型输入：

​								表征学习：化学反应数据集USPTO系列  

​								Encoder:单模态分子数据集、多模态分子数据集

​								Decoder:化学反应数据集USPTO系列，【其他模型输出的错误反应，对比学习，learning from nosiy label】

​			疑难：（1）USPTO数据集较少，PMSR训练时所用数据集大小在USPTO的五倍以上

​						（2）尚不清楚分子数据集有多少是公开可用的。

​						   (3)怎样收集其他模型输出的错误反应，怎样收集专业期刊中可用的文本信息





# 数据集

​	多模态:标准4M数据集（COCO SBU VG CC）、 LAion数据集（4B,5B） 

​	CV:Image net 22k(14M)

​    NLP:

# IDEA

仿BLIP 左脚踩右脚生成新数据集（图片——文本对）进而训练模型 类似于题改一改数字

模型的训练可以是一个从难到易的过程？简单题——中等——困难

从人的角度去思考问题：学习是一个从难到易的过程、



有两条路：1.一条是改变模型架构，但这事OGNN已经做了(但是OGNN没有用于无模板生成，只用在了LocalRetro)

​					2.另一条是从数据集角度出发  设计多模态小模型？