# 神经网络tricks

## data

## model

## loss

## optim

# Pytorch函数

## 指定CUDA（多个CUDA核心时）:

放在代码最开始，尤其是import torch前

import os
os.environ["CUDA_VISIBLE_DEVICES"] = "1"#两个CUDA时指定第二个，即系统此时只可见第二个cuda

#对于程序来说我们指定的cuda"1"被认为是cuda:0，即在下面代码中作为cuda:0使用

# 论文

![image-20240626173137132](.\image-20240626173137132.png)![image-20240626173253733](.\image-20240626173253733.png)作者：ML researcher
链接：https://www.zhihu.com/question/422296229/answer/3459572262
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。



ICML、ICLR、NeurIPS三个会各自有各自的特点。从不同角度看排名也各有千秋。以下是笔者根据自己审稿、投稿以及读论文的经验，从不同的目的和角度给出的个人看法：

**论文水平下限：ICML>ICLR>=NeurIPS:**

从我的审稿经验来看，ICML对论文的solid程度要求显著高于另外两个会。这其中的原因不止一个。首先，大部分reviewer本身对于ICML就有**要有理论、有大量实验支撑**的印象，因此reviewer在审稿的时候就是设置好了这个格子然后把论文往里面放。有朋友说，这三个会的审稿人都是一波人，但是即使是同一个人，审不同会的标准也会是不一样的。另外一个可能的原因（笔者审稿时候的个人感觉，不一定对），ICML的AC似乎比NeurIPS的AC更被动。NeurIPS的AC给人一种更相信自己判断的感觉，既敢于捞低分论文，也敢于拒绝高分论文，遇到分数不一致（比如7744僵持不下这种）更敢于自己下判断。但是ICML的AC似乎就更保守，比起7644这种论文更喜欢录665这种，这从一定程度上抬高了论文的下限（但是笔者本人其实非常反对这点，后面会具体说）。而ICLR更倾向于录有[novelty](https://www.zhihu.com/search?q=novelty&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A"3459572262"})的论文，哪怕论文内容没那么solid或是争议性很大。NeurIPS属于二者取平衡，在solid达标的基础上倾向于比较新鲜的工作。这样做虽然拉低了录取文章的下限，但是也是有好处的。

**对于找idea的读者，如果自己水平初级：NeurIPS>=ICLR>>ICML；水平高级：ICLR>NeurIPS>>ICML**

如果屏幕前的你读paper是为了给自己找idea，笔者更推荐ICLR和NeurIPS，因为这两个会有很多很新鲜的idea。刚才笔者提到，和ICML比起来，ICLR和NeurIPS更偏向novel，对solid程度有一定包容性。而新鲜的idea刚出来的时候往往是有很多不足和改进空间的。或者说，正是这样的不足和改进空间为后来者提供了自由发挥的基础。但是ICML对于这种不足相对缺乏包容性，所以ICLR和NeurIPS在提供idea这件事上，要比ICML好很多。ICML的审稿机制和风格导致它对对下面这种类型的论文尤其友好：

***已知一个很著名的模型work了，这个模型的某个零部件（比如[损失函数](https://www.zhihu.com/search?q=损失函数&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A"3459572262"})）是A+B，作者对这个模型做了一大堆实验，发现这里面有个问题，然后根据某个很强的假设，用一系列让人望而生畏的巨大公式，推导出这里不应该是A+B，而应该是c\*A+(1-c)\*B，并给出了这个c的一个形式或者某个下界，但是这个c是某个我们算不出来的东西（比如c可能包含有PAC可学习里面那一堆我们算不出来的复杂度），但是知道这个c大概的范围，因此可以把c当成参数来调。于是作者把A+B换成了[c\*A+(1-c)\*B](https://www.zhihu.com/search?q=c\*A%2B(1-c)\*B&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A"3459572262"})，通过在validation集上调参c，在测试集上将各种指标提高了1-2个百分点。***

这里我主要拿损失函数举例子。事实上还有别的情况，比如哪些参数共享哪些参数不共享，或者哪一层在前哪一层在后。我们不能说这种论文没有价值。实际上这种论文确实可能在工程上提供帮助。但是，对于找idea的读者，我们读完之后除了能说一句“woc，nb！”，好像也说不出别的什么话了。而相比之下，NeurIPS和ICLR上有不少“璞玉”型工作或者是可以借鉴的工作。

笔者私下和朋友聊天时也讨论过，我们发现好像那些特别突破性的工作的第一篇，发在NeurIPS和ICLR上的要比发在ICML上的多得多。这件事至少对于2019年之前是成立的，而2019年之前这仨会接受论文数量其实差不多，所以这件事似乎不是论文接受量造成的。笔者和朋友捋过的论文包括：GAN（NeurIPS），VAE（好像一直在Arxiv），AlexNet（NeurIPS），Transformer（NeurIPS），Attention（ICLR），GCN（ICLR），Adam（ICLR），NeuralODE（NeurIPS）。当然，这也许是因为ICML早期主要面向全体ML，而NeurIPS和ICLR主要面向[Deep Learning](https://www.zhihu.com/search?q=Deep Learning&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A"3459572262"})和神经网络。也有可能笔者和朋友过的论文不够全面，如果有朋友有其他意见的话可以在评论区补充。这个印象主要是笔者看[Ilya](https://www.zhihu.com/search?q=Ilya&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A"3459572262"})和另外几个大佬的google scholar感觉到的。

另外，感兴趣的朋友也可以看看我最近一篇[关于GAN的文章](https://zhuanlan.zhihu.com/p/691002382)。文章的最后笔者提到了GAN投稿到NeurIPS时的审稿意见。就笔者个人的审稿经历而言，如果当时GAN的审稿意见平移到ICML，那大概AC就听从低分审稿人的意见把论文拒绝了

不过，尽管同样是找idea，ICLR和NeurIPS差别有的，ICLR的论文天马行空程度要更大一些，对于初级水平的读者反而容易把握不住。所以对于初级读者，更推荐NeurIPS。

**笔者个人投稿/审稿体验：NeurIPS>=ICML>>ICLR**

笔者个人的投稿体验似乎和很多知乎上的朋友不太一样。ICLR对于笔者的主要槽点在于[openreview](https://www.zhihu.com/search?q=openreview&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A"3459572262"})。zhihu上的朋友们大多认为有openreview审稿人不敢乱来。但是笔者今年审ICLR的时候并没有这种感觉。笔者今年审稿ICLR时发现审稿人彼此可以看到个人信息，其中复数篇论文都有非常junior的审稿人，甚至可能包括没有领域内顶会发表的本科生。但是这些junior审稿人给低分的勇气和对自己判断的信心让笔者这个发过复数篇三大会的老phd感到望尘莫及，深感后生可畏。而与此同时，openreview有两个大问题：

第一，openreview上的论文在审稿阶段就对所有人开放。这就意味着，完全存在洗稿的空间。笔者和朋友合作的ICLR投稿就曾经被人洗稿然后投到某个领域内顶会（真顶会，是这个大领域内的三大会这个级别）并且还中稿了。最倒霉的是笔者因为平时基本只看三大会和自己的领域顶会，不怎么看这个洗稿论文投的领域（是我合作者的领域），所以几年后才被笔者的合作者发现。而笔者自己的论文被拒稿多次后则兜兜转转投了个CCF-C会然后不了了之。

第二，openreview上的论文审稿意见对所有人可见。因此，一旦被ICLR拒绝一次，后面recycle的时候审稿人可能就会带有偏见。笔者的另一篇ICLR投稿就遭遇过这个问题。当时笔者的论文ICLR上遇到了三个拒绝交流的审稿人。最后AC直接复制了在[rebuttal](https://www.zhihu.com/search?q=rebuttal&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A"3459572262"})里已经解答了的问题然后拒稿了。这种事情本来很常见，笔者也没有太放在心上，然后便将论文根据ICLR的审稿意见进行修改之后投稿给了ICML，结果一个ICML的审稿人直接复制了ICLR的[meta review](https://www.zhihu.com/search?q=meta review&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A"3459572262"})。。。

以上的问题让笔者对ICLR望而却步。至于ICML和NeurIPS，笔者审稿时感觉ICML的AC似乎主动性更低。笔者审稿NeurIPS时基本每篇论文的AC都会催促审稿人回复rebuttal，但是ICML似乎就不都是这样。而笔者投稿ICML时，则遇到过另一个事情。当时，AC在发现reviewer复制ICLR审稿意见之后又添加了一个新审稿人。结果这个新审稿人反复纠缠我们的理论证明，然而其提出的问题基本都是大一微积分（甚至是大一上半个学期的，比如这个审稿人看不懂[epsilon-delta](https://www.zhihu.com/search?q=epsilon-delta&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A"3459572262"})语言）里学过的知识。。。。。。最后论文被ICML再次拒稿（不过最后还是被NeurIPS捞了起来，所以笔者对NeurIPS可能也有一些倾向性的印象加分）

以上都是笔者的个人经验，其中包含不少小样本带来的幻觉。如果有朋友有不同的意见，可以分享在评论区，更好的帮助大家

## 如何读论文

1.title
2.abstract
3.introduction
4.method
5.experiments
6.conclusion
第一遍：**标题、摘要、结论。可以看一看方法和实验部分重要的图和表**。这样可以花费十几分钟时间了解到论文是否适合你的研究方向。

第二遍：确定论文值得读之后，可以快速的把整个论文过一遍，不需要知道所有的细节，**需要了解重要的图和表**，知道每一个部分在干什么，**圈出相关重要文献**。觉得文章太难，可以读引用的文献。

第三遍：提出什么问题，用什么方法来解决这个问题。实验是怎么做的。合上文章，回忆每一个部分在讲什么。



​	第二遍阅读完之后，你就对整个论文的各个部分，都有一个大概的了解，中间可以把作者引用的别人的相关文献圈出来，比如作者是在某某某的方法上进行了改进，做了哪些改进之类的。这里需要注意的是，如果你发现作者引用的这些重要文献是你没有读过的，那么你需要把它圈出来，作为你的稍后阅读清单

​	第三遍是最后一遍了，也是最详细的一遍，这里就需要自己知道每一句话在干什么，每一段在说什么
​        一边读，可以一边在脑子里面思考一些问题：
​            比如说，如果要是我来写这篇文章，我会如何组织这个结构？
​            读实验部分的时候，可以思考一下，作者是如何描述自己的实验的，你可以思考，如果换自己来做的话，能不能比作者做得更好？
​            这一遍读的时候，一定要明白作者每句话，每个字在说什么，并且最好可以脑补出它整个流程是什么样子的，似乎是自己在做实验，写论文一样。如果有困难的话，可以借助思维导图或者流程图这样的工具，把他的整个流程以可视化的形式展现出来，帮助自己理解。

## 大模型时代做科研的几点思路

<img src=".\image-20240621112318416.png" alt="image-20240621112318416" style="zoom:80%;" /><img src=".\image-20240621112435523.png" alt="image-20240621112435523" style="zoom:80%;" /><img <img src=".\image-20240621171615036.png" alt="image-20240621171615036" style="zoom:80%;" /><img src=".\image-20240621174951130.png" alt="image-20240621174951130" style="zoom:80%;" /><img src=".\image-20240621175015003.png" alt="image-20240621175015003" style="zoom:80%;" /><img src=".\image-20240621175152777.png" alt="image-20240621175152777" style="zoom:80%;" /><img src=".\image-20240621175220635.png" alt="image-20240621175220635" style="zoom:80%;" /><img src=".\image-20240621175245182.png" alt="image-20240621175245182" style="zoom:80%;" /><img src=".\image-20240621175339552.png" alt="image-20240621175339552" style="zoom:80%;" /><img src=".\image-20240621175450598.png" alt="image-20240621175450598" style="zoom:80%;" /><img src=".\image-20240621175517687.png" alt="image-20240621175517687" style="zoom:80%;" /><img src=".\image-20240621175538218.png" alt="image-20240621175538218" style="zoom:80%;" />

## NLP

### Transformer

### Bert

### GPT

## CV

### InstDisc（个体判别+memory bank）

链接

[李沐论文精读系列三：MoCo、对比学习综述（MoCov1/v2/v3、SimCLR v1/v2、DINO等） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/639246058)

意义

这篇文章提出了个体判别任务（代理任务）以及`memory bank` ，非常经典，后人给它的方法起名为InstDisc。

在有监督学习的分类模型中，如果给一张豹子图片进行分类，会发现排前几名的都是跟这张图很像的图片，而排名靠后的那些往往是跟豹子一点关系都没有的类别。

作者研究发现，让这些图片聚集在一起的原因并不是因为它们有相似的语义标签，而是因为这些照片里的物体都很相似。最后作者由此提出了个体判别任务：**把每一个instance（实例，这里就是指每一张图）都看成是一个类别（有多少图就有多少类），目标是学一种特征，把每张图片都区分开来。**

![img](https://pic4.zhimg.com/80/v2-03d50bfd7efab8b1e0ccabc2fa2a9e03_720w.webp)

模型概述

将图片经过**CNN网络**（CNN backbone）编码后得到的图片特征，使用对比学习的方式将其在特征空间中尽可能的区分开来（因为每张图都是自己的类）。

既然是对比学习，就需要正负样本。InstDisc中正样本就是就是这个图片本身（可能经过一些数据增强），负样本就是数据集里所有其它的图片，这些负样本都存储在 memory bank里。对于ImageNet有128万张图片，那么memory bank就要存储128万行，所以最后每张图都用128维特征表示（维度太高存储不了）

![在这里插入图片描述](https://img-blog.csdnimg.cn/20191111232810315.png)**前向过程**

- 𝑖𝑚𝑎𝑔𝑒→𝑅𝑒𝑠𝑁𝑒𝑡502048𝐷𝑖𝑚→128𝐷𝑖𝑚，即经过ResNet50编码得到128维的图片特征
- 论文的softmax不设置参数w。而是和Word2vec一样把特征当作参数，并创建一个叫做memory bank的堆进行存储所有单词的128维特征，**每次通过loss更新**。这样训练和测试通过存储的memory bank同使用一个度量空间。
- 论文取batch_size=256，则每个batch有256个正样本，然后从 memory bank 里**随机地抽取4096个负样本**。根据正负样本计算对比学习目标函数`NCELoss`。然后根据**loss更新backbone和memory bank**（把 mini batch里的数据样本所对应的那些特征，在 memory bank 里更换掉，这样无论是训练还是测试就都来自于一个度量空间了）。
- 测试时，使用KNN(K Nearest Neighbors)进行分类
  我们获得了训练好的模型后，对于一张图片提取他的特征，将他和memorybank中所有的存储图片特征计算相似度，然后采用k近邻算法，返回最相似的k张图片。最后根据相似度权重投票，得到其类别c。

**LOSS：NCE** ,详见MOCO

结论

`Inst Disc` 这篇论文也是一个里程碑式的工作：它不仅提出了**个体判别**这个代理任务，而且用这个代理任务和 **NCE loss**做对比学习，从而取得了不错的无监督表征学习的结果。同时它还提出了用别的数据结构存储这种大量的负样本，以及如何对特征进行动量的更新，所以真的是对后来对比学习的工作起到了至关重要的推进作用。



### Inva Spread（simCLR前身）

链接

[李沐论文精读系列三：MoCo、对比学习综述（MoCov1/v2/v3、SimCLR v1/v2、DINO等） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/639246058)

意义

`nva Spread`是一种**端到端**（也就是模型整体调整）的训练方式，直接训练特征本身，无需额外的数据结构（比如上文的memory bank），提升了效率和准确度。作者还使用了新的采样方式，降低了计算复杂度。

`Inva Spread`可以看做是`SimCLR`的前身，但由于数据增强策略不足以及负样本数量太少，也没有`SimCLR`提出的mlp projector ，使得最终的训练效果不好，没有太大的影响力。

> `Inva Spread`的作者太穷，没有TPU，只能选择`batch_size=256`来训练。这样每次迭代的负样本只有255*2个，数量太少，对比学习的效果不够好（也就是在MOCO中说过的字典太小）。而`SimCLR`的作者来自谷歌，可以使用大量的TPU，最终训练的`batch_size=8192`，足以达到不错的训练效果。





模型概述

作者认为提升效率的方法就是直接优化特征本身，拒绝额外的数据结构，也就是用端到端的方式。但这样做会有两种阻碍：一是如果抛弃通过参数w来学习，也不采用memory bank利用时间差更新而让特征自己乘自己，就会使得网络得不到训练。二是不采用NCE等方式，训练的复杂度就太大了。

作者认为，相似图片通过编码器以后，它的特征应该很类似，不同的图片，它的特征出来就应该不类似，这就是题目中说的invariant和 spreading 。于是作者提出的孪生神经网络结构，有效地解决了这两个问题：

![img](https://pic4.zhimg.com/80/v2-fc62a83132992e017c7cb3ddfd513f53_720w.webp)

- 设batch_size=256，即输入256张图片。经过数据增强，又得到了256张增强后的图片。这样每个batch有256个正样本和（256-1）*2个负样本。
- 根据正负样本计算loss（NCE loss 的一个变体），然后更新网络参数。
- 训练结果表示在最后特征空间中，就是绿色的两个球靠近，和所有别的球远离；其余类似。



### CPC（生成式代理任务）

**链接**

[李沐论文精读系列三：MoCo、对比学习综述（MoCov1/v2/v3、SimCLR v1/v2、DINO等） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/639246058)



**意义**

之前的几篇代理任务都是个体判别任务，那么自然也有**生成式的代理任务**，CPC就是其中之一，它**使用预测的代理任务去做对比学习**。CPC是一个**通用结构**，其输入是一个序列，可以是图片（不同patch）、文字或者音频、视频等等。



**模型概述**

- 对于一个输入序列x，当前时刻为t。t时刻输入经过编码器𝑔𝑒𝑛𝑐得到编码特征𝑧𝑡。

- 𝑧𝑡经过自回归模型𝑔𝑎𝑟（比如RNN/LSTM）得到输出𝑐𝑡（context representation，上下文特征，因为含有之前时刻的信息）。如果𝑐𝑡表示的足够好，包含之前所有时刻的信息，那么应该可以用来预测未来时刻的输出特征𝑧𝑡+𝑖。

- 对比学习的正样本就是未来的输入通过编码器以后得到的未来时刻的特征输出，负样本就是任意输入通过这个编码器得到输出。（感觉这里负样本都没说明白，老师一句话带过。别的博文说负样本是其它的输入序列，比如另一段音频的编码输出，但如果这样的话，前面一大段讲𝑐𝑡预测𝑧𝑡+𝑖有啥意义。还有很重要的互信息也没讲，这里先放着了）

  ![img](https://pic4.zhimg.com/80/v2-88c5a0c5b3ca3fef114638ce0e33a5a7_720w.webp)

  





### CMC（多视角正样本）

链接

[李沐论文精读系列三：MoCo、对比学习综述（MoCov1/v2/v3、SimCLR v1/v2、DINO等） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/639246058)

意义

`CMC`使用**一个物体的多个视角来作为正样本**。这个思想来自于人类对世界的感受、观察。
在摘要中，作者说人类观察这个世界是通过很多个不同视角的传感器，比如说眼睛或者耳朵，来给大脑提供不同的信号。每一个视角都是带有噪声的，而且有可能是不完整的。但是最重要的那些信息，比如物理性质，几何形状以及语义信息，在所有的这些视角中间共享（即互信息）。例如一只狗可以被看到、听到、感受到。

基于此，作者认为**一个强大的特征，应该具有视觉不变性**（不论是看到还是听到，都应该能判断出那是一只狗）。所以CMC目的，**就是最大化同一个场景不同视角的互信息，并且可以扩展到任意数量的未知视角，且视角越多效果越好。**



`CMC`正负样本确定的方式由个体升级成了个体的不同的视角（如色彩模型）。它同样使用了NCE，但将其扩展以适应不同的视角。`CMC`采用多视角对比学习，证明了对比学习的灵活性，也同时证明了多视角多模态的可行性，为之后的CLIP工作（图文配对的多模态对比学习）打下了基础。

但是本文也有一个局限，即处理不同的视角（模态）时，可能需要不同的编码器，因为不同的输入特点不一样。 如果每个视角都有一个编码器，那么训练的成本就有点高（比如在CLIP里，文本编码器是BERT，图片编码器是ResNet或者ViT）。

所以这也是现在`Transformer`最吸引人的地方，这个结构可以同时处理文本和图片，那么就可以用一个解码器处理两种模态，而不用做针对每种数据去做特有的改进。今年在ICLR上发表的[MA-CLIP](https://link.zhihu.com/?target=https%3A//paperswithcode.com/paper/ma-clip-towards-modality-agnostic-contrastive)，就是用一个`Transformer`去同时处理两个输入模态，效果反而更好。





模型概述

![img](https://pic2.zhimg.com/80/v2-620ff8c6a998947916efbf055ec08239_720w.webp)

`CMC`选用 `NYU RGBD` 数据集进行 训练。数据集中每张图有4个视角（view）：原始的图像、原图对应的深度信息（每个物体离观察者到底有多远）、SwAV ace normal以及原图的分割图像。

在CMC中，一张图的四个视角就是互为正样本，因为其代表的是同一个东西；其它的图片就是负样本。在上图表示，就是特征空间中四个绿色的点互相靠近，而都和红色的点远离。



### MOCO(队列存储+动量编码器)



链接

[李沐论文精读系列三：MoCo、对比学习综述（MoCov1/v2/v3、SimCLR v1/v2、DINO等） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/639246058)



意义

它是用一种**对比学习的方式进行无监督训练**的模型。`MoCo`是第一个在很多主流的机器视觉领域上（比如分类、检测、分割、人体关键点检测等），都超越了有监督预训练模型的无监督模型，从某种程度上证明了无监督学习在机器视觉领域，也能达到很好的效果。

`MoCo`虽然是基于对比学习的，但是本文是从另外一个角度来看对比学习，即把对比学习看作是一个**字典查询任务**。



模型概述

将对比学习中提到的𝑥𝑖当做是`query`，其它包括𝑥𝑖1、𝑥𝑖2这些图片都是字典中的`key`。我们每次判断正负样本，就是看字典中的这些key和query是否相似，而这些`key`都是通过`encoder`来更新的。

具体来说，我们构建了一个**动态的字典**，这个字典有两个特性：**队列**特性和`moving-averaged encoder`

> GPT和BERT已经证明了无监督的表征学习在NLP领域是非常成功的，但是在视觉领域，无监督学习效果差很多，作者认为可能是二者的原始信号空间不同。

- 在**NLP**任务中，原始信号空间是离散的（都是一些含有不同语义的单词或者词根），信号本来就拉得比较开，容易建立tokenize（将单词映射成向量）的字典。这样无监督学习容易建模，且模型容易优化。
- CV中，视觉信号都是在一个连续且高维的空间里，不像单词那样信息和语义浓缩的那么好，不够简洁，这样就不容易建立一个这样的字典，也就不容易进行无监督学习。

有一些无监督学习方法表现不错，但是都可以归结为**建立动态字典**。
如果将对比学习讲到的所有样本都构建到一个字典中，**字典的key就是各个样本，字典的value就是编码之后的特征**（后面直接以𝑘0表示第一个样本的编码特征）。我们先编码好锚点的特征，当做query；其它所有样本特征当做字典中不同的key，那么那对比学习就转化成为了一个字典查询的问题了。
如下图所示，我们训练一些编码器，再根据q去字典中查找key。查找的目的，就是让已经编码好的特征q，和与它匹配的特征key（其实就是正样本𝑥𝑖2的特征）最相似；与其它不匹配的特征不相似

![img](https://pic2.zhimg.com/80/v2-3408289e7fbb640198043ffc95065329_720w.webp)

一个好的字典应该：**1.字典非常大**，**2.特征一致性非常好**，从而便于进行对比学习。

- **字典足够大**

- - 字典越大，`key`越多，所能表示的视觉信息、视觉特征就越丰富 ，这样拿`query`去做对比学习的时候，才越能学到图片的特征。
  - 反之，如果字典很小，模型很容易通过学习一些捷径来区分正负样本，这样在碰到大量的真实数据时，泛化就会特别差（我的理解是，字典中只有猫和狗，狗都是黑色，猫都是黄色。模型简单的判断图片中物体是否是黄色，来区分猫和狗，而不是真的学到了猫和狗的特征）

- **编码的特征尽量保持一致性**
  **字典里的`key`都应该用相同或者说相似的编码器去编码得到**，否则模型在查找`query`时，可以简单的通过找到和它使用相同或者相似编码器的`key`，而不是真的和它含有相同语义信息的key（变相引入两一个捷径）。
  以前的对比学习，都至少被上述所说的两个方面中的一个所限制（要么一致性不好，要么字典不够大）。本文最大的贡献，就是使用队列以及动量编码器来进行对比学习，解决了这个问题。具体来说：

- `key`（编码特征）并**不需要梯度更新**，而是通过更新编码器，新的编码器使输出的`key`更新。（每个batch更新encoder,则新一批的key由新encoder生成）

- `queue` ：**整个队列里面的元素都是字典(即每个batch)**，队首输入当前batch的编码特征，队尾弹出最旧的batch特征。**每次移除的是最老的那些key**，从一致性的角度来说 ，有利于对比学习。

- - 用队列的好处是可以重复使用那些已经编码好的key，而这些key是从之前的那些mini-batch中得到的。

  - 用队列结构，就可以把的mini_batch的大小和队列的大小直接分开了，所以最后这个队列的大小，也就是字典的大小可以设的非常大，因为它大部分的元素都不是每个iteration都需要更新的。

  - 在字典里计算loss而不是整个数据集上计算loss，使用队列的数据结构，可以让维护这个字典的计算开销非常小。

    

- `momentum encoder`：

- - 如果只有当前batch的key是从当前的编码器得到特征，其它的key都是另外时刻的编码器输出的特征，这样就无法保证字典中key的一致性。所以作者又提出了动量编码器
  - 动量编码器，即编码器参数的更新方式就是𝑦𝑡=𝑚⋅𝑦𝑡−1+(1−𝑚)⋅𝑥𝑡（`MoCo`中`m=0.999`）。
  - 初始化的编码器来自于`query`的编码器，之后每次更新，**只有`1‰`的参数会从`query`的编码器参数里拿过来更新**，所以这个编码器参数更新的非常缓慢。从而保证了字典中所有的key都是由相似的编码器抽取得到的，尽最大可能地保持了他们的一致性。（直接更新编码器k的所有参数，会导致编码器更新过快，降低了这个队列中所有key的特征的一致性）



- 动态字典：字典中的key都是随机取样的，而且key的编码器在训练的过程中也是在不停的改变。





**损失函数**：infoNCE

![img](https://pic2.zhimg.com/80/v2-f3fbc4e0dfac22445e7c8df8aa61f70d_720w.webp)

<img src=".\image-20240422155211503.png" alt="image-20240422155211503" style="zoom:67%;" />

式子中，`τ`是一个超参数。如果去掉`τ`，整个式子其实就是**交叉熵损失函数**（`cross entropy loss` ），在后面的伪代码中，也是基于cross entropy loss实现。

- 分子表示q和正样本做计算，分母其实是**k个负样本*（k:负样本个数）*上做累加和，因为是从0到k，所以是k+1个样本，也就指的是字典里所有的key。
- 直接计算复杂度太大： `MoCo`使用 `instance discrimination`（**个体判别**）作为代理任务，那么光是ImageNet数据集，就有128万个类别，直接计算，复杂度会非常高，难以训练（128万类的softmax）。
- `NCE loss`（`noise contrastive estimation` ）：**将超级多分类转为二分类**——数据类别data sample和噪声类别noisy sample。这样**解决了类别多**的问题。

> `estimation`：近似的意思。为了**降低计算复杂度**，不是在每次迭代时遍历整个数据集128万张负样本，而是只从数据集中选一些负样本来计算loss（也就是选队列字典中的6万多个负样本(即每个batch)），相当于一种近似。MoCo`一直强调的希望字典足够大，因为越大的字典，越能够提供更好的近似。

`InfoNCE`:NCE的一个简单的变体.

- 作者认为如果只把问题看作是一个二分类（只有数据样本和噪声样本）的话，可能对模型学习不是很友好，毕竟在那么多的噪声样本中，大家很有可能不是一个类，所以还是把它看成一个多分类的问题比较合理。
- 公式中的q * k，其实就相当于是logit，也可以类比为softmax中的z。
- `τ`：**温度系数**，一个超参数，用来控制分布的形状 。τ越大，分布中的数值越小，经过exp之后就更小了，分布就会变得更平滑，相当于对比损失对所有的负样本都一视同仁，导致学习的模型没有轻重。τ越小，分布更集中，模型只关注那些特别困难的样本，其实那些负样本很有可能是潜在的正样本，如果模型过度地关注这些特别困难的负样本，会导致模型很难收敛，或者学好的特征不好去泛化。



**input:**在代理任务不一样的时候，输入𝑥𝑞和𝑥𝑘既可以是图片，也可以是图片块（CPC），或者是含有上下文的一系列的图片块。

**编码器：**`query`的编码器和`key`的编码器既可以是相同的（模型的架构一样，参数完全共享，比如Inva Spread），或者说它们的参数是部分共享的，也可以是彻底不一样的两个网络（CMC，多视角多编码器）。

**伪代码：**

下面是论文中作者给出的伪代码，其中：

- fq、fk分别是query和key的编码器
- queue这个队列指的是字典，里面一共有k个key，所以它的维度是`c*k`，c指的是每个特征的维度（`c=128`）
- `m`是动量，`t`是`InfoNCE`里面的超参数`τ`
- aug表示数据增强

1. 初始化编码器`fq`，并将其参数赋值给编码器`f_k`
2. 从data loader里拿一个batch的数据（n=bacth_size=256，n是采样数）
3. 通过数据增强得到正样本对`x_q`和`x_k`，然后通过各自的编码器得到特征`q`和特征`k`（大小都是`N*C`）。key不需要梯度回传，所以用.detach() 去掉梯度信息。
4. 计算N张图片的自己与自己的增强图的特征的匹配度
   `q 、k`之间计算`logit`（正样本），也就是之前公式1中算InfoNCE loss的时候的分子𝑞∗𝑘+，其特征维度就变成了`n * 1`（`256，1`）。
5. 计算N张图片与队列中的K张图的特征的匹配度
   `q、queue`拿出来计算，得到InfoNCE的分母，也就得到了负样本的logit，维度是`n*k`（`256*65536`，MoCo中，字典大小为65536）
6. 将正负样本logit进行`cat`拼接
7. 通过交叉熵损失函数实现loss计算。具体的，设置一个全0向量作为`ground truth`来进行计算。
   因为按照作者的这种实现方式，所有的正样本永远都是在logit的第一个位置上，也就是位置0，所以对于正样本来说，如果找对了那个key，在分类任务中得到的正确的类别就是类别0，所以巧妙地使用了这种方式创建了一个ground truth，从而计算出了对比学习的loss
8. 根据loss进行梯度回传，更新编码器`fq`
9. 动量更新编码器`f_k`
10. 更新队列（队首压入新的batch编码的key，队尾弹出最旧的key）

```python
f_k.params = f_q.params # 初始化
for x in loader: # 输入一个图像序列x，包含N张图，没有标签
    x_q = aug(x) # 用于查询的图（数据增强得到）
    x_k = aug(x) # 模板图（数据增强得到），自监督就体现在这里，只有图x和x的数据增强才被归为一类
    q = f_q.forward(x_q) # 提取查询特征，输出NxC
    k = f_k.forward(x_k) # 提取模板特征，输出NxC
    # 不使用梯度更新f_k的参数，这是因为文章假设用于提取模板的表示应该是稳定的，不应立即更新
    k = k.detach() 
    # 这里bmm是分批矩阵乘法
    l_pos = bmm(q.view(N,1,C), k.view(N,C,1)) # 输出Nx1，也就是自己与自己的增强图的特征的匹配度
    l_neg = mm(q.view(N,C), queue.view(C,K)) # 输出Nxk，自己与上一批次所有图的匹配度（全不匹配）
    logits = cat([l_pos, l_neg], dim=1) # 输出Nx(1+k)
    labels = zeros(N)
    # NCE损失函数，就是为了保证自己与自己衍生的匹配度输出越大越好，否则越小越好
    loss = CrossEntropyLoss(logits/t, labels) 
    loss.backward()
    update(f_q.params) # f_q使用梯度立即更新
    # 由于假设模板特征的表示方法是稳定的，因此它更新得更慢，这里使用动量法更新，相当于做了个滤波。
    f_k.params = m*f_k.params+(1-m)*f_q.params 
    enqueue(queue, k) # 为了生成反例，所以引入了队列
    dequeue(queue)
```



相关工作

**SimCLR**：端到端的学习方式

字典大小和mini_batch大小一致，但是现在一般是存不了太大的batch的，而且太大的batch难以优化，处理不好的话，不容易收敛，所以最终模型效果没那么好。

**memory bank （InstDisc模型）**

InstDisc有一个明显的问题，就是特征的一致性非常差。并且对于一个拥有亿级图片规模的数据，存储所有的特征就需要几十G甚至上百G的内存了，所以memory bank的扩展性不如MoCo好。

`MoCo` 的主要贡献就是把之前对比学习的一些方法都归纳总结成了一个**字典查询**的问题，并提出了**队列存储和动量编码器**。前者解决字典太大不好存储和训练的问题，后者解决了字典特征不一致的问题；从而形成一个又大又一致的字典，能帮助模型更好的进行对比学习。





### SimCLR（Projector head）

链接

[李沐论文精读系列三：MoCo、对比学习综述（MoCov1/v2/v3、SimCLR v1/v2、DINO等） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/639246058)

意义

- `SimCLR`可以被认为是`inva spread`的改进工作。其最大创新点就是在图片编码特征之后加了一个`projector`，但就这么简简单单的一层mlp，能让模型在ImageNet 分类任务上直接涨了近10个点。
- 使用更优的数据增强技术
- 使用更大的batch_size（256→8192）
  `SimCLR`的前两点贡献，添加`projector`和使用的数据增强，在之后的对比学习模型（MoCov2、BYOL）中也一直被沿用。

模型概述

- 图片x经过不同的数据增强得到不同的图片𝑥𝑖~和𝑥𝑗~，这两个就是互为正样本；同一个mini_batch里面的其它图片都是负样本，这点和`inva spread`一样。
- 正负样本经过同一个编码器𝑓(⋅)（权重共享）得到编码特征ℎ𝑖,ℎ𝑗。比如encoder选ResNet50，就是输出2048维特征。
- ℎ𝑖,ℎ𝑗经过同一个projector即图中的𝑔(⋅)（其实就是一个mlp层，**全连接层+relu激活**，具体见下图）得到最终的对比学习特征𝑧𝑖,𝑧𝑗（**128维**，消融实验证明128就够了）。是**最大创新点**
- 对比学习的训练目标就是使正样本特征更相似（同一张图片得到的 𝑧𝑖,𝑧𝑗），而负样本的特征不相似。
- 选用的损失函数是 `NT-Xent loss`（the normalized temperature-scaled cross entropy loss）。normalized是指在特征后面进行了 L2 归一化，temperature-scaled 就是说在 loss 里加了个τ，所以和`infoNCE loss`也是非常接近的。
- **projector在训练时才使用，推理时直接去掉，只用特征h特征。，也就是下游任务只用f不用g**

![img](https://pic4.zhimg.com/80/v2-cbc36c4b3d3ceb823fe993ef04118b87_720w.webp)

![img](https://pic4.zhimg.com/80/v2-b41624f71214a7f89ee6d635c3e61c53_720w.webp)

作者试验了10种数据增强，比如随机裁剪、变换色彩、翻转、Cutout、高斯噪声、blur噪声等等；并做了如下的消融试验（除了最后一列，余下是两两组合）。最后发现**随机的裁剪和随机色彩变换组合**效果最好。

![img](https://pic3.zhimg.com/80/v2-acdf14dd4df549d4293fbac811c2ea66_720w.webp)

![img](https://pic2.zhimg.com/80/v2-30f4913447ae729bb27c817d0078d97d_720w.webp)



### MOCOV2(引入Projection head)

链接

[李沐论文精读系列三：MoCo、对比学习综述（MoCov1/v2/v3、SimCLR v1/v2、DINO等） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/639246058)

意义

`MoCov2`主要是借鉴了`SimCLR`而做的优化，比如引入了**mlp projection head以及使用更多的数据增强**。`MoCov2`刷新了ImageNet 上的最好成绩，比之前的`MoCo`以及最新的`SimCLR`都高很多 。其上传的日期是3月9日，离`SimCLR`的发布还不到一个月。

直到现在，做一些一些对比学习的尝试工作时，还是会用`MoCov2`当基线模型，因为**训练快、效果稳，而且下游任务迁移的好**。

模型概述



![img](https://pic3.zhimg.com/80/v2-021d7cf1cb1b57fd9e82b834ae3606f6_720w.webp)

`MoCov2`对比`MoCo`主要有4个改动：

- 添加 projection head
- 使用更多的数据增强
- 训练时使用cosine的learning rate schedule
- 训练的epoch，从200增加到800

![img](https://pic1.zhimg.com/80/v2-150b67bb7630521d7fd06f66e87df2cc_720w.webp)



上图列出了模型效果对比图。

- MLP表示增加projection head，可以看到只增加这一点，就提了近6个点
- aug+和cos分别表示上面提到的数据增强和cosine schedule（余弦退火）
- 灰色行是有监督baseline模型





### SimCLRv2（引入动量编码器）

**链接**

[李沐论文精读系列三：MoCo、对比学习综述（MoCov1/v2/v3、SimCLR v1/v2、DINO等） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/639246058)



**意义**

`SimCLRv2`的主要思想体现在其标题里，即大的自监督模型很适合做半监督学习。在摘要中，作者提出：一种从少量带标签数据+大量无标签数据中进行学习的方案是：无监督预训练（必须是大模型）+有监督微调，这种半监督学习的方案在ImageNet上极为有效，具体的可以总结为三步：

1. `pretrain`：在无标签数据上无监督训练（SimCLR对比学习）一个Big ResNet模型（模型大小至关重要）以学习广义视觉特征表达。
2. `fine-tune`：在少量有标签数据上通过进行有监督的微调
3. `distill`：用微调后的模型作为`teacher`模型，在之前的无标签数据集上生成**伪标签**，然后训练一个`student`模型进行自监督训练（蒸馏阶段采用KL散度）。

> 微调后，作者发现：模型的任务已知预测属性可以进一步改善并蒸馏到一个更小的网络中。为此，作者对无标签数据进行了二次利用以促使学生网络尽可能的模拟老师网络的标签预测性能，且蒸馏阶段采用伪标签方式且不会造成额外的更多复杂度。
> 整个框架其实也是受启发于google的另外一篇工作 [Noisy Student](https://link.zhihu.com/?target=https%3A//paperswithcode.com/method/noisy-student)。`noisy student`就是在`ImageNet`数据集上先训练了一个 teacher 模型，然后在`JFT 300M`那个数据集上生成了很多的伪标签，最后一起训练了一个student模型，其精度为88，霸榜ImageNet快一年。

`SimCLRv2`在仅仅采用`1%/10%`有标签数据时，backbone使用ResNet50就取得了`73.9%/77.5%`的top-1精度。



**模型概述**

`SimCLRv2`相比`SimCLRv1`有三处改进：

- 大模型：backbone从`ResNet50`替换为`ResNet152+SK net` （selective kernels）
- 加深`protection head` ：**从一层加到两层**。
  protection head在SimCLRv1和MOCOv2中都被证明很有用，所以作者考虑多加几层。最后发现加到**两层**效果就够了
- 引入了**动量编码器**：使用了类似`MOCO`的动量编码器，效果提升了一个点。
  作者解释是，`SimCLR`模型的 batch_size已经够大了，也就是字典的大小和字典里特征一致性，SimCLR v2 都已经做的很好了。换成`MOCO`这种队列结构的动量编码器，虽然可训练的负样本更多，但是提升没有那么明显了。
- **微调**
  - `SimCLRv1`在微调时，是去掉𝑔(⋅)（projector层），只保留编码器𝑓(⋅)进行微调，即𝑓𝑡𝑎𝑠𝑘(𝑥𝑖)=𝑊𝑡𝑎𝑠𝑘𝑓(𝑥𝑖)；
  - `SimCLRv2`在微调时，是保留𝑔(⋅)的第一层 ，即𝑓𝑡𝑎𝑠𝑘(𝑥𝑖)=𝑊𝑡𝑎𝑠𝑘⋅𝜎(𝑊𝑀𝐿𝑃⋅𝑓(𝑥𝑖))

![img](https://pic1.zhimg.com/80/v2-14836db9959217f6cd46a5002a20b384_720w.webp)







### SwAV(负样本->聚类中心)

**链接**

[李沐论文精读系列三：MoCo、对比学习综述（MoCov1/v2/v3、SimCLR v1/v2、DINO等） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/639246058)



**意义**

`SwAV`即swap assignment view的缩写，意思就是一张图片不同视角的特征可以互相预测，因为来自同一张图片的不同视角特征按道理来说都是相似的。具体的做法，就是**将聚类加入到了对比学习**中。（将匹配问题转为预测问题，预测时借助簇类中心，而不是和所有负样本直接进行对比）。可以认为SwAV是承上启下。

`SwAV`也提出了一种**新的数据增强方法`Multi-crop`（多次裁剪）**。这个想法非常简单但是确实有用，**也是真正提点的技术**，在后面的很多对比学习中也被一直沿用。



**模型概述**

作者认为之前的对比学习，直接拿所有图片的编码特征去做对比有点原始而且计算量太大，因为所有的图片都是自己的类。作者考虑，能不能不做近似，能不能借助一些先验信息，一些更简洁的东西比进行对比，而不是和所有负样本直接进行对比。由此作者提出了可以**和聚类中心特征进行对比**（128万张图片被聚成3000个簇类中心`cluster center`）。

> 比如MoCo在ImageNet上训练那就有128万类，即使在计算loss时取近似，只是取队列编码器里的作为负样本，那负样本也有6万多个。
>
> 之前的一些聚类方法常常将`ImageNet`数据集聚成`3000`个簇类中心。

作者选择聚类这个想法有两个原因。首先，聚类方法也是一种无监督的特征表示学习方式，其目标也是希望相似的物体聚在一起，不相似的物体尽量互相远离，这个思想与做法和对比学习都比较接近；第二就是论文一作之前是做聚类的，比如deep cluster，也是一篇很好的无监督学习论文。

![img](https://pic4.zhimg.com/80/v2-b46c2da3e526693e68ae76f6b6cd2953_720w.webp)

- 左图：普通的对比学习方法。
  同一张图片，做两次数据增强得到𝑥1,𝑥2（正样本），然后所有的样本通过一个图片编码器（比如ResNet50等等，也可以加几层mlp之类的，这里没有具体说明）输出编码特征𝑧1,𝑧2，然后在编码特征z上去做对比学习。

- 右图：`SwAV`的做法

- - 每个batch输入数据为 𝑥∈𝑅𝑁∗𝐶∗𝐻∗𝑊， 分别经过不同的Aug， 得到 𝑥1,𝑥2
  - 将𝑥1,𝑥2 输入编码器中，得到编码特征𝑧1,𝑧2∈𝑅𝑁∗𝑑
  - **已知K个聚类中心prototypes** {𝑐1,...,𝑐𝐾}，表示为𝐶∈𝑅𝐾∗𝑑。**将编码特征与聚类中心计算相似度**，得到相似度矩阵𝑄∈𝑅𝐾∗𝑁，这样算完又获得了一个新的表示 𝑞1,𝑞2（Codes）
    理想情况下，样本与自己的类簇中心相似度为1，与其他的为0，类似于有监督任务中的one-hot label。不过作者发现soft label效果会好一些。
  - **理论上同一张图片不同view（比如Augment）所产生的 z 和 q 可以相互预测**。也就是说，如果拿𝑧1这个特征去跟c去做点乘，按道理来说也是可以去预测𝑞2；反之亦然。所以说点乘之后的结果就是预测，而ground truth就是之前按照clustering分类而得到的q1和q2。作者由此定义了新的loss：

𝐿(𝑧𝑡,𝑧𝑠)=𝑙(𝑧𝑡,𝑞𝑠)+𝑙(𝑧𝑠,𝑞𝑡)

其中

𝑙(𝑧𝑡,𝑞𝑠)=−∑𝑘𝑞𝑠(𝑘)log⁡𝑔𝑝𝑡(𝑘)

𝑝𝑡=𝑒𝑥𝑝(𝑧𝑡𝑇𝑐𝑘/𝜏)∑𝑘′𝑒𝑥𝑝(𝑧𝑡𝑇𝑐𝑘//𝜏)

- 通过这种Swapped prediction，也就是**换位预测**的方法，SwAV可以对模型进行训练 。

**用聚类做对比学习的好处到底有哪些？**

1. 减少计算量
   聚类可以将需要对比的样本数大大减少。比如之前的对比学习，需要去和成千上万的负样本进行对比，即便如此也只是算一个近似。而如果只是跟聚类中心做对比，则只需要最多3,000个聚类中心。因为ImageNet也就1,000类，COCO才80类，所以说 3,000个聚类中心就足够用了。
2. 聚类对比更加合理
   随机抽取的负样本中，有的可能还是正样本（本身就相似的图片），而且有的时候抽出来的负样本类别也不均衡。但是聚类中心是有明确的语意含义的，自然更加有效，这也是SwAV的基本思想。

**Multi-crop增强**

`SwAV`也提出了一种新的数据增强方法`Multi-crop`（多次裁剪）。

- 之前的那些对比的学习方法都是用的两个crop。比如输入一张图片，先把它resize 到256×256，然后随机crop两个224×224的图片当成 正样本对𝑥1,𝑥2
- `Multi-crop`：一张图片经过两个160×160的crop，和四个96×96的crop得到6个正样本。
  **前两个crop争取学习到全局特征，后四个crop争取学到局部特征**。因为之前crop尺寸是224，明显非常大，学习到的基本都是全局特征。如果可以学习局部特征，就更容易关注到局部的物体了。但是为了保持和原来计算量差不多，所以原先的尺寸从224降到了160。

这个想法非常简单但是确实有用，在后面的很多对比学习中也被一直沿用。





### CPCv2（融合tricks）

CPCv2其实也是融合了很多的技巧，它用了更大的模型、用了更大的图像块、做了更多方向上的预测任务，把batch norm 换成了 layer norm，而使用了更多的数据增强，所以这一系列操作下来，CPC v2直接就把CPC v1之前在 ImageNet 上40多的准确率一下就拔到70多。



### **BYOL**（不用负样本的对比学习）

**链接**

**意义**

`BYOL`就是论文标题`Boostrap Your Own Latent`的缩写。**Latent、Hidden、Feature、Embedding其实都是特征的意思**，就是各种花里胡哨的用法而已；Boostrap就是类似自我改造的意思。

`BYOL`使用了一种新的对比学习方法（A New approach），即**没有引入任何形式的负样本**，而是用图片的编码特征（梯度更新）去预测自己的编码特征（动量更新），模型就这样训练起来了。（相当于用一个视角的特征取预测另一个视角的特征，将匹配转为预测问题）

这种训练方式类似`SwAV`，但是这次连簇类中心都没了，所以听起来有点不可思议。后来还有一篇博文分析了`BYOL`，认为其实是在使用BacthNorm时引入了隐式的负样本进行对比学习（BN相当于提前看了一整个batch的内容）。BYOL作者一听不高兴了，这样不是说明我的工作大大折扣了吗，所以立马写了一篇技术实验论文驳斥了这个说法，证明了对比学习完全不使用负样本是可行的（后面会详细介绍）。



**模型概述**

![img](https://pic4.zhimg.com/80/v2-7d13797524edef7239f9c79cb05621ef_720w.webp)

![img](https://pic2.zhimg.com/80/v2-de94ebcc90a9daebd978e8080e91d575_720w.webp)

sg: stop gradient,即不计算梯度

前向过程：

- 输入x经过两次不同的Aug得到𝑣,𝑣′。

- 编码特征

- - 上面的online分支𝑣经过编码器𝑓𝜃得到编码特征𝑦𝜃，𝑓𝜃是梯度更新
  - 下面的target分支𝑣′经过编码器𝑓𝜉得到编码特征𝑦𝜉′，𝑓𝜉和𝑓𝜃模型结构一样，但用的是动量更新的方式。也就是说， 𝑓𝜉引入了MoCo中的**动量编码器**，其参数和𝑓𝜃不同，但是结构一样。
  - 如果这两个编码器都是ResNet50，则输出特征是2048维



- projection head

- - 使用类似`SimCLR`中一样的projection head 𝑔𝜉和𝑔𝜃（也是一个MLP，`BYOL`中也把这个结构叫`predictor`），将特征降到256维，得到特征𝑧𝜃,𝑧𝜉′。
  - 𝑔𝜉和𝑔𝜃分别是梯度更新和动量更新，但二者结构一样。



- 对比预测

- - 在 SimCLR中，是在𝑧𝜃,𝑧𝜉′之间做maximum agreement，即使不同增强后再编码和MLP映射后的特征尽可能的接近
  - 在SwAV中，是将𝑦𝜃,𝑦𝜉′分别和K个簇类中心c计算相似度得到𝑞𝜃,𝑞𝜉，然后互相预测作对比学习（𝑦𝜃和相似度矩阵点乘的结果去预测𝑞𝜉，反之亦然）
  - `BYOL`中，上分支使用`prediction head`（也是`predictor`结构）将𝑧𝜃映射为𝑞𝜃(𝑧𝜃)，然后**用𝑞𝜃(𝑧𝜃)去预测𝑠𝑔(𝑧𝜉)′来进行对比学习**，其中sg表示`stop-gradient`，因为下分支编码器是动量更新。
  - 损失函数是`MSELoss`，即直接计算预测特征𝑞𝜃(𝑧𝜃)和标签𝑠𝑔(𝑧𝜉)′这两个向量之间的mse。



**推理**：

**当训练完成只留下编码器𝑦𝜃用于下游任务**，剩下所有的东西都被拿掉了。然后用这个编码器编码图片，输出维特征去做下游任务的推理。

对比：

- 按过程来看，`BYOL`就是将上分支输入经过一个梯度更新的编码器和两个`predictor`得到的𝑞𝜃(𝑧𝜃)，去预测下分输入经过一个动量更新的编码器和一个`predictor`得到的𝑠𝑔(𝑧𝜉)′。
- 所以可以看出`BYOL`使用了MoCo的动量编码器、SimCLR的`projection head`以及预测任务，但是没有负样本，目标函数也不一样。通过自己预测自己就学起来了。
- `BYOL`的两个分支叫online和target，其实就相当于`MoCo`中的query和key分支。

**学习机制分析**

为何不使用负样本这么重要

- 在对比学习中，负样本是一个约束。如果在算目标函数的时候只有正样本，也就是让所有相似的物体的特征也尽可能的相似，此时就有一个很明显的捷径：模型输出恒等于输入，对比学习的loss永远都是0，模型直接就躺平（也叫模型坍塌`model collapse`，表示模型根本就没有在学习）。
- 只有加上负样本这个约束，即不相似的物体也要有不相似的特征，这样模型才会继续学习，否则负样本的loss就无穷大了。所以加入负样本能防止模型学到捷径，是必须的。
- `BYOL`之所以神奇就是它没有用负样本，正样本自己跟自己学最后在ImageNet上也达到了74.3的top-1准确率，也是相当高了。



`BYOL`被认为是使用了**隐式负样本**

`BYOL`发布到arxiv之后，在reddit、twitter、知乎全都引起了剧烈的讨论，因为大家都觉得很不可思议；不用负样本，只是自己预测自己，模型的学习怎么能不坍塌。由此引出了一篇博文[《Understanding self-supervised and contrastive learning with "Bootstrap Your Own Latent" (BYOL)》](https://link.zhihu.com/?target=https%3A//generallyintelligent.ai/blog/2020-08-24-understanding-self-supervised-contrastive-learning/)。
这篇博文的作者在复现`BYOL`时遗漏了一个小细节，即借用了 `MoCov2`的`projection head`导致`projection head`中**没有**加`batch norm`，最终模型坍塌。认为在`Projector`层使用BN之后，是计算了整个batch的均值和方差，这意味着是有信息泄露的（MoCo使用了 Shuffling BN ，就是为了防止这种信息泄露）。模型不光是正样本自己和自己学，还和batch norm产生的平均图片（mode，中值）对比着学，这种平均图片就类似 `SwAV`的聚类中心了。

所以说，这篇博客的作者认为batch norm是`BYOL`能够成功的关键，其实是做了一种隐式的对比学习，这个观点很快就被大家所接受了，因为听起来确实很合理，而且后续试验也都验证了这一点。batch norm确实至关重要，拿掉batch norm以后模型就是不好训练，对超参数的设置非常的敏感，稍有不慎它就啥也不学了。



**反转**

BYOL作者是在encoder（比如ResNet50）和两层`Projector`里分布使用BN/LN和什么都不用去做对比实验，最后发现：

- BN非常关键：只要是projector中没有BN的地方，`SimCLR`性稍微下降；但是`BYOL`全都模型坍塌了

- 有BN也会坍塌：作者找到了特例（红色框），即使当projector有BN的时候，`BYOL` 还是训练失败了 。如果BN真的很关键，它真的提供了隐式负样本的对比学习的话，训练就不应该失败

- 完全没有BN，`SimCLR`也坍塌（最后三列的结果。要注意`SimCLR`只有一层projector）。这表明完全不用归一化，`SimCLR`这种使用负样本进行对比学习的方式也无法训练。
  最终结论：**BN跟它原来的设计初衷一样，主要作用就是提高模型训练时的稳定性，从而不会导致模型坍塌** 。作者进一步延伸，如果一开始就能让模型初始化的比较好，后面的训练即使离开了BN也没有问题。

  作者为此又设计了一个实验，借鉴`BEiT`中的`group norm+weight standardization` （前者也是一种归一化方式，后者是一种模型初始化的方式，但都没有进行批量统计操作），BYOL的top-准确率可以达到74.1%，和原来精度可以认为是一样了（74.3%）。



### SimSiam（大道至简）

**链接**

[李沐论文精读系列三：MoCo、对比学习综述（MoCov1/v2/v3、SimCLR v1/v2、DINO等） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/639246058)

**意义**

`SimSiam`即simple Siamese network（**简单孪生网络**）。在BYOL发布时，就已经有很多对比学习的分析性工作了。大家发现，对比学习的成功好像是被很多trick一点点堆起来的性能，比如projection head、更多的数据增强、使用用动量编码器、更大的 batch size等等，好像都缺一不可。

这样因素太多就不方便分析，也不知道每个点到底带来了哪些贡献，所以凯明团队又再次出手，把整个过程**化繁为简**了一下，最后提出了SimSiam。

SimSiam结构非常简单，不需要用负样本（结构类似 `BYOL`）、大的batch size，也不需要动量编码器。然而即使在这种情况下，模型效果也很好。

实验证明去掉负样本、动量编码器、大的batch-size这些trick，模型也能训练的很好。



**模型概述**

具体的模型总览图如下图所示，整体结构非常类似 `BYOL`：

![img](https://pic2.zhimg.com/80/v2-8f2e9401b442e37e3c086948f803a8b1_720w.webp)

前向过程如下：

- image x经过两次数据增强得到𝑥1,𝑥2
- 经过两个编码器`encoder f`（结构一样参数共享，所以叫孪生网络）得到编码特征𝑧1,𝑧2。
- 𝑧1,𝑧2的经过`Projector`得到预测 𝑝1,𝑝2，然后计算**对称性loss**（𝑝1预测𝑧2，同时𝑝2预测𝑧1，单次结果除以2）。
- 和`BYOL`不同的是，这里没有使用动量编码器，**两个encoder完全一样**。



作者做了一系列实验分析，发现SimSiam能够成功训练，而没有模型坍塌，主要是因为有`stop gradient`这个操作。

BN有助于训练优化，但主要是提高模型训练的稳定性，而非避免模型坍塌（见第一行结果）。

![img](https://pic2.zhimg.com/80/v2-53f9e7da4d38121e8155151eea508e79_720w.webp)

- 对比`SimCLR`：SimSiam可以是作为“SimCLR without negative”（SimCLR依赖于负采样以避免“坍塌”）；
- 对比 `SwAV`：SimSiam可以视作“SwAV without online clustering”；
- 对比`BYOL`: SimSiam可以视作“没有动量编码器的BYOL”。



### MOCOv3（引入transformer）

链接

**意义**

无监督的预训练（BERT/GPT等）已经彻底改变了NLP，自从Vision Transformer成功之后，将**ViT引入CV领域的自监督训练**已经是大势所趋了。作者**将backbone从一个残差网络换成了ViT**,但是使用ViT作为backbone会导致训练很不稳定，这种不稳定性是造成模型准确率降低的一个主要问题。

本文作者发现只需要做一点小小的改动（**冻结ViT的`patch projection`层**），就能让这个训练变得更稳定、效果也更好。所以作者不得不写一篇论文来把这个发现告诉大家，也就是标题说的An Empirical Study （一个实验性的study ）。这篇论文是ICCV 21的一篇口头报告论文，但它的的影响力依旧很大。



**模型概述**

MoCo v3的架构，**其实就相当于是MoCo v2和SimSiam 的一个合体**。因为没有模型总览图，所以直接看伪代码：

```python
解释# f_q: query encoder: backbone + proj mlp + pred mlp
# f_k: key momentum encoder: backbone + proj mlp
# m: momentum coefficient
# tau: temperature，也就是τ
for x in loader: # load a minibatch x with N samples
	x1, x2 = aug(x), aug(x) # augmentation
	q1, q2 = f_q(x1), f_q(x2) # queries: [N, C] each
	k1, k2 = f_k(x1), f_k(x2) # keys: [N, C] each
	loss = ctr(q1, k2) + ctr(q2, k1) # symmetrized
	loss.backward()
	update(f_q) # optimizer update: f_q
	f_k = m * f_k + (1-m) * f_q # momentum update: f_k
	
# 对比 loss
def ctr(q, k):
	logits = mm(q, k.t()) # [N, N] pairs
	labels = range(N) # positives are in diagonal
	loss = CrossEntropyLoss(logits/tau, labels)
	return 2 * tau * loss
```

- 整体的框架来说，它还是有两个网络：query编码器和key编码器（动量编码器），目标函数是对比学习loss，所以说从这个角度讲，它是个`MoCov2`
- query编码器除了backbone之外，还有projection head和predictor，而是还算了对称性loss，即`loss = ctr(q1, k2) + ctr(q2, k1)`。所以从这个角度讲，它又是`SimSiam`。
- 所以说，从整体结构上来看，`MoCov3`就是`MoCov2`和`SimSiam`一个延伸工作。



作者发现，每次准确度大幅下降时，模型第一层梯度也会有一个波峰。于是作者尝试将这一层的权重全部冻住，结果发现问题就解决了。而且很神奇的**是这个trick不光是对`MoCov3`有用，它对`BYOL`和 `SimCLR`也有用。**

> 第一层就是`ViT`的`patch projection`层，会将图片分割成一个个patch，然后经过线性层映射为Pacth embedding。



### DINO(自蒸馏学习)

**链接**

**意义**

`DINO`这个名字，来自于它的题目self distillation with no labels，也就是**无标签的自蒸馏方法**（学生网络预测教师网络的输出）。本文和`MoCov3`一样，也是一种自监督训练Vision Transformer的方式，但作者使用另一种操作——**centering**（可以看做是计算整个batch样本的均值，然后减掉这个均值，类似于BN），使ViT可以稳定训练。另外本文发现自监督训练为 Vision Transformer features提供了一些新的特性。本文将ViT和自监督学习结合，并研究**自监督预训练对ViT feature的影响**。

> 自监督学习通过利用句子中的词创建`pretext tasks` ，相比于有监督学习中每个句子对应一个label， `pretext task`提供了更加丰富的学习信号。类似的，图像层面的有监督学习将丰富的图片信息减少到单一的分类概念。

通过研究，本文发现自监督ViT features具有一些独有的特性

1. 自监督ViT features 中**包含清晰的图像语义分割信息**，而这在有监督ViT和convnets中都没有类似的表现。

   一个完全不用任何标签信息训练出来的`Vision Transformer` ，将它的自注意力图进行可视化，会发现能非常准确的抓住每个物体的轮廓，效果甚至可以媲美对这个物体做分割

   ![img](https://pic4.zhimg.com/80/v2-32b50b01c83df4c7c06019a05414b007_720w.webp)

2. 只使用一个比较小的ViT backbone（`ViT-S/8`），自监督训练出来的ViT features 就能在KNN分类器中表现的很好，ImageNet数据集的 top-1精度达到78.3%，超过之前的自监督方法。（也就是ViT features直接去做最近邻分类，连线性分类头或微调都不需要）。

另外在消融实验中证明，动量编码器、multi-crop 数据增强和更小的 ViT patches（计算量更高）都有重要的作用。



**模型概述**

![img](https://pic1.zhimg.com/80/v2-ad2ebb613007a73c759852f26728ebcc_720w.webp)

前向过程：

- 一张图片经过不同的视角得到𝑥1,𝑥2
- 𝑥1,𝑥2分别经过两个编码器𝑔𝜃𝑠,𝑔𝜃𝑡（结构相同参数不同，包含projection head和prediction head）得到编码特征。
- teacher网络的编码器𝑔𝜃𝑡是动量更新；且为了避免模型坍塌，其编码特征会额外进行一个centering的操作
- 这样学生分支和教师分支经过softmax分别得到K维概率分布𝑝1,𝑝2，然后用𝑝1去预测𝑝2 （−𝑝2𝑙𝑜𝑔𝑝1）



- `DINO`的知识蒸馏是一种范式，是通过训练一个学生网络 𝑔𝜃𝑠 去match一个教师网络𝑔𝜃𝑡的输出。
- 两个网络分支最后分别输出概率分布𝑝𝑠,𝑝𝑡，这里概率P是对网络输出进行softmax归一化的结果：



𝑃𝑠(𝑥)𝑖=𝑒𝑥𝑝(𝑔𝜃𝑠(𝑥)𝑖)/𝜏𝑠∑𝑘=1𝐾𝑒𝑥𝑝(𝑔𝜃𝑠(𝑥)𝑘)/𝜏𝑠

其中温度参数𝜏𝑠>0控制分布的sharp程度。𝑃𝑡结果也是这样的公式算出。然后通过固定教师网络，训练学生网络使其参数𝜃𝑠最小化交叉熵损失函数来匹配分布：

𝑚𝑖𝑛𝜃𝑠𝐻(𝑃𝑡(𝑥),𝑃𝑠(𝑥)),𝑤ℎ𝑒𝑟𝑒𝐻(𝑎,𝑏)=−𝑎𝑙𝑜𝑔𝑏

- Teacher Network：和知识蒸馏不同，这里没有一个预先已知的teacher网络。teacher网络来自过去几轮的student网络，因为作者实验发现经过一个epoch训练后冻结teacher网络的训练方式表现不错。（应该是理解为教师网络使用动量编码器，如果参数全部从student网络复制，模型坍塌）



前向过程可以看出，DINO也是**自己预测自己**（student要预测teacher，teacher的输出当成是ground truth ），所以叫自蒸馏。DINO其实就是延续的BYOL，只不过是换了个名字。

| 模型 | 左分支          | 右分支          |
| ---- | --------------- | --------------- |
| MoCo | query 编码器    | key编码器       |
| BYOL | online network  | target network  |
| DINO | student network | teacher network |

- centering：可以看作在teacher分支上加一个偏置项c：𝑔𝑡(𝑥)←𝑔𝑡(𝑥)+𝑐，其中c通过EMA更新：𝑐←𝑚𝑐+(1−𝑚)1𝐵∑𝑖=1𝐵𝑔𝜃𝑡(𝑥𝑖)，m是一个大于0的参数。
- centering可以看做是计算整个batch样本的均值，然后减掉这个均值。centering类似BYOL对于 batch norm 的讨论，因为batch norm也是对整个batch里的样本做了一个均值和方差 。

### Vit（Transformer in CV,打破CV和NLP的模型壁垒）

**链接**

[李沐论文精读系列二：Vision Transformer、MAE、Swin-Transformer-CSDN博客](https://blog.csdn.net/qq_56591814/article/details/127358168)



**意义**

VIT的出现，打破了AlexNet出现以来CNN网络在CV领域的统治地位。VIT表明，在图片分类任务中，只使用纯的Vision Transformer结构也可以取的很好的效果（最佳模型在ImageNet1K上能够达到88.55%的准确率）开启CV新时代。而且 VIT将CV直接当做NLP来做，还打破了CV和NLP的模型壁垒，推进了多模态领域的发展。**比运用ResNet架构的CNN更便宜**



**模型概述**

vit/16,16:patchsize；有监督学习（作者尝试自监督学习，效果不好——》MAE）

简单而言，模型由三个模块组成：

- `Embedding`层（线性投射层Linear Projection of Flattened Patches）
- `Transformer Encoder`(图右侧有给出更加详细的结构)
- `MLP Head`（最终用于分类的层结构）

在CV中运用transformer,将图片**分成一个个16X16的Patch**，降低input的规模防止内存不够（Bert的d_model才512）**使图片各部分类似于句子中的一个个单词作为输入进transformer.**

(如图片原始分辨率224X224=50000，降低为196个图像块（224/16=14,14X14=196），每个图像块为16X16X3(channel)=768,

最终经线性层(Linear projection of FP,可训练，是一个768X768的矩阵，得到patch embedding)并与位置编码（197X768，1D位置编码，同Bert）**sum**后进入transfomer的**input**为197(196+CLS,借鉴bert)X768)，

经transfomer的**output**也为197X768(**输入输出同维，所以可以无限叠加transformer模块**)，

而CLS（视为图像整体特征）经过transformer后的输出作为transformer的最终输出，再进MLP Head![image-20240421114511057](.\image-20240421114511057.png)

因为Vit没有用到CNN的先验知识（平移不变性与locality），所以在小数据集上效果不好。

![image-20240421123334467](.\image-20240421123334467.png)











### MAE（Bert in CV,基于Vit并引入自监督学习）

链接

[Self-Supervised Learning 超详细解读 (六)：MAE：通向 CV 大模型 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/432950958)

意义

有标签的数据集**很贵**，打标签得要多少人工劳力去标注，那成本是相当高的，太贵。相反，无标签的数据集网上随便到处爬，它**便宜**

**模型概述**

MAE 的方法很简单：**Mask 掉输入图像的随机的 patches 并重建它们。**它基于两个核心理念：研究人员开发了一个**非对称**编码器 - 解码器架构，其中一个编码器**只对可见的 patch 子集进行操作** (即没有被 mask 掉的 token)，另一个简单解码器可以从**潜在表征和被 masked 掉的 token** 重建原始图像。Decoder 的架构可以是十分轻量化的模型，且具体的架构对模型性能影响很大。研究人员进一步发现，Mask 掉大部分输入图像 (例如 75%) 会产生重要且有意义的自监督任务。结合这两种设计，我们就能高效地训练大型模型：提升训练速度至 3 倍或更多，并提高准确性。

相较于文本，图片含有大量冗余信息。为避免Model可以根据邻居的冗余信息（局部的）仅进行简单工作（如插值）就能还原出图片，作者选择**盖住图片的大部分**（如3/4），以**降低信息冗余**，并**使模型去学全局的而非局部的信息**，即给模型创造出一个很有挑战性的任务。

并且，因为文本的语义空间更高维，从而在NLP领域解码器只是很简单的MLP即可（如Bert的下游矩阵），但是CV就需要复杂的解码器。（我的理解是：因为单词的语义信息比较丰富，所以不需要通过复杂解码器获取低层特征，可以直接由高层特征确定单词；而图片则需要用如转置卷积去重构图片）

对图片的处理同VIT，将图片分成若干Patch,随机抽样出一些Mask住.

**MAE Encoder** 采用 ViT 架构，但只会作用于 unmasked images。被Mask的块就不看了以节省开销。被Mask的块（统一用同一个共享的、可以学习到的向量表示，类似于Bert的[Mask]）与经过VIT的块（Patch被浅表示为特征向量）加上位置信息后（认为MASK块仅包含位置信息，不确定经过VIT的块还要不要再加一次位置信息，因为进VIT的时候就加过一次了）一起进入decoder

**MAE Decoder**，负责根据中间向量重构图片，其LOSS使用MSE（计算重构块**各像素**与原始块间差异），并只在MASK块上计算MSE（同Bert,没被盖住的块Model已经看过了，没必要算LOSS）。所以Decoder实际上是**预测被mask的pixel**而非patch.

<img src=".\image-20240421162514790.png" alt="image-20240421162514790" style="zoom:50%;" />



MAE 的具体实现方法是：

1. 首先通过 Linear Projection （即VIT）和位置编码得到 image tokens。
2. 随机 shuffle 这些 tokens，按照 masking ratio 扔掉最后的一部分。
3. 把 unmasked patches 输出到 Encoder 中，得到这些 tokens 的表征。
4. 把 Encoder 的输出，结合 masked tokens (可学习的向量)，执行 unshuffle操作恢复顺序，再一起输入到 Decoder 中。
5. shuffle 和 unshuffle 操作的时间开销可忽略不计。

**训练时训Auto-encoder（非对称的解码器与编码器，编码器更复杂，主要计算量来自于编码器）,实作时只用encoder，将其产生的中间向量（特征向量）拿去做实际下游任务**（如encoder+分类头）

### CLIP（跨模态无监督训练）

### Swin transformer

## 多模态

现有的VLP模型（Vision-and-Language Pre-training，视觉文本多模态模型）**抽取文本特征基本上都使用 pre-trained BERT的 tokenizer**来得到text embedding，但抽取视觉特征存在着差异。往往处理视觉特征的网络越复杂，模型效果就越好，所以抽取视觉特征是现有VLP模型的瓶颈。图上图所示，**获取visual embedding**的方法总共有三大类：

​	**Region feture**：通常采用Faster R-CNN二阶段检测器提取区域性特征，这种操作也是最贵的；比如图像经过ResNet101 backbone(一系列的卷积层用于提取图像的feature maps)提取特征，再经过RPN(Region Proposal Network，区域生成网络)得到一些RoI(Region of interest，感兴趣区域)，然后使用NMS(Non-Maximum Suppression，非极大抑制)过滤冗余的RoI，最后经过RoI Head(在RPN生成的候选区域中，对候选区域进行分类和边界框回归的神经网络模块)得到一些一维的向量（Region Feature），也就是一个个bounding box（检测框）。
​	**Grid feature**：将CNN backbone得到的feature map，作为网格特征，大大简化了计算量。比如将ResNet50最后得到的7×7特征图拉直为一个序列，或者是上一层的14×14的特征图。
​	**Patch projection**：使用类似ViT模型中的patch projection层直接得到patch embeddings，ViLT是首个这么做的。



***为什么用目标检测器来处理图像特征？***

​		1.图像的像素不能直接扔给Transformer，不然**序列长度就太长**了，Transformer处理不了。ViT提出将图片分割成一个个固定大小的patch，然后使用线性层映射为patch embedding输入网络（比如patch size=16×16时，处理后序列长度从224×224降为14×14）。
​		2.上面提到了**VLP想要的是离散的且语义性强的特征表示形式**，而目标检测正好是一个离散化的过程，返回的是 bounding box，它代表一个个物体，有明确的语义信息（可类比文本中的token），而且还是离散化的。
​		3.跟当时的 VLP 下游任务有关，当时主要是 VQA（Visual Question Answering）、Image Captioning、Image Retrieval 等等（这些任务的简介可以参考VL (Vision and Language) 任务简介及数据集），这些任务往往都跟物体有非常直接的联系，有非常**强的对物体的依赖性**。



**多模态特征的融合**有两种常见方式：

Single-stream：单通路结构，文本特征和图像特征直接concat连接，然后输入一个transformer进行交互；
Dual-stream：双通道结构，文本特征和图像特征分别过一个文本模型和图像模型，充分挖掘单模态特征，然后再经过一个transformer layer做融合。

 这两种方法的效果其实差不多，dual-stream明显更贵，参数量、计算量更多。







### 多模态论文串讲

### Transformer Encoder

#### ViLT（引入VIT）

**链接**

[【李沐论文精读】ViLT精读_李沐 vilt-CSDN博客](https://blog.csdn.net/qq_45276194/article/details/136648796)

**意义**

 ViLT是一个极其简单的**视觉-文本多模态**的框架。其最主要贡献：就是**把多模态学习框架中的目标检测，也就是论文中反复强调的Region Feature（区域性特征，也就是检测框）直接拿掉了**。这个操作简直算是神来之笔，因为它极大地简化了视觉模态特征的抽取过程，大大提高了模型的推理速度，可称之为多模态领域一个里程碑式的工作。

ViLT的三大贡献：

​	使用 **Patch projection层抽取视觉特征**，极大**简化**了多模态学习框架，减少了运行时间和参数量；
​	ViLT是第一个不使用卷积特征和区域性特征的同时（Without Convolution or Region Supervision），模型性能还**表现的比较好**的模型；
​	首次在VLP训练中使用了**整词掩码**和**图像数据增强**，并被证明可以明显提升模型性能。

![img](https://img-blog.csdnimg.cn/direct/f5df225182f740f58fba01a05d8c3053.png)

![img](https://img-blog.csdnimg.cn/direct/965027c805b64bc3a6044d3b9f585936.png)

(a)：VSE/ SCAN等模型的做法，视觉特征的处理远大于文本特征，模态融合只使用了简单的点乘操作或很简单的浅层attention网络；即VE > TE > MI
(b)：CLIP，每个模态单独使用transformer encoder，两者计算量差不多。特征融合部分，只是简单的计算了一下图文特征的相似性；即 VE = TE > MI。CLIP特别适合需要图文特征（GroupViT/GLIP等）或者是图文检索的任务，但做VQA或者visual reasoning（视觉推理，更难的VQA）这种需要视觉推理的任务时，会稍逊一筹。因为一个简单的不可学习的点乘，是没法做深层次的特征融合和分析的。
(c)：这些年80%的工作都是这个方向，比如ViLBERT、UNITER、Pixel-BERT等等。文本侧很轻量，但图像侧使用很重的CNN抽取特征；最后特征融合使用了Transformer，所以VE > MI > TE；
(d)：：本文的模型，ViLT，借助 ViT 的想法**把图像部分也变得非常轻量**。



**模型概述**

现有的VLP模型（Vision-and-Language Pre-training，视觉文本多模态模型）**抽取文本特征基本上都使用 pre-trained BERT的 tokenizer**来得到text embedding，但抽取视觉特征存在着差异。往往处理视觉特征的网络越复杂，模型效果就越好，所以抽取视觉特征是现有VLP模型的瓶颈。

如上图所示，**获取visual embedding的方法**总共有三大类：

​	**Region feture**：通常采用Faster R-CNN二阶段检测器提取区域性特征，这种操作也是最贵的；比如图像经过ResNet101 backbone(一系列的卷积层用于提取图像的feature maps)提取特征，再经过RPN(Region Proposal Network，区域生成网络)得到一些RoI(Region of interest，感兴趣区域)，然后使用NMS(Non-Maximum Suppression，非极大抑制)过滤冗余的RoI，最后经过RoI Head(在RPN生成的候选区域中，对候选区域进行分类和边界框回归的神经网络模块)得到一些一维的向量（Region Feature），也就是一个个bounding box（检测框）。
​	**Grid feature**：将CNN backbone得到的feature map，作为网格特征，大大简化了计算量。比如将ResNet50最后得到的7×7特征图拉直为一个序列，或者是上一层的14×14的特征图。使用目标检测器来提取图像特征实在是太浪费资源，于是就开始尝试把视觉这里的计算量降下来。其中一个尝试就是Pixel-BERT，它是用了一个在 ImageNet 上预训练好的 ResNet，将ResNet最后得到的特征图当成是一个离散的序列，然后和文本特征一起输入transformer做融合，速度就快很多
​	**Patch projection**：使用类似ViT模型中的patch projection层直接得到patch embeddings，ViLT是首个这么做的，有三个原因：
​			1.**不需要使用额外的网络。**无论是CNN backbone还是目标检测，都非常贵。
​			2.**不需要缓存特征。**Region feture和Grid feature都需要在线下使用预训练的模型提前抽取好图片特征，然后再训练。虽然这样训练还是比较轻量的，但在部署的时候是一个很大的局限性。真实场景里每时每刻都在生成新数据，都需要抽取新数据的特征，这时推理速度就是一大瓶颈了，所以作者才想设计一个更轻量更简单的视觉特征抽取方案。
​			3.**ViT的patch projection层表现很好。**和 Vision Transformer（简称ViT）的预处理一样，通过一个Linear Embedding层实现，将 patch 变成 token。ViLT在视觉方面的运行时间仅仅需要0.4ms，相比传统模型的运行时间大大减少，且模型效果并不会下降很多。

在**文本方面**，这些模型都是基本一样的，通过一个Embedding矩阵，变成一个个的word token。得到了视觉的序列和文本的序列后输入到Modality Interaction（基本都是Transformer）进行模态之间的融合。

​	多模态特征的融合有两种常见方式：

Single-stream：单通路结构，文本特征和图像特征直接concat连接，然后输入一个transformer进行交互；
Dual-stream：双通道结构，文本特征和图像特征分别过一个文本模型和图像模型，充分挖掘单模态特征，然后再经过一个transformer layer做融合。
       这两种方法的效果其实差不多，dual-stream明显更贵，参数量、计算量更多 ，所以作者**采用了Single-stream**。

![](https://img-blog.csdnimg.cn/direct/f52de759783a4c758bc0f5b930321c18.png)

文本经过pre-trained BERT tokenizer得到word embedding（前面有CLS token，图中*表示）
图片经过ViT patch projection层得到patch embedding（也是用*表示CLS token）；
文本特征+文本位置编码+模态嵌入得到最终的text embedding，图像这边也是类似的操作得到image embedding；二者concat拼接之后，一起输入transformer layer，然后做MSA交互（多头自注意力）

*问：为什么要进行嵌入区分？*

        模态嵌入即Modal-type embedding，使用0代表文本，1代表图像。因为在Single-stream模型中，图文特征是直接拼在一起输入一个transformer。如果不进行标注，模型是不知道哪一块是文本，哪一块是特征，这样不利于学习。加了模态嵌入可以区分之后，模型就可以在训练时找出图文之间的关系，学习的更好。



**目标函数**

​        ViLT使用了一般VLP模型常用的目标函数，即**图文匹配loss**（ ITM，image text matching）和 **BERT的掩码学习loss**（MLM，Masked Language Modeling）。另外ViLT还使用了**Word Patch Alignment（WPA）**。

ITM loss：以50%的概率将文本对应的图片随机替换成数据集中的其它图片，然后将文本CLS token对应输出使用一个FC层（即分类头）映射成一个二值logits，用来判断图像文本是否匹配（是不是一对）；说白了itm就是个二分类任务。但是太简单了。因为正样本可能不够好找，但是负样本很好判断，所以LOSS 很快会收敛。后期改进是找负样本中与正样本最接近那个负样本（ALBEF）。
MLM loss：随机mask一个文本token，然后将其重建出来。
其实图片这边也可以使用masked patch 重构任务，但是当时MAE还没出来，重构效果还不够好，所以作者没有这么做。后续有VL-BEiT，就使用了图像-文本掩码任务（masked vision-language modeling ）。
WPA：简单理解就是将文本和图像的输出都当做一个概率分布，然后使用最优运输理论计算一下两者的距离。**非常慢**，限制了ViLT的速度。



**Whole Word Masking整词掩码**
        将整个词都 mask 掉。作者在论文中举例，如果有单词 “giraffe”，如果用 tokenizer 如 BPE 等，那么 “giraffe” 就会被分成 [“gi”, “##raf”, “##fe”]，此时这些小部分才是一个个的 token，假设此时把其中的一个 token “##raf” mask 掉，即变成 [“gi”, “[MASK]”, “##fe”]，英文中以 “gi” 开头，“##fe” 结尾的单词很少，那么模型很容易就知道中间填 “##raf”，这就导致再做多模态时，根本不需要借助图像这边的信息，仅根据文本就能判断中间填 “##raf”，这样这个 loss 就失去了意义。既然如此，作者就将整个单词都 mask 掉，也就是在整个句子中去掉了 “giraffe”，这时模型再想把 “giraffe” 重建出来，就必须要借助图像的信息，进一步加强图像与文本间的联系。

 **Image Augmentation**
        ViLT是一个端到端的模型，作者在微调时直接就上了 RandAugment。考虑到需要图文匹配，作者改动了其中两处，即去掉了cutout和color inversion（前者是随机去掉图像中某一区域，后者是进行颜色变换）

*作者还提供了未来的几个可能的研究方向：*

Scalability：如果模型更大，用的数据集更多，那么结果也会更好。
Masked Modeling for Visual Inputs：在图像部分也做掩码重建（完形填空）。这部分当前已经有 BEiT 和 MAE 了，也已经有论文对 ViLT 在这部分进行了图像重建方面的改进。
Augmentation Strategies：根据消融实验来看，数据增强确实是很有效果的，作者希望可以优化这一块。



#### Clip（跨模态无监督训练）

**链接**



**意义**

CLIP(Contrastive Language-Image Pre-training)，是一种**基于对比文本-图像对的预训练方法**，属于对比学习。CLIP用**文本作为监督信号**来训练**可迁移**的视觉模型，使得最终模型的**zero-shot**效果堪比ResNet50，**泛化性非常好**。

CLIP算是在跨模态训练无监督中的开创性工作，作者在开头梳理了现在vision上的训练方式，从有监督的训练，到弱监督训练，再到最终的无监督训练。这样训练的好处在于可以避免的有监督的 categorical label的限制，具有**zero-shot**性质，极大的提升了模型的实用性能。

CLIP 最大的贡献就是**打破了之前固定种类标签的范式**，无论是在收集数据集时，还是在训练模型时，都不需要像 ImageNet 那样做 1000 个类，直接搜集图片和文本的配对就行，然后去预测相似性。在收集数据、训练、推理时都更方便了，甚至可以 zero-shot 去做各种各样的分类任务。**CLIP得到的图片与文本embedding是在同一个语义空间的。**

​	**方法出奇的简单，但是效果呢又出奇的好**，clip的这个**迁移学习能力是非常强**的。它预训好的这个模型能够在任意一个视觉分类的这个数据集上，取得不错的效果，而且最重要的是它是**zero shot**的，意思就是他完全没有在这些数据集上去做训练，就能得到这么高的效果，Clip在不使用ImageNet的训练集的情况下，也就是不使用任何一张的128万张图片训练的情况下，直接zero short做推理就能获得和之前有监督训练好的这个 res50取得同样的效果。

**用CLIP实现zero-shot分类只需要简单的两步**：

- 根据任务的分类标签构建每个类别的描述文本：`A photo of {label}`，然后将这些文本送入`Text Encoder`得到对应的文本特征。如果类别数目为n，那么将得到`n`个文本特征；
- 将要预测的图像送入`Image Encoder`得到图像特征，然后与`n`个文本特征计算缩放的余弦相似度（**和训练过程保持一致**），然后选择相似度最大的文本对应的类别作为图像分类预测结果。进一步地，可以将这些相似度看成logits，送入softmax后可以到每个类别的预测概率。

我们不再需要预先定义好的标签（类别）列表，直接将图片喂给不同的文本句子，就可以知道图片中是否有我们感兴趣的物体。即，CLIP的多模态特性（利用文本监督信号）为具体的任务构建了动态的分类器，使得模型不再受限于预先定义好的类别，更加具有通用性和可用性。

> 比如新增三轮车的图片时，只需要在文本部分也加上三轮车这个类别，模型很有可能直接`zero-shot`推理出图片属于三轮车这个类。而之前的模型，是永远不会预测出ImageNet1000个类之外的类的，这也是`CLIP`最吸引人的地方。
>
> 　　类别单词变成句子，有`prompt engineering`和`prompt ensemble`两种方法，进一步提高模型准确率，在论文后面会讲到
>
> 

CLIP 这种**双塔式多模态特征提取、模态交互部分较为简单**（点乘计算**余弦相似度**）的多模态模型十分适合**图像文本匹配、图像文本检索等多模态任务**。因为这种模型有一个显著的优点，即可以将数据库中的图像或文本特征预先进行特征提取并保存到硬盘中。这使得检索过程仅需要提取待检索图像的特征并计算相似度即可，大大提升了检索过程的效率（矩阵运算非常快）。



**模型概述**

模型的结构很简洁，就是将image和text通过两个各自模态的encoder提取feature之后，将互相配对的image-text所属的feature作为正样本（图中矩阵对角线），其余不配对的样本作为负样本（除对角线之外的元素）来进行**对比学习**。（即文本与图像是两个transformer,模态融合部分是**点乘余弦相似度**)

假如模型输入的是![n](https://latex.csdn.net/eq?n)对图片-文本对，那么这![n](https://latex.csdn.net/eq?n)对互相配对的图像–文本对是正样本（下图输出特征矩阵对角线上标识蓝色的部位），其它![n^2-n](https://latex.csdn.net/eq?n%5E2-n)对样本都是负样本。这样模型的训练过程就是最大化![n](https://latex.csdn.net/eq?n)个正样本的相似度，同时最小化![n^2-n](https://latex.csdn.net/eq?n%5E2-n)个负样本的相似度。相似度是计算文本特征和图像特征的**余弦相似性**cosine similarity。（即模态融合部分是**点乘余弦相似度**)

<img src="https://img-blog.csdnimg.cn/direct/e9da0520bd124a4dbff148b65f7c6f76.png" alt="img" style="zoom: 80%;" />



**预训练方法（训练效率至关重要）**

CV领域的模型都很大，训练起来也很贵。比如[noise student](https://link.zhihu.com/?target=https%3A//paperswithcode.com/paper/self-training-with-noisy-student-improves)之前在ImageNet一直霸榜，但是这个模型需要在一个 TPUv3上训练33年，这还只是在包含1000类的ImageNet上预训练的，而且只训练视觉特征。

由于训练数据量和模型计算量都很大，训练效率成为一个至关重要的因素。作者做了很多尝试，最终选择了**对比学习**：

- `VirTex`模型：预测文本，对应下图蓝色线`Transformer Language Model`

- - `Image Encoder`使用CNN模型，`Text Encoder`使用transformer模型，两个模型一起从头训练，任务是预测图片对应的文本（image caption）。
  - 这种方法的训练效率太慢，因为根据图片进行文本描述，可能性太多了，你可以从各个角度去描述一张图片。

- `Bag of Words Prediction`（橘色线）：不要求每个词都是按顺序的进行预测，所有词都预测出来就行。这样放宽了约束，训练速度提高了三倍。

- `CLIP`：简化版的`ConVIRT`，基于对比学习。

- - **只需要判断图文是否配对**，进一步简化了训练任务，训练效率一下子提升4倍（绿色线）
  - 训练任务更加合理。因为训练数据所包含的文本-图像对是从互联网收集来的，它们存在一定的噪音，二者并不完全匹配。适当的降低训练目标，反而能取得更好的收敛。





![img](https://pic2.zhimg.com/80/v2-a44d2d31afcb41ea2f6f0147cd3dda9d_720w.webp)

**模型的输入**是若干个**图像-文本对**

如图最上面的数据中图像是一个小狗，文本是 ”Pepper the aussie pup”）。

- 文本部分：文本通过一个Text Encoder得到一些文本的特征。同样假设每个training batch都有 N 个图像-文本对，那么就会得到N 个文本的特征（如下图中的![T_1,T_2,...,T_N](https://latex.csdn.net/eq?T_1%2CT_2%2C...%2CT_N))![img](https://img-blog.csdnimg.cn/direct/6737a17ef14643baa8a70fe3836fe568.png)
- 图像部分：图像通过一个Image Encoder 得到一些特征，这个 encoder 既可以是 ResNet，也可以是 Vision Transformer。假设每个 training batch 都有 N 个 图像-文本 对儿，那么就会得到 N 个图像的特征（如下图中的）![img](https://img-blog.csdnimg.cn/direct/a1bdf47f7433454c8c492a7016081c19.png)

CLIP 就是在以上这些特征上去做对比学习，对比学习非常灵活，只需要正样本和负样本的定义，其它都是正常套路。这里配对的图像-文本对就是正样本（即下图中对角线（蓝色）部分，![I_1\cdot T_1,I_2\cdot T_2,...,I_N\cdot T_N](https://latex.csdn.net/eq?I_1%5Ccdot%20T_1%2CI_2%5Ccdot%20T_2%2C...%2CI_N%5Ccdot%20T_N))

![img](https://img-blog.csdnimg.cn/direct/1f3a5c9bfccc4e2a810d89fdecf91f89.png)



LOSS:ITC（image-text contrastive）来对齐文本和图像的嵌入空间  LOSS：目的是把成对的图像-文本拉近。让正确的图像文本对之间的距离越小越好，让错误的图像文本对间的距离越大越好。

![image-20240424121134960](.\image-20240424121134960.png)

axis=0:横轴   axis=1：纵轴

> OpenAI是一家GPT化的公司，从GPT系列、DALL-E到Image-GPT等等都是基于GPT做的，唯有`CLIP`因为效率的原因，选择了对比学习进行训练。

最终`Text Encoder`固定选择一个包含63M参数的text transformer模型，而`Image Encoder`采用了两种的不同的架构。因为CLIP虽然是多模态模型，但它主要是用来**训练可迁移的视觉模型**。

- `Image Encoder`架构

- - ResNet：ResNet50，ResNet101，RN50x4，RN50x16和RNx64（后面三个模型是按照EfficientNet缩放规则对ResNet分别增大4x，16x和64x得到）
  - ViT：ViT-B/32，ViT-B/16和**ViT-L/14**。

- ViT-L/14效果最好，所以作者还将其在336的分辨率下**额外finetune了一个epoch来增强性能**，记为 `ViT-L/14@336px`。后面论文中没有特别说明的情况下，进行对比实验的CLIP模型都是指这个。

![img](https://pic2.zhimg.com/80/v2-f6853b4b1545eabd5b31d34c0a8ecef5_720w.webp)



- 训练细节

- - 数据集非常大，几乎不会出现过拟合，所以`Image Encoder`和`Text Encoder`不需要提前进行预训练。
  - 只使用线性投射层（线性非线性影响不大）。
  - 数据增强只使用图片的随机剪裁，这是因为数据集非常大。
  - 对比学习目标函数中的超参数`τ`，设置成可学习的标量，在训练中自动优化，而不用慢慢调参（还是因为数据集太大，训练很贵）。

**prompt engineering** 

作者提出 **prompt template**：以 ImageNet 为例，CLIP 先把 ImageNet 这1000个类（如图中"plane", “car”, “dog”, …, “brid”）变成一个句子，使推理和预训练时保持一致（**消除distribution gap**）。，也就是将这些类别去替代 “A photo of a {object}” 中的 “{object}” ，以 “plane” 类为例，它就变成"A photo of a plane"，那么 ImageNet 里的1000个类别就都在这里生成了1000个句子，然后通过先前预训练好的 Text Encoder 就会得到1000个文本的特征。

  其实如果直接用单词（“plane”, “car”, “dog”, …, “brid”）直接去抽取文本特征也是可以的，但是因为在模型预训练时，**与图像对应的都是句子**，如果在推理的时候，把所有的文本都变成了单词，那这样就跟训练时看到的文本不太一样了，所以效果就会有所下降。并且词语存在**歧义性**：

- 比如在做物体检测时，有一个类别是remote（遥控器）。但如果直接喂给文本编码器，很可能被模型认为是遥远的意思。
- 同一个词语在不同数据集中所表示的意思可能有所不同。例如在 Oxford-IIIT Pets 数据集中，boxer指的是狗的一个种类，在其他数据集中指的是拳击运动员。
- 所以 CLIP预训练时，用来描述图片内容的文本是一个句子，比如`A photo of {label}`。这里的label就只能是名词，一定程度上消除了歧义性。



**prompt ensembling**

作者尝试了集成多个模板的效果，即在多个zero-shot分类器上进行集成，这些分类器使用不同的提示模板来构造不同的文本。由于是在嵌入空间(embedding space)而不是概率空间(probability space)上集成的，因此节约了计算成本。在大多数数据集上，`prompt ensembling`都能够提升模型性能。

最终作者使用了80种模板来进行集成，每种模板使用了不同的形容词，来描述不同的情境。



![img](https://img-blog.csdnimg.cn/direct/24a615463c3a46eead897d200b49ab00.png)

**VE（视觉嵌入）、TE（文本嵌入）和 MI（模态交互）**分别表示不同的网络组件，他们在途中模块的大小表示对应网络的复杂度。

**视觉部分**：在图 1 所示的四种结构中，(a)、(b)、(c)三种方法中，由于使用**目标检测器**确定图像的区域，因此视觉端（Visual Embed）都是一个复杂的网络。在 (d) 中，也就是 ViLT 中，使用简单的**线性映射**(patch project)，实现了视觉端处理。ViLT 将网络的大部分复杂度放在多模态任务中重要的模态交互部分。 问题是图像特征提取过于简单。一张图像中包含的信息多于一个句子的信息，按照道理，图像特征提取器应该比文本特征提取器更复杂。

**文本部分**：a:词嵌入 b:transfomer c:词嵌入 d:词嵌入   词嵌入：一般是pre-trained BERT的 tokenizer，比较轻量化

**MI**：a: 点乘 b:点乘 c,d:transformer			只采用点乘作为模态融合手段。这样无法处理复杂的多模态融合。多个模态的融合要有一定的复杂度。



ViLT论文的研究动机其实就是为了把目标检测从视觉端拿掉。图文多模态任务，关键是提取视觉特征和文本特征，然后对齐。在之前的多模态研究工作中，视觉侧通常需要一个目标检测器来确定图像中物体所在的区域，再提取各区域的特征。ViT 将 Transformer 迁移到视觉领域之后，人们意识到，直接使用 patch projection 来处理图像输入也是可行的。由此，ViLT 首次使用 patch projcetion 来直接提取图像特征，摆脱了笨重的目标检测器。  但ViLT由两个局限性  

虽然 ViLT 通过改用线性映射，降低了视觉端嵌入网络的复杂度，但是**性能有所下降**。原因是文本端的 tokenizer 已经有一定语义理解能力了，而视觉端的 patch embedding 是随机初始化的。
虽然 ViLT 的推理很快，但是训练时间比较长。

 **CLIP**一种典型的双塔结构：在视觉端和文本端分别有一个独立的编码器来提取视觉特征和文本特征，而模态间的交互就是简单的**点乘余弦相似度**。CLIP可以提前把数据库里所有的图像文本的特征提前抽好，想用的时候直接点乘。

CLIP的缺陷：

CLIP 这种**双塔式多模态特征提取、模态交互部分较为简单**的多模态模型十分适合图像文本匹配、图像文本检索等多模态任务。因为这种模型有一个显著的优点，即可以将数据库中的图像或文本特征预先进行特征提取并保存到硬盘中。这使得检索过程仅需要提取待检索图像的特征并计算相似度即可，大大提升了检索过程的效率。然而，也正是由于**模态交互过程过于简单**，这类模型在面对视觉问答、视觉推理等需要处理复杂的模态间关系的任务时，效果一般。
        总结：根据ViLT论文里的figure2, 可以得出这样一个结论，我们需要**好的visual embedding**，图像编码器比文本编码器要大（假设1），因为图像更为复杂，同时**modality interaction也要很好**（假设2）而不只是一个简单的点乘交互；text embedding已经很成熟，一般是pre-trained BERT的 tokenizer，这个已经很轻量化了。因此我们总结出理想化的情况应该是接近下图(c)的情况。



至此，我们可以在图 1 的基础上，分析现有模型结构与损失函数，得到接下来多模态学习可行的方向。

我们可以考虑一些常用的loss: ，下面前三个loss是比较好用的。

**ITC**:Image text contrastive loss（CLIP模型训练方式） 
**ITM**:Image text matching loss（ViLBERT和ViLT使用过）
**MLM**:Masked language modelling loss（BERT训练方式）
**WPA**:Word patch alignment (这个在vilt中用到，但是计算很慢)
        通过总结，就可以引出ALBEF



#### ALBEF(ALign BEfore Fuse，博采众长，开源代码)

**链接**

**意义**

在 ALBEF之前，多模态领域的工作通常是使用一个 Transformer 模型来联合编码文本 token 和图像 token。其中图像 token 是经过目标检测器检出的图像区域，目标检测器是预训练得到，而非与整体网络一起进行端到端的训练。因此文本与图像没有“**对齐**”（align）。ALBEF 提出在进行多模态交互之前，先通过一个**对比损失**（其实就是 CLIP 中的 **ITC 损失**）来对齐图像和文本数据(拉近配对的图文见距离)。这是 ALBEF 的第一个贡献。

第二个贡献是在训练数据和训练方式上。ALBEF 在训练时，通过**动量蒸馏**（momentum distillation）这种自训练的学习方式来从网络图文对数据中学习，缓解原始数据中噪声较大的问题。ALBEF 通过改进训练方式，通过自学习生成伪标签的方式来进行数据清洗，改进数据的质量。在理论上，ALBEF 论文通过互信息最大化的角度，解释**不同的损失函数LOSS**其实是在为同一个图像文本对，其实就是在为图文对提供不同的视角（view），类似于在做一种数据增强，使得训练得到的多模态模型能理解不同模态下的语义，即具备 Semantic Preserving 的能力。



**模型概述**

![img](https://img-blog.csdnimg.cn/direct/162f7395c3574b0db600b07f271c17c5.png)

在这张图中，满足了我们两个假设，**文本编码器（6X）比图像编码器（12X）小**且**模态融合（6X）也大**。

同时也用了我们提到的三个loss去训练模型：Image-text contrastive loss(**ITC**)、Image text matching loss(**ITM**)和Masked language modelling loss(**MLM**)。



**目标函数**
        通过对比学习可知，只要定义一个正样本对和多个负样本对，就可以进行对比了。我们希望正样本对之间的距离越来越近，正负样本对之间的距离越来越远。首先要做的就是去出去这种全局特征，在特征之间去做embedding space上的拉近和拉远。

ALBEF 有三个预训练 loss：

- Image-Text Contrastive Loss (**ITC** loss)：让正确的图像文本对之间的距离越小越好，让错误的图像文本对间的距离越大越好。它就是 CLIP 中的损失函数。图像和文本分别通过encoder tokenise, CLS token是一个全局特征（图中绿色方块旁边的黄色方块）， down sample和normalisation(786x1 => 256x1)，然后进行正负样本的对比学习（预先存了很65536个负样本q，没有gradient，由Momentum Model产生），这一步就是**align**。
- Masked Language Modeling (**MLM** loss)：类似BERT的完形填空，mask一个词语，去预测mask的词语，但是借助了图像的信息。
- Image-Text Matching (**ITM** loss)：判断某个图像和文本是不是正确的一对，是一个二分类任务。在multimodal encoder的输出之后加一个二分类头(FC层)，这里很特别的是，每个batch里我拿一个图片和batch里除了配对文本之外的所有的文本做cosine similarity (，余弦相似度，借助之前ITC的那个模块)，挑一个相似度第二高的作为负样本 (**hard negative**) 来训练，来加大难度（这个相似度第二高的负样本几乎已经可以当正样本来使用）。


​        在这里有一个小细节，计算ITC和ITM loss的时候，输入的都是原始的image and text embedding（下图橙色的T '表示masked text embedding），算MLM loss的时候，用的是原始的image embedding，但是是masked后的text embedding，因此每一次训练iteration其实做了**2次forward**，一次用了原始的image and text embedding，另一次用了原始的image和masked的text embedding，因为你要算多个loss函数，这也是多模态往往训练较慢的原因，每次迭代要两遍forward。

![img](https://img-blog.csdnimg.cn/direct/efff86198ee44d95b8bbf9c930d855da.png)



最后的目标函数计算如下：![L=L_{itc}+L_{mlm}+L_{itm}](https://latex.csdn.net/eq?L%3DL_%7Bitc%7D&plus;L_%7Bmlm%7D&plus;L_%7Bitm%7D)



 **Momentum Distillation**

ALBEF 中动量蒸馏的提出，是为了解决网络图文对训练数据噪声过大的问题。

​        做动量蒸馏(Momentum Distillation)的动机：从网上爬下来的图像文本对通常weakly-correlated，即文本并没有很好地描述图像，从而产生了noise。这种弱关联的训练样本中可能出现某些负样本的图文匹配程度，比 GT 中正样本的 one-hot 标签的匹配程度更高的情况，见下图，一昧的惩罚这种负样本不利于 ITC 和 MLM 两种任务的训练。

比如一张青山绿水的景点照片，网络上的对应文字不会是“一座很美丽的山，下面有清澈的河流”这种我们想要的描述性的文本，而很可能会是这个景点的名字，如“桂林山水”。从语义的角度来说，这样的图文对是弱关联（weakly correlated）的，不是我们想要的训练样本。

![img](https://img-blog.csdnimg.cn/direct/f684323c8d6b4c9a80fade5987bbf289.png)

​	ALBEF 中除了梯度更新的主模型之外，还有一个**动量模型**，用于为主模型的训练生成 multi-hot 的**伪标签**（为经过softmax的概率分布，即**softmax score**，而非one hot label）。动量模型通过滑动指数平均(EMA)的方式，根据主模型进行动量更新。这样，除了Ground Truth(GT，真实标签)中的 one-hot 标签，我们就又得到了multi-hot的伪标签(pseudo-targets)，用于 ITC 和 MLM 任务的损失计算（**计算KL散度**）。具体的说，我们希望模型输出同时与GT和伪标签接近。

补充一句，对于 ITM 任务，由于其本身就是基于 GT 的二分类任务，并且通过 ITC 中计算的相似度结果进行了难负例挖掘，因此无需进行动量计算(有冲突)。

    则实际上论文设计了5个loss，2个ITC（基于GT的和基于伪标签的），2个MLM，1个ITM，ITM这个loss ground truth很清晰，所以不需要momentum的版本。



虽然 ViLT 的卖点是推理速度快，但是它的训练还是很慢。**而 ALBEF 的训练速度与推理速度**都很快，预训练阶段只用了 8 张 A100；作为对比，ViLT 的预训练用到了 64 张 V100

不管是性能还是速度，ALBEF 都非常出色，算是 2021 年多模态领域承上启下的一篇工作。



#### VLMo（统一框架，最大化利用各领域数据集分阶段训练）

链接

**意义**

**贡献1**：dual-encoder (双塔模型，如CLIP) 解决了检索问题，而fusion encoder，也叫单塔模型，解决了不同模态之间的交互问题，VLMo就把2种的好处都结合了起来，一个模型，想当双塔和单塔 (论文命名为vision-language expert, language expert, vision expert，其实就是不共享参数的FC层) 用都可以。

**贡献2**：**分阶段模型训练**的改进(stage-wise pre-training), 简单来说就是多模态的数据集不够大，那我就先预训练单独的一个模态。

双编码器模型（dual-encoder，结构如图 1 (b)）的优点是在进行检索等任务时，可以预先对数据库中的数据进行特征提取，运行效率高。缺点是模态交互部分只有一个简单的余弦相似度的计算，过于简单，在视觉推理等模态交互复杂的任务上表现较差。与之相反的，融合编码器模型（fusion-encoder，结构如图 1 (c/d)）的优点是模态交互充分，缺点是无法预先进行特征提取，效率稍差。

为了解决这种冲突，VLMo 提出了 MoME（Mixture of Multi Expert），**由不同的 “专家” 来处理不同类型（文本/图像）的输入数据**。简单来说，就是在每个 Tranformer 块中：**自注意力层权重在不同类型输入间共享**，而 **FFN 层权重则根据输入类型的不同而不同**。

VLMo的目标函数和ALBEF一样也是**ITC、ITM和MLM**。因为在NLP使用Transformer时，数据集越大训练效果越好，在做多模态时也希望如此，但是在当时还没有开源的大规模数据集。**曲线救国**：所以VLMo的作者想到可以用文本和视觉各自领域的超大规模数据集先分别对 “文本专家” 和 “视觉专家” 进行预训练(stage-wise pre-training)，然后再在多模态数据集上进行预训练。



**模型概述**

![img](https://img-blog.csdnimg.cn/direct/5883a67b05f74cf5a53e27fb97664487.png)

预训练任务的选择上，VLMo 与 ALBEF 一致，同样使用 **ITC、ITM 和 MLM** 三种，并且同样借助 ITC 为 ITM 进行**hard negtives**。在进行不同的任务时，会使用 MoME 结构中不同的 FFN 层参数进行训练：

ITC：在计算 ITC 损失时，VLMo 的模型是一种 “dual encoder” 模型，以双塔的结构分别对文本和图像进行嵌入。即**变成了CLIP**
ITM、MLM：在计算 ITM、MLM 损失时，VLMo 模型又成了一种 “fusion encoder” 模型，分别提取图像文本的特征之后，再用 FFN 层 Transformer Block 进行模态融合。
       MoME 结构最大的优势就是**灵活**。在训练时，对应不同的任务时使用不同结构计算损失函数，并更新对应参数。这样的训练有一个**缺点**是需要做多次模型前向。**在推理时，灵活性的优势得到体现。**如果要做检索类任务，可以用单独的文本/图像编码器去提取特征(把已经看过的特征存起来)，提高处理效率（即用**CLIP**）；而如果要做推理类任务，又可以通过图文编码器进行充分的模态交互（即用**Fusion model**）。巧妙地解决了前言部分提到的两种结构的冲突。

**分阶段训练方式**
首先，VLMo **先**在单独的图像数据上训练自注意力层和视觉 FFN ；
然后，在单独的文本数据上训练文本 FFN ；
最后，在多模态数据上训练自注意力层和三种 FFN 专家。
        这里特别有趣的点是在单独的文本数据上进行训练时，自注意力层是冻结的。也就是说，通过图像数据训练出的自注意力层，在文本数据上甚至连微调都不需要，就能工作得很好。如果换过来，先文本，在视觉，效果会怎样呢（不好）？是否不同模态间的注意力是可以通用的呢？

![img](https://img-blog.csdnimg.cn/direct/7376b68df98649539daecf2fd2c7e292.png)****



### Transformer Encoder-Decoder

#### BLIP(融合了ALBEF的VLMo，可以作为多模态领域通用的数据清洗模型)

**Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation**

**链接**

**意义**

BLIP是 ALBEF 原班人马做的，基本可以看做吸收了 VLMo 思想的 ALBEF。训练的 loss 和技巧都与 ALBEF一致，属于 ALBEF 的后续工作。

研究动机：

模型：最近要么用transformer encoder，要么用SimVLM**，encoder only的模型很难用在文本生成任务，encoder decoder模型没有统一的框架很难做image-text retrieval 任务。**
数据层面：CLIP,ALBEF,SIMVLM数据都是爬的，存在**noisy**，使用起来不好。如何更好利用？设计了**captioner（生成好的描述性文本）和filter**可以选择GT和cap的文本选个好的，以训练出更好的模型。

BLIP 的两个关键点都包含在标题内，一是 **bootstrapping**，是数据方面的改进，指的是用含噪声的数据训练出模型后，再用某些方法**得到更干净的数据**，用这些干净的数据**训练出更好的模型**；二是 **unified**，指的是 BLIP 作为一种 encoder-decoder 架构，不只能做 understanding 类的任务（如上一节介绍的下游任务），也能做 **generation** 类的任务，如图像字幕 image captioning。

![img](https://img-blog.csdnimg.cn/direct/2bb0b18bac6048dfa977c9a8d33e80ec.png)

**模型概述**

![img](https://img-blog.csdnimg.cn/direct/35b8faa4ac794e0aa288b5c414d22ac7.png)

BLIP 的模型结构和目标函数如上图所示，图中**相同的颜色表示相同的参数**（共享参数，思想来自VLMo）。

细分开来，BLIP 模型共包含四个网络。图中左侧是一个**标准的 ViT 模型，用于处理图像数据**。右侧三个网络都用于处理**文本数据**，但他们的细节有所不同，细节如下：

 **Text Encoder**，提取文本特征，用于与视觉特征计算 ITC 损失。Text Encoder 不与视觉特征计算交叉注意力。**文本输入是CLS token**
**Image-gounded Text Encoder**，与视觉特征计算交叉注意力，提取文本特征用于计算 ITM 损失。**文本输入是Encode token**
 **Image-gounded Text Decoder**，与视觉特征计算交叉注意力，用于进行 LM （是GPT的LOSS，而不是Bert的LOSS即MLM）语言建模训练。为了进行语言建模训练，需要 mask 掉后面的单词。因此该网络的注意力层是 Causal SA，而非 Bi SA。**文本输入是Decode token**
        BLIP模型是ALBEF模型原班人马做的，所以与 ALBEF 一样，**同样采用动量模型为 ITC 生成伪标签**；**同样使用 ITC 为 ITM 进行难负例（Hard Negative）挖掘**。

​    BLIP 的整个模型称为**MED**（Mixture of Encoder and Decoder）。虽然看起来模型很多，但实际上大部分网络是共享参数的，因此实际模型参数增加并不多。和ALBEF的区别是 **fusion的text是直接输入的，不是从文本的encoder获取的**，因为参数共享，不用劈成两份了，第一个文本编码器和第二个文本编码器基本一样。

**除了模型结构的创新之外，BLIP 的另一个贡献在数据清洗方面**，其方法流程如图 9 所示。图中 *I , T*  分别表示图像数据和文本数据；红色、绿色字体分别表示噪声较大、较小的文本；下标 *h , w , s* 分别表示人工标注数据、网络数据和模型生成数据。

BLIP 先使用含噪声的数据训练一个 MED 模型，然后将该模型的 Image-grounded Text Encoder 和 Image-grounded Text Decoder 在人工标注的 COCO 数据集上进行微调，分别作为 Filter 和 Captioner。**Captioner 为图像数据生成对应的文本，FIlter 对噪声较大的网络数据和生成数据进行过滤清洗**(**算余弦相似度**，淘汰不匹配的图文对)，得到较为可靠的训练数据。

为什么有Captioner呢？BLIP（decoder）训练出来的图像文本对**比原始的图像文本对的文本描述更贴切，质量更高，更匹配**。然后就有了Captioner去训练出质量更高的图像文本对。

**经数据清洗后，D(data)由两项(CC12M+COCO)变为三项(CC12M-F即过滤后的，CC12M-S即新生成的CC12M文本对，COCO，即人工标注的文本对)**，再根据这些可靠的训练数据，训练更好地 MED 模型，从而实现 bootstraping 训练。

 BLIP的特点：最显著的特点就是解码器输出的 caption 确实是不错，以至于很多下游任务都拿 BLIP 来生成 caption。实际上，BLIP 中 Captioner + Filter 的数据处理策略可以为任何需要图像文本对来训练的模型进行数据生成和清洗，**可以视作为多模态学习领域的一个通用的数据处理工具**。如 lambdalabs 用 BLIP 生成宝可梦图像的文本描述、LAION 用 BLIP 进行数据集清洗等。

![img](https://img-blog.csdnimg.cn/direct/f73f5a7adbec483b8f2ae622b88537d4.png)



#### CoCa（一次迭代只前向一次）

[CoCa: Contrastive Captioners are Image-Text Foundation Models](https://arxiv.org/pdf/2205.01917)

**链接**

**意义**

它也是 ALBEF 的后续工作，模型非常像。区别在于：

​		图像用了 attentional pooling（**可学习的pooling**），这在本文的实验中有效
​		**去掉了 ITM loss**，目的是加快训练，原本文本需要 forward 2-3 次，去掉 ITM loss 之后只需要 forward 一次就可以了。
​				在 ALBEF 中，ITM 需要完整的 text，而 MLM 需要掩码，所以是两次输入。
​				在 BLIP 中，ITC 一次，ITM 因为在文本模型中插入了新的模块，所以得单独做forward。而 LM 因为用了既多了新的模块又得用 causal self-attention 所以又得单独做一次。
​				在 CoCa 中，为了完成captioning loss(LM)和ITC loss，只需要做一次forward即可。GPT 中把 cls-token 放在最后面就可以得到全局表征来做 ITC loss 了。

**注意这里长度cls token是放在句子的后面的，这样mask的时候这个token就能看到全部句子了**。这里与ALBEF不同

![在这里插入图片描述](https://img-blog.csdnimg.cn/19899d275f064d3695ba8140ff868278.png#pic_center)

**模型概述**

CoCa（Contrastive Captioning），看方法名称就能猜测出它是**使用对比损失(ITC loss)和文本生成损失(LM loss)进行训练**，实际上也的确如此，CoCa 的模型框架和目标函数如图 所示。

从上图可以看出来，CoCa使SimVLM和ALBEF一个非常直接的后续工作。从结构上看来，CoCa 与 ALBEF 十分接近，都是左侧网络处理图像，右侧网络劈开，前半段处理文本，后半段进行多模态交互。与 ALBEF 最大的不同在于：

CoCa 左侧处理文本和进行多模态交互的网络是一个**文本解码器**（Text Decoder）而非文本编码器。目标函数为 ITC 对比损失和文本解码器的语言建模损失 。使用文本解码器，模型能够处理生成式多模态任务（如 image captioning）。

并且，CoCa 在图像编码器的最后使用**可学习**的attention pooling进行降采样。

另外，CoCa 没有使用 ITM 损失，**减少了模型参数每次迭代所需前向传播的次数**，大大降低了训练时间。输入从刚开始就是经过了causal SA,即mask住后面的部分。这样经Unimodal Text Decoder处理后的特征既可以直接算ITC LOSS,也可以直接算LM LOSS。

![image-20240426090419140](.\image-20240426090419140.png)



#### BEiTv3(更大一统的框架)

**链接**

**意义**

BEITv3 的关键词就是**大一统（big convergence）**，输入形式大一统，目标函数大一统，模型大一统。**统一的multi-way transformer (mixture of experts ) 架构和单个masked modeling loss，将任意模态看做是同一个模态来建****

BEITv3 将图像也视作一种语言**（Imglish），与文本输入（English），图像文本对输入（parallel sentence）一起，实现了输入形式的大一统。在输入形式统一之后，也不需要 ITC、ITM、MLM、WPA 等其他目标函数，而是可以使用**统一的 mask language modeling (MLM)来驱动训练**。模型层面上，自从 ViT 在视觉领域取得成功之后，Transformer 架构已有一统多模态模型的趋势。虽然在纯视觉领域，CNN 与 Transformer 谁更适合至今尚无定论，但如果要实现多模态模型大一统，Transformer 无疑更加适合。BEITv3 使用本组之前工作 VLMo 中提出的 **MoME（本文中称为 Multi-way Transformer），对不同模态使用不同的专家 FFN，实现统一**。

BEITv3 的性能对比实验直接放在了论文的第一页。同样采用了 CoCa 中的多边形图，展现出了 BEITv3 相较于现有工作的巨大提升。除了多模态的任务之外，BEITv3 还可以做语言、视觉单模态的任务，图中展示了语义分割、图像分类、目标检测等视觉任务，但没有展示语言单模态的性能。可以看到，BEITv3 相较于 CoCa，也实现了全面的领先。并且，BEITv3 使用的训练数据量是远小于 CoCa 的，并且全都是公开数据集（CoCa 使用了谷歌自家的 JFT-3B）。BEITv3 的大一统框架取得了巨大的领先。

![img](https://img-blog.csdnimg.cn/direct/9efb1917cdd840faa84d12d64ffe8e53.png)

**模型概述**

![img](https://img-blog.csdnimg.cn/direct/7d0db575a38b409d9699c1ba7affd3f1.png)

**模型结构就是之前VLMo 中的 MoME，自注意力层权重共享，根据不同的输入来选择不同的 FFN 专家。**与 VLMo 不同之处在于训练的目标函数，是大一统的 masked data modeling，即遮住部分数据，要求模型还原出被遮住的数据即完形填空，BERT。

​		multiway transformer：VLMo 的模型 MoME。该网络的 transformer block 中的自注意力层是共享的，而 FFN 层（模态专家）则有三种，分别针对文本、图像、图文，当接收不同类型的输入数据时，数据会通过对应的 FFN 层进行计算。
​		masked data modeling：**BEiTv3 在单模态、多模态数据上，通过一个统一的掩码数据建模任务进行训练。**在训练时，随机掩码掉一定比例的 token，然后训练模型恢复出被掩码的 token。统一的掩码数据建模不仅能够学习数据的表示，还能学习对不同模态数据进行对齐。BEiTv3 中，使用 SentencePiece 对序列数据进行 tokenize，使用 BEiTv2 中使用 VQ-KD 训练得到的 tokenizer 对图像数据进行 tokenize（得到离散的视觉 token），作为重构目标。



大一统的 BEITv3 具有极高的灵活性，可以处理视觉、文本各自单模态以及视觉文本多模态的各种任务。BEITv3 用于各种下游任务的示意图如图 14 所示。(a)、(b) 中，仅使用视觉编码器或文本编码器，BEITv3 可以处理视觉、文本各自领域的单模态任务；© 中，使用视觉编码器和文本编码器提取特征之后，再经过多模态交互，相当于 Fusion Encoder 多模态模型，适合于处理推理类多模态任务；(d) 中，分别使用视觉编码器和文本编码器提取特征之后计算相似度，相当于 Dual Encoder 多模态模型（CLIP），适合于处理检索类多模态任务；(e) 中，将输入文本 mask 掉，可用于 image captioning 这种生成类多模态任务（把下一个词MASK掉）。就像搭积木一样，大一统的 BEITv3 模型可处理视觉、文本领域各类任务。

![在这里插入图片描述](https://img-blog.csdnimg.cn/d891bc8c684b4c69846e6c4a830a972a.png#pic_center)

## 多模型总结

![在这里插入图片描述](https://img-blog.csdnimg.cn/4d9fd1badfdf4848b9c349920197a64c.png#pic_center)

## 对比学习

- **原理** ：对比学习是**无监督学习**的一种，着重于学习同类实例之间的共同特征，区分非同类实例之间的不同之处。

> 举个例子，从imagenet中抽出猫、猫、狗、飞机四张图，那么猫和猫的图片肯定是相似的，和狗不相似。但是和飞机比起来，猫和狗是相似的。所以**对比学习就是对比着差异去学习，模型并不需要真的知道图片中代表的是什么，而只需要知道哪些图片是类似的，哪些图片是不一样的就可以了**。

- **训练目的**：对比学习，希望相似数据（图片）最终学到的特征是相似的，在特征空间（`embedding space` ）中，特征向量尽量靠近；反之还希望不同的数据学到的特征向量，尽量远离。

- `**pretext task`（**代理任务**）**：对比学习是不需要标签的（比如不需要知道图片是哪一类），但模型还是需要知道哪些图片是类似的，哪些是不相似的，才能训练。这就需要通过通过设计一些巧妙的代理任务，人为指定一些任务来实现。

- 应用最广的代理任务：[instance discrimination](https://link.zhihu.com/?target=https%3A//paperswithcode.com/paper/unsupervised-feature-learning-via-non-1) 。（**instdisc**）

- - 简单说就是，从一堆图片中调出任意一张图片𝑥𝑖，将其做一次转换（`transformation` ，比如随机裁剪等数据增广），得到新的图片𝑥𝑖1、𝑥𝑖2。那么**样本𝑥𝑖1叫做基准点（锚点），𝑥𝑖2被认为是正样本**（两者都是从𝑥𝑖变化得到的，虽然看起来有差异，但语义信息不应该发生变化），数据集中其它所有图片都是负样本。
  - 有了正负样本的划分，就可以将数据都输入编码器进行编码提取特征了。因为所有的正负样本都是基于锚点来说的，所以𝑥𝑖1会单独使用一个编码器𝐸11，𝑥𝑖2和其它所有负样本使用另外的编码器（可以是同一个编码器，也可以也可以使用不同的编码器。但是不同的编码器之间必须相似，这样编码的特征才有一致性，才有比较的意义）。
  - **对比学习就是要让正样本的编码特征和锚点的编码特征尽可能靠近（相似），让负样本的特征和锚点特征尽量远离**。
  - `instance discrimination`直译过来就是**个体判别**，在这个任务中，只有经过这张图片转换的样本才是正样本，其它图片都是负样本，所以每张图都自成一类。对于ImageNet来说，就不是1000类，而是**128万个类别。**



- 目标函数：确定了代理任务，知道如何定义正负样本之后，就需要用一个目标函数，来告诉模型该如何学习，比如常见的对比学习目标函数`**NCE loss**`等。
- 特性：对比学习最大的特性，是这种方法非常的灵活，可以设置各种不同的代理任务。只要找到一种方式去定义正负样本，剩下的都是一些比较标准化的流程。



如果把 近几年对比学习在视觉领域有代表性的工作做一下总结，那么对比学习的发展历程大概可以分为四个阶段：

1. 百花齐放
   这个阶段代表性工作有InstDisc（instance discrimination，）、CPC、CMC等。在这个阶段中，方法、模型、目标函数、代理任务都还没有统一，所以说是一个百花齐放的时代

- InstDisc：一个编码器+memory bank，特征一致性比较差
- Inva Spread：只使用一个编码器进行端到端训练，但是字典太小，负样本不够
- CPC：一个编码器+一个自回归模型
- CMC：有两个甚至多个编码器

2.CV双雄
代表作有MoCo v1、SimCLR v1、MoCo v2、SimCLR v2；CPC、CMC的延伸工作、SwAV等。这个阶段发展非常迅速，有的工作间隔甚至不到一个月，ImageNet上的成绩基本上每个月都在被刷新。

3.不用负样本
BYOL及其改进工作、SimSiam（CNN在对比学习中的总结性工作）

4.transformer
MoCo v3、DINO。这个阶段，无论是对比学习还是最新的掩码学习，都是用Vision Transformer做的。

<img src=".\image-20240422112836122.png" alt="image-20240422112836122" style="zoom:70%;" />

| 模型        | 创新点                                                       | 优势                           | 局限性                     |
| ----------- | ------------------------------------------------------------ | ------------------------------ | -------------------------- |
| **阶段一**  | 百花齐放                                                     |                                |                            |
| Inst Disc   | 提出了个体判别的任务，对比学习loss，使用一个 memory bank的外部数据结构去存储负样本来做对比学习 |                                | 特征一致性差               |
| Inva Spread | 只使用一个编码器而不需要额外的数据结构去存储负样本           | 可以进行端到端的对比学习       | 字典太小，对比学习效果不好 |
| CPC v1      | 提出了**infoNCE Loss**，以及预测型的代理任务，其输入可以是图像、音频、视频、文字或加强学习 | 是一个非常全能的结构           |                            |
| CMC         | 把两个视角的任务扩展到了多个视角，为以后的多视角多模态对比学习打下了基础 。 |                                |                            |
| 阶段二      |                                                              |                                |                            |
| MoCov1      | Inst Disc的延伸工作，使用队列结构代替 memory bank来存储负样本，使用动量更新编码器代替动量更新特征；把之前对比学习方法都归纳成字典查询问题；第一个让无监督预训练媲美有监督预训练的方法 | 字典大且特征一致性好，训练便宜 |                            |
| SimCLR v1   | Inva Spread延伸工作。batch-size加大到8192，引入projection head ，使用更优的数据增强（随机裁剪和随机色彩变换） | 端到端训练                     |                            |
| CPC v2      | 引入SimCLR v1的几个技术，ImageNet精度直接从40多提到70多      |                                |                            |
| MoCov2      | 相比 MoCov1，引入了projection head；使用更多数据增强、cosi调度器和更长的训练epoch |                                |                            |
| SimCLR v2   | 受noisy student影响，使用伪标签进行半监督训练。相比SimCLRv1使用了更大的backbone，动量编码器和两层的 projection head |                                |                            |
| SwAV        | 结合聚类和对比学习，使得对比学习不再需要负样本（跟聚类中心对比）；使用multi crop技术 |                                |                            |
| **阶段三**  | 不用负样本                                                   |                                |                            |
| BYOL        | 处理负样本实在是太过麻烦，所以完全舍弃负样本，自己预测自己（mse loss），也可以训练 |                                |                            |
| SimSiam     | 化繁为简，使用孪生网络，不需要动量编码器、负样本、大的batch-size就可以训练。不过一个分支必须是stop gradient，这样交替 优化，类似K-means |                                |                            |
| 阶段四      | 引入Vision Transformer                                       |                                |                            |
| MoCov3      | 冻住ViT结结构中的patch projection layer就可以稳定训练        |                                |                            |
| DINO        | teacher网络的输出先做centering归一化也可以稳定训练           |                                |                            |

总体来说，现在的对比学习都使用：

​	动量编码器（MOCO）

​	目标函数（infoNCE）

​	模型：用一个编码器+projection head（simCLR）

​	数据增强



## GNN

## AI in chemistry

开源的CPM，可用的数据集，重要论文：https://github.com/junxia97/awesome-pretrain-on-molecules

分子指纹综述：Concepts and applications of chemical ﬁngerprint for hit and lead screeni

端到端（End-to-End）在人工智能领域通常指的是一种系统或方法，**能够直接将原始输入映射到最终输出，而无需中间的人工干预或手动设计的特征提取过程**。换句话说，端到端的方法旨在让机器自己完成整个任务，从原始输入数据到最终的输出结果，而不需要人为的干预或特征工程。

### 化学预训练模型综述

**A Systematic Survey of Chemical Pre-trained Models**

链接：https://arxiv.org/abs/2210.16484

简介：纯利用有标签数据从头训练模型很贵，且这样的模型在分布外泛化能力差。解决方案受NLP领域启发，人们在化学预训练模型 （CPM）方面做出了巨大努力，即利用大规模的未标注分子数据库对 DNN 进行预训练学习通用能力，然 后针对特定的下游任务进行微调。

常用的分子数据结构：

​		1.分子指纹（FP）：分子指纹是较常用的通过遍历分子中所有原子间键连关系来进行分子表示的方法。分子指纹会通过hash以用相同长度的向量来表征不同长度的分子的要求，保证向量长度的统一，因此分子指纹往往是高维的0/1向量。

​		2.序列（如分子的smiles表示）

​		3.二维图：分子可以自然地表示为二维图，原子为节点，键为边。

​		4.三维图：三维几何图形表示分子中原子在三维空间 中的空间排列，其中每个原子都与其类型和坐标以及一 些可选的几何属性（如速度）相关联。使用三维几何的优势在于，构象信息对许多分子特性，尤其是量子特性 至关重要。此外，利用三维几何图形还可以直接利用立体化学信息，如手性。

CPM预训练策略：

![image-20240617141405437](.\image-20240617141405437.png)

​		1.AE（自编码器）：使用自编码器重构分子（图 3a）是学习具有表现力的分 子表征的天然自监督目标。分子重构中的预测结构是给定分子的（部分）结构，如原子或化学键子集 的属性。一个典型的例子是 SMILES 变换器。该研究利用**基于变换器的编码器-解码器网络，通过重建 SMILES 字符串所代表的分子来学习表征。**虽然自编码器可以学习 有意义的分子表征，**但它们只关注单个分子，无法捕捉 分子间的关系**，这限制了它们在一些下游任务中的表现

​		2.AM（自回归建模法，同GPT，预测下一个token）：自回归建模法（AM）将分子内含物因子化为一系列子 成分，然后**以序列中的前一个子成分为条件，逐一预测 这些子成分**。（如预测 SMILES 字符串中的下一 个字符串。或给定一个节点和边都 被随机屏蔽的图，GPT-GNN 每次生成一个屏蔽节点及其边，并最大化每次迭代生成的节点和边的可能性。然后，迭代生成节点和边，直到生成所有屏蔽节点。）与其他策略相比，**AM 使 CPM 在生成分子方面表现得更好**，其训练过程与分子生成过程类 似 [Bagal 等人，2021]。不过，AM 的**计算成本较高， 而且需要事先对原子或化学键进行排序**，这对于分子来 说可能并不合适，因为原子或化学键并不存在固有的 排序。

​		3.屏蔽组件建模 (MCM)（同BERT,MLM）：屏蔽成分建模 （MCM，图 3c）将 MLM 的理念推广到分子中。具体来 说，**MCM 屏蔽了分子的某些成分（如原子、键和片 段），然后根据剩余成分训练模型来预测它们**。**MCM 尤其适用于注释丰富的分子**。例如，屏蔽原子 属性可以GNNs 可以**学习简单的化学规则（如化合价）以及其他 潜在的复杂化学描述符**（如官能团的电子效应或立体效 应）。此外，与上述 AM 策略相比，**MCM 可根据周围 环境预测被遮蔽的成分**，而 AM 则仅仅依赖于按预定顺 序预提成分。因此，MCM可以**捕捉到更完整的化学语义。**然而，由于 MCM 在 BERT [Devlin 等人，2019 年] 之后的预训练过程中通常 会**掩盖每个分子的固定部分**，因此它不能对每个分子中的所有成分进行训练，从而降低了样品的 利用效率

​		45略

​		6.更换组件检测 (RCD) ：替换成分检测（RCD，图 3f）建议识别输入分子中随机 替换的成分。例如，MPG [Li 等人，2021] **将每个分子 分成两部分，通过组合两个分子的部分来改变其结构， 并训练编码器来检测组合后的部分是否属于同一分子**。

​		7.去噪 (DN）：如（Uni-mol）将噪声转化为三维分子几何的原子坐标，并预先训练编 码器预测噪声。

​		8.多模态预训练。除了第 2 节中提到的描述符之外，还可 以使用其他模式（包括图像和生化文本）来描述分子。 最近的一些研究对分子进行了多模态预训练。

应用：

​		1.分子性质预测 (MPP)

​		2.分子生成（MG）：分子生成是计算机辅助药物设计领域的一项长期挑战， 机器学习方法，尤其是生成模型，缩小了搜索空间，提 高了计算效率，使深入研究看似无限的药物化学空间成 为可能[Du 等人，2022b]。**事实证明，采用自回归预训练 方法的 CPM，如 MolGPT [Bagal 等人，2021]，有助于生 成有效、独特和新颖的分子结构。**多模态分子预训练技 术的出现[Edwards 等人，2022；Zeng 等人，2022b]通过 **将描述性文本转化为分子结构**，进一步拓展了分子生成 的可能性。CPM 展示其能力的另一个关键领域是**生成三 维分子构象**，特别是用于预测蛋白质配体结合位置。基 于分子动力学或马尔科夫链蒙特卡洛的常规方法往往受 到 计 算 能 力 的 限 制 ， 尤 其 是 对 较 大 的 分 子 而 言 [Hawkins，2017]，而基于三维几何的CPM[Zhu等人， 2022a；Zhou等人，2023]则不同，它们能在预训练过程 中捕捉二维分子与三维构象之间的一些固有关系，因此 在构象生成任务中表现出显著的优越性

​		3.药物-目标相互作用 (DTI)

​		4.药物之间的相互作用（DDI）

5 结论和未来展望  

​		1 改进编码器架构和预训练目标 虽然在分析神经架构的学习能力（如 GNN 的 WL 测 试）方面取得了长足进步，但这些分析在确定高度结构 化分子的最佳设计方面缺乏特异性。此外，迫切需要探索如何将消息传递技术无 缝集成到转换器中，作为统一的编码器，以适应大规模分子图的预训练。此外，由于 在第 3 节中讨论过，预培训目标仍有很大的改进余地， MCM 中子组件的高效屏蔽策略就是一个很好的例子。 

​		2 建立可靠和现实的基准 尽管对 CPM 进行了大量研究，但由于采用的评估设置 （如随机种子和数据集分割）不一致，其实验结果有时 并不可靠。例如，在包含多个昂贵的分子性质预测数据 集的 MoleculeNet [Wu 等人，2018]上，可能由于这些分 子数据集的规模相对较小，同一个模型的性能在不同的 随机种子下会有很大差异。

​		3 扩大化学预训练模型的影响 CPMs研究的最终目标是开发出多功能分子编码器，可应 用于与分子相关的大量下游任务。然而，与 NLP 界 PLM 的进展相比，CPM 的方法论进步与实际应用之间 仍存在巨大差距。**一方面，CPM 生成的表征尚未被广泛 用于取代化学领域的传统分子描述符，预训练模型也尚 未成为社区的标准工具。另一方面，对于这些模型如何 有利于单个分子之外的更广泛的下游任务（如化学反应 预测、虚拟筛选中的分子相似性搜索、逆合成、化学空 间探索等）的探索也很有限**。

​		4 建立理论基础 尽管 CPM 在各种下游任务中表现出令人印象深刻的性 能，但人们对这些模型的严谨理论理解却十分有限。



思考：1.没有提及GAN  GAN是否可行？比如面对生成合成反应链的任务，训练一个可以判断是否可行的D与生成反应链的G。（可能对分布外的泛化能力不强）

​		2.没有提及酶、催化剂、反应条件？（需要看看数据集）

​		3.一种提升模型分布外泛化能力表现的方案：无标签数据预训练模型+有标签数据微调  

​		4.生成式可能更适用于AM模型

​		5.难就难在泛化能力  这种泛化能力几乎等同于对真实世界的仿真？现实世界很多化学合成反应是大量试出来的  我们并不清楚内在反应机理（很多反应机理是假设？）  也就是说这其实也是黑箱  而DL也是黑箱 黑箱+黑箱=？ 可能需要一个更大的仿真平台 



### 深度学习用于化学反应与逆合成预测综述

**A Unified View of Deep Learning for Reaction and Retrosynthesis Prediction: Current Status and Future Challenges**

简介：化学家通常致力于设计合成路线，通过一系列化学反应获得目标分子。一种常见的策略是**将目标分子分解成更容易合成的简单前体结构**，这一过程被称为逆合成分析。逆合成规划涉及两个子任务：**多步骤逆合成规划和单步骤逆合成预测**。在本研究中，我们将重点讨 论后者，因为多步规划通常是作为搜索问题来处理的， 这与**反应预测和单步逆合成预测所提出的条件结构预测问题**有着本质区别。在本研究的其余部分，我们将单步 逆合成预测简称为逆合成预测。**反应预测是有机合成分 析的另一项关键任务。**一个强大的反应预测模型可以帮助我们深入了解生化反应的内在机理，并生成虚拟反应 以扩展用于逆合成规划的数据库。总之，反应预测和逆合成预测是相互关联、相互促进的。

反应预测和逆合成预测是双重的，它们也分别被称为前向反应预测和后向反应预测。 在深度学习中，这两项任务都被表述为**条件生成任务。**

![image-20240617165520216](.\image-20240617165520216.png)

**定义**：

**分子表示**：SMILES 字符串和分子图

​		1.对于 SMILES 格式，分子结构 M 被描述为字符序列 mi，使得 M ：= m1m2...mL，其中 L 表示字符串的总长度。序列表示二维分子结构的生成树，每个字符 mi 表示一个结构元素，例如原子元素、化学键、分支符号

​		2.分子图:分子也可以抽象为无向图 G = {V， E}，其中 V = {v1， ..， vn} 表示 n 个原子的集合，E = {e1， ..， em} 表示 m 个边的集合。每个节点都与一个特征向量 hi ∈ R d 相关联，其中包含原子信息，例如芳香性和电荷。然后，我们有一个包含所有原子信息的特征矩阵 H ∈ R n×d。邻接矩阵 A ∈ R n×n×c 描述了 M 的拓扑结构，其中 Aijk 表示原子 i 和原子 j 之间是否存在 k 型化学键。多个分子可以很容易地用上述两种格式表示。

对于 SMILES 格式，可以通过句号“.”将多个 SMILES 字符串连接成一个 SMILES 序列。对于分子图，一组分子被视为一个不相连的图，每个分子都是一个独立的连接组件。

**反应预测**：给定一组反应物预测所有可能的产物集合

**逆合成预测**：给定产物，推断可能的反应物集合。**在实际应用中产物种类为1，这是因为在公共基准数据集中只记录了主要产物。不幸的是，这个问题使得逆合成预测比反应预测更加困难，需要重新合成来连接新出现的原子**（如上图逆合成预测中的-BR,-Cl），**从而导致更大的组合搜 索空间。**

**反应中心和反应模板**。在反应预处理中，反应中心 C 是原子对 C = 的子集。 {(vi , vj )} ⊆ V × V，当化学反应发生时，这些键的 类型会发生改变。在逆合成预测中，反应中心 C 被定 义为现有键 C = {ei } ⊆ E 的子集，可以通过修改这 些键来获得更简单的结构。反应模板库 T 是一组从大型 化学反应数据库中提取的反应子图规则。反应模板 T∈ T是从相应的化学反应数据库中提取的子图式。 具体来说，

**原子映射**：反应预测和逆合成预测都遵循原子映射原则。 该原则规定，**反应物/生成物中的每个原子在生成物/反 应物中都有一个对应原子**。这种基本的一对一映射关系 从物理上限制了反应空间，并决定了化学反应主要是键 的断裂和键的形成。

![image-20240617170217147](.\image-20240617170217147.png)

**使用的深度学习方法**

![image-20240617170423711](.\image-20240617170423711.png)

​	**1.基于反应模板的方法**（可能类似于搜索+匹配？搜索可能的反应路线）：基于模板（TB）的方法主要是利用反应模板库来推断可能的反应中心。模板法是模仿人类专家进行化学推理的 方法。**TB 方法的推理过程旨在选出最佳模板，最关键的部分是将分子与模板进行匹配。*优点**(1) TB 方法是可靠的，因为它们使用的是提取的人 类知识，而人类知识总能为预测提供很好的解释。(2) 训练和输入过程相对简单	**缺点**(1) TB 方法的性能高度依赖于模板数据库的规模。 因此，模板数据库必须经常更新，这显然非常昂贵。(2) **TB 方法对域外未见反应的普适性较差**。(3) 模板是提取局部子图规则，而忽略全局信息。因此，TB 方法无法捕捉到全局信息的相互作用。因此， TB 方法无法捕捉到全局信息的交互作用，很容易产生错 误信息。

​	**2.基于序列和基于图形的自回归模型**：

​		基于序列的自回归（SAR）模型在前向和后向预测中被广泛采用。SAR认为这两个问题都是神经机器翻译问题。对于反应预测，输 入是反应物的 SMILES 字符串 M R := m mRR ...mR ，长 度为 L1 ，输出是长度为 L 的 SMILES字符串 M P := mP mP ...m P。输入源和输出目标是翻转的，以便进行回溯预测。每个生成步骤对标记空间进行贪婪搜索，选出最佳标记。要 生成前 k 个候选者，我们只需对 贪 婪 结 果 进 行 波 束 搜 索 。

​		 基 于 图 的 自 回 归 （GAR）模型具有类似的学习机制，但生成序列定义不 同。GAR 模型首先定义一个动作空间 π，其中包括多个 编辑动作，如原子添加/删除、键添加/删除和终止。这 些动作的序列会将反应物/生成物转化为相应的产品/反应物。每个步骤 i 都会选择最优行动 π ∗ ，并将最优行动 π 应用 于每个步骤。π将反应物 G R,i−1 和生成物 G P,i−1 分别转化为下一状态 G R,i 和 G P,i 。当预测所有生成步骤都结束时，最佳操作是 " 终止"。![image-20240617172636107](.\image-20240617172636107.png)

 **基于图形的两阶段模型** ![image-20240617173559191](.\image-20240617173559191.png)

​	对于基于图的两阶段**反应预测**模型，他们将反应预测分为两个阶段，即**反应中心识别阶段**和**候选物排序阶段**。对于反应中心识别，其目的是**选择具有高反应性得分的原子对**，第二阶段 是学习如何对 G ˆ P 中的**可生成产物进行排序**（可能性排序）

​	对于基于图的两阶段逆合成预测模型，他们 将逆合成分为两个阶段，即**编辑预测阶段和合成阶段**。 在编辑预测阶段，他们选择一些预测反应性得分较高的 现有边缘进行分解。在获得预测的编辑中心 Cˆ 之后，我们通过打破预测的编 辑中心来分解目标分子，即产生一组合成子。![image-20240617174927816](.\image-20240617174927816.png)

**基于图形的非自回归模型**

​	只有反应预测在 NERF实现了基于 图的非自回归模型，目前还没有用于逆合成的非自回归 模型。



**现有局限**

​	**副产物**

​		1.缺少副产物：公开基准数据集中缺少副产品，导致监督信号不完整。

​		2.同1。逆合成直接使用 USPTO 中的反应数据，因此 所有单步分析只包含一个单一结果（因为没有副产物的数据）。结果侧的侧产物缺 失可能不会影响逆合成预处理的一般过程。不过，侧产 物仍能提供有关离去基团的重要信息。

​	**数据集**

​		1.USPTO-479K 数据集有两个主要问题。首先，**反应类型非常不平衡**。线性拓扑结构的反应是主要的反应 类型，而环状拓扑结构的反应很少。

​		2.其次，在实际应用中，**同一组反应物 在不同的物理条件下会产生不同的产物**，这就是反应预 测中的多模态。多模态可以为条件生成模式提供丰富的 信 息 ， 从 而 生 成 有 效 和 多 样 的 候 选 产 物 。 **然 而 ， UPSTO-MIT 中的大多数反应都是一对一映射**，这意味 着同一组反应物只能生成唯一的主要产物。

​		3.逆向反应的数据库规模太小。USPTO-50K 的规模不够大，只包含 50K 个逆反应。

![image-20240617175830904](.\image-20240617175830904.png)

**思考：**

1.数据集只是对真实世界的抽样，具体到化学反应这一类型数据集上，现有数据集样本少，且没那么具有代表性。

2.可以考虑分子的三维表示。与二维 分子图相比，三维欧几里得几何中的相对成对距离可能 会有很大不同。例如，原子 v1 和原子 v2 在非欧几里得 分子图中可能相距甚远，而在三维欧几里得空间中可能 相距很近。这对反应中心排序特别有用。但是这很贵，往往也很慢。而有些反应按现在的观点来看对空间构想有要求（如两个官能团间隔XXnm）



### 大型语言模型在化学中能做什么？综述

**简介：**实验结果表明，**在需要深入理解分子 SMILES 字符串的生成任务（如反应预测、名称 预测和逆合成）中，LLM 的表现竞争力较弱。**在产率预测和试剂选择等分类或排序形式的 任务中，LLMs 的表现具有竞争力。在涉及文本提示的任务（如性质预测和基于文本的分子 设计）或可解释的任务（如分子标题）中，LLMs 具有选择性竞争力。

**代码**：https://github.com/ChemFoundationModels/ChemLLMBench

**结果：**在**需要精确理解分子 SMILES 表征的任务（如名称预测、反应预测和逆合成）中，GPT  模型表现出较低的竞争力**； • GPT 模型在与文本相关的解释任务（如分子标题）中表现出强大的定性能力（图 14 中由 化学家进行的评估）和定量能力； 11 • 对于可以转换为分类任务或排序任务的化学问题，如性质预测和产量预测，GPT 模型可以 获得与使用经典 ML 模型作为分类器的基线相比具有竞争力的性能，甚至更好，如表 2 所示。

​		**性能不具竞争力（NC）的任务**。在反应预测和逆合成等任务中，GPT 模型的性能不如由 大量训练数据训练出来的现有 ML 基线，**部分原因是对分子 SMILES 字符串的理解能力 有限**。在反应预测和逆合成中，GPT 模型的输入和输出中都存在 SMILES 字符串。如果 **不能深入理解 SMILES 字符串所代表的，如果没有反应物和生成物以及将反应物转化为生成物的反应过程** ，GPT 模型将很难生成准确的反应。GPT 模型在名称预测任务中的表现也很差（见表 4） 。这进一步验证了一个观点，**即 GPT 模型在理解 SMILES、IUPAC 名称和分子式等格式 的长字符串并在它们之间进行正确转换方面存在困难。**

​		**具有竞争（C）性能的任务**。当化学任务被表述为**分类**（例如，将产 量预测格式化为 "高 "或 "低 "分类，而不是回归）或**排序**（如试剂选择）形式时，GPT 模 型可以取得令人满意的结果。这是可以理解的，**因为做出选择本身就比生成产物、反应 物或名称简单。**当要求 GPT 模型从提供的候选反应物、溶剂或配体中进行选择时，其准 确率可达 40% 至 50%。虽然 GPT-4 在产率预测方面的表现不如基线模型 UAGNN，**但 当给定更多的演示示例时，GPT-4 的表现会有所改善，**值得注意的是， UAGNN 模型是在针对这些特定反应的数千个示例中训练出来的。最后，虽然 GPT 模型 在已评估的高通量实验（HTE）数据集（特别是 Buchwald-Hartwig 数据集[1] 和 SuzukiMiyaura 数据集[50]）上表现出良好的产量预测性能，但在更具挑战性的数据集（如 USPTO-50k 数据集[53]）上，它们的表现与其他 ML 基线一样糟糕。这一观察结果表明， **GPT 模型在具有挑战性的化学数据集上的表现是未来研究和改进的潜在领域**。

​		**具有选择性竞争（SC）性能的任务。**GPT 模型在两类任务上具有选择性竞争力。 – 在某些数据集（HIV、ClinTox）的**属性预测任务**中，GPT 模型的表现明显优于基线 模型，F1 分数和准确率接近 1，如表 6 和表 7 所示。**这可能是因为要预测的属性标 签包含在提示中，而 GPT 模型的任务只是回答是或否**。例如，提示中包括抑制 HIV  复制或药物因毒性原因未能通过临床试验，我们观察到，从**提示中移除属性标签后 ，GPT 模型的性能显著下降**（参见附录 B 部分）。相比之下，采用机器学习模型的 基线在其输入中不包含这些标签的语义。这些模型的输入只包括图形式的分子表征 ，而不包括标签。 – 对于与文本相关的任务，如基于文本的分子设计和分子字幕，GPT 模型因其语言生 成 能 力 而 表 现 出 强 劲 的 性 能 。 在基 于 文 本 的 分 子 设 计任 务 中 ， 使 用 BLEU 和 Levenshtein 等 NLP 指标进行评估时，GPT 模型的表现优于基线模型。但是，如表 14 和表 15 所示，当涉及精确匹配时，准确率低于 20%。这表明，GPT 模型设计的分子 可能与地面实况不完全相同。特别是在分子设计/生成方面，精确匹配是一个重要指 标。**与自然语言生成不同，自然语言生成允许与输入有一些偏差，而分子设计则要 求精确的准确性和化学有效性。然而，与基本事实不完全一致并不会自动导致结果无 效。如果 GPT 模型生成的分子符合输入文本中列出的要求，并且大多数（超过 89% ）分子的化学成分是有效的（见表 14），那么它们仍有可能被证明是有益的，并有 可能成为基本事实的可行替代品。**不过，评估这些生成分子的真正用途（如评估其 在实际应用中的新颖性）可能是一项耗时的工作。

​		**在所有任务中，ICL(in context learning) 提示的性能都优于零镜头(zero-shot)提示。** • 在大多数任务中（如表 4、6、7、11、13、14 和 15 所示），使用脚手架相似性来检索与 问题最相似的示例作为 ICL 示例，比随机抽样取得了更好的效果。 • 在大多数任务中（表 4、6、7、10、11、14、15），使用较大的 k（更多的 ICL 示例）通 常比使用较小的 k（较少的 ICL 示例）性能更好。 这些观察结果表明，ICL 示例的质量和数量对 ICL 提示的性能起着重要作用[23, 36]。这可 能启示我们，有必要设计更多针对化学的 ICL 方法，以建立高质量的 ICL 示例，从而进一 步提高 ICL 提示性能。



​		**局限性：**LLMs 的**一个重要局限是缺乏对 SMILES 字符串中分子表示法的理解**，这在很多情况下会 导致不准确或不一致的结果，如 A 节中不同分子命名方式的翻译所示。SMILES（简化分子 输入行输入系统）[60, 61]是一种广泛使用的化学结构文本表示法。例如，乙醇（一种简单 的酒精）的 SMILES 字符串为 "CCO"。该字符串表示一个分子，其中两个碳原子（C）通 过单键连接，一个氧原子（O）与第二个碳原子连接。SMILES 字符串可以与其他自然语言 文本一起作为 LLM 的输入和输出。**然而，有几个问题使得 LLM 难以准确理解和解释 SMILES 字符串**：

​		1）SMILES 字符串中没有明确表示氢原子，因为氢原子可以根据标准成 键规则推断出来。**LLM 经常难以推断出这些隐含的氢原子**，甚至可能在计算分子中的原子 数等简单任务中失败[27, 6]。

​		2.**一个给定的分子可能有多种有效的 SMILES 表示法**，如果 处理不当或标准化不当，可能会导致歧义。因此，LLM 可能无法一致地识别和比较由不同 SMILES 字符串表示的分子结构。

​		3.LLM 对 SMILES 字符串没有固有的理解，将其视为字 符或子词序列。在处理较长的 SMILES 字符串时，LLM 依赖于字节对编码标记化技术，**这 种技术会将字符串分解成较小的片段或子词**，而这些片段或子词并不代表 SMILES 字符串 所代表的分子结构和分子特性。由于化学信息学中的许多任务都依赖于 SMILES 字符串对 分子的准确表述，因此 GPT 模型在将结构转换为 SMILES 字符串（反之亦然）方面的非竞 争性表现会影响下游任务，如逆向合成、反应和名称预测。**因此，有必要加强 LLM 处理分 子结构及其特定属性的能力，或将其与 RDKit [35] 等现有工具进行耦合。**



思路：

​		虽然在大多数任务中，LLMs 的表现都不如基线，但值得注意的是，LLMs 只利 用了少量示例来解决化学问题，而基线则是在大量特定任务数据集上训练出来的，因此受到了限制，**是不是可以将LLM在化学数据集上fine-tune，以增强LLM对SMILES、对任务的理解**

​		采用先进的提示技术，如**思维链（CoT）[59]、分解提示（Decomposed Prompting）**[31]，有 可能提高 LLMs 执行复杂推理的能力。思维链作为提示能够进一步提高性能，在大模型（如5400亿参数的PaLM）上表现出最大增益，但是当模型参数规模小于1000亿时，思维链并不能产生性能增益，甚至可能会导致性能下降。因此，思维链引发的可能是模型规模上的“涌现”能力，即一种**仅存在于大规模模型中的能力。**

​		gpt-4在分子设计上有很大潜力，可以设计从文字设计分子并给出合成路线的一条龙服务？



### 利用局部反应性和全局注意力进行深度逆合成反应预测（LocalRetro基于模板+图神经网络+自注意力机制）

模型代码：https://github.com/kaist-amsg/LocalRetro

简介：先前方法依照分子全局结构推荐反应物，但**分子变化往往发生在化学反应的局部**，故设计名为LocalRetro的局部逆合成框架。且因为远端官能团也会对整体反应产生次要影响，故加入**全局注意力机制（GRA，即transformer中的多头注意力机制）**。同时，**将逆合成任务的两个阶段（识别和完成）合并**。

​		预测化学反应一般涉及两个映射方向， 要么是正向 （从给定的反应物预测产物），要么是逆向（从目标产 物设计适当的反应物），而术语 "逆合成 "指的就是后 一种合成路径规划。**正向预测一般更为直接**、 因为所需的任务是**一对一的映射**，即对于一组给定的反 应物，反应产物通常是唯一确定的（在实验条件变化的 范围内）。另一方面，**逆合成是一种一对多的映射**，而 且更具挑战性，因为可能有几种不同的反应途径来合成 一种目标化合物。



**差别：**

​	**先前**

​		逆合成任务通常分为两部分：反应中心识别和变异图翻译。在反应中心识别过程中，分子中的每个键都要经过预测被切割或保留。在识别和编辑反应中心后，由 GNN 完成生成分子（合成物），以生成相应的反应物。这种识别和完成概念类似于有经验的化学家如何设计给定分子的合成途径。

​		上述几乎所 有现有的逆合成方法都采用目标分子的全局结构来进行 预测。例 如 ，在大多数现有的基于图的逆合成方法 中， 全局特征是通过对 所有原子特征求和或求平均得 到的，并用于预测反应物。在基于分子相似性的逆合成 中，也会使用分子间的全局相似性。然而，使用全局特 征进行合成途径预测可能会产生对与目标反 应 **无直接 关系的细节的关注**，这是不可取的。

​	**本实验**

​		我们认为，**识别步骤和完成步骤是高度相关的**。换句话说，由此产生的反应物 高度依赖于已识别的反应中心，因此这两个步骤应合并 并**同时进行**。

​		在大多数基本化学反应中，分子式和结构因键的断裂 或形成而发生的变化大多是局部的。**所以本实验更关注局部，并使用GRA关注远端官能团。**

​		即：在本地推导反应模板，并评估这些本地模板在目标分子的所有列举的可能反应中心的适用性。所有化学变化信息都包含在局部反应模板中，因此，一旦预测出所选反应中心的 正确模板，只需应用推导出的模板，就能立即得到反应物。换句话说，我们的本地方法将识别和完成两步流程合并为一步。此外，我们使所有反应中心都能通过注意机制交换信息，以考虑全局情况。这与化学反应的非局部效应相对应，其中反应 性有时会因为远处的化学变化而改变。

**模型方法：**我们的方法**只关注分子结构（原子或键）的变化**，以完成逆合成。也就是说，我们不是从头开始寻找合适 的反应物，而是重点推断在键的形成或断裂和/或原子的添加 或移除方面发生了哪些局部变化以形成给定的产物。 接下来，我们开发了一个模型，**通过学习每个原子和化学键的局部环境来预测产生给定产物的正确局部反应模板。**由于化学反应并不总是局部的，有些反应可能会因为分子内存在某些偏远的化学环境而受到影响，为了考虑反应的这 种全局依赖性和非局部性，我们通过应用**全局注意力机制**来 更新所有原子和化学键的特征。为了捕捉原子和化学键之间 不 同 的 反 应 关 系 ， 我 们 应 用 了 **多 头 自 注 意 机 制 ，** 即 Transformer中应用的注意机制，通过学习给定特征的键、查 询和值来学习不同的上下文。原子和化学键的特征由分子中 的所有原子和化学键共同更新。我们将非局部注意力操作称 为**全局反应注意力（GRA**）



​		模型学局部：通过学习每个原子和键的局部环境（见代码unbatch_mask、unbatch_feats），训消息传递神经网络

​				学整体：GRA

​		将局部反应模板分类：**原子反应模板A、键反应模板B、混合反应模板C**![image-20240618000945551](.\image-20240618000945551.png)



**模型流程**



![image-20240618000404241](.\image-20240618000404241.png)

​	

​		首先，根据原子和化学键的属性初始化给定**分子的分子 图（G）、原子特征（va ，点）和化学键特征（eab，边 ）**。然后，通过**消息传递神经网络**（MPNN）、**化学键特征编码层**和**全局反应注意 层（GRA）** 更新原子和 化学键 特 征 ， 以编码分子中原子和化学键的局部环境和非局部反应依赖性。最后，通过原子反应模板分类器和键反应模板分类器（两个FC）预测每个局部反应模板 在每个原子和键上的得分（即使用FC分类头）。将预测的局部反应模板应用到预测的原子和化学键上，就得到了预测的反应物，并按预测得分排序。



**模型优越性**：

​		1.即使不使用GRA也表现优于其他模型

​		2.GRA的好处：提升1-2个点。虽然从数字上看， 1-2% 的改进似乎很小，但这却是一个重要的改进。这是**因为数据集中并非所有反应都需要非局部效应来描述**。事实上，大多数化学反应都是局部的，这就是为什么我们的基线局部编码模型在 没有非局部效应的情况下已经取得了很好的结果。然 而，在许多化学反应中，**非局部效应仍是化学家考虑用来解释选择性的一个重要因素**。因此，考虑到数据集中非局部效应确实起重要作用的一小部分反应，我们认为 1-2%（额外获得 5000-10000 个反应的正确性）是统计 意义上的显著改进，在化学上也具有重要意义。此外， **当逆向合成任务包括不止一种产物时，GRA 也会发挥 重要作用**。在测试集中，由于认识到了其他分子中存在的其他原子， 在包含多个产物的反应中，有 GRA 的 LocalRetro 的 top- 1 精确匹配准确率比没有 GRA 的模型高出 12.3% （表 S2）。我们注意到，USPTO-MIT 数据集中总共 有 471 个反应（1.2%）含有多个产物。应可能不会发生。**因此，GRA 允许算法对反应中心 进行优先排序，并找到正确的反应中心，从而防止在没 有全局关注机制的情况下可能形成的不想要的产物。**

​		3.模型成功针对多个实际合成规划问题给出了多个多步骤逆合成路线，与文献所给路径几乎相同。![image-20240618001532909](D:\Mengzhou\新建文件夹\神经网络\image-20240618001532909.png)

**缺点：**但它们仅限于现有模板数据库的范围，但它们（基于模板的模型）都存在覆盖范围不完整的问题，并且不能很好地扩展。

**展望：**未来的工作应着眼于**更大的反应数据库**，以进一步推广我们的模型。这些未来的反应数据库 还可能包括**反应条件**，如试剂、温度和 pH 值，这些都 是我们在本研究中使用的当前数据集中缺乏的关键信 息。随着反应映射方法的进步，31 ，我们希望将来能 使用更大的高质量数据集来训练我们的模型。我们还希 望这里提出的局部/非局部反应性概念可以用于正向反应产物预测模型，我们的研究小组目前正在开发这种模 型 。





1.图注意力机制一定程度上类似于transformer，（本文即运用了多头自注意力机制，但不是用在消息传递上，用在了全局信息上），是否可以将本文的消息传递机制也换为多头自注意力机制？

2.是否可以利用在NLP中train好的BERT等模型，迁移到逆向合成预测中（李宏毅实验室将这样的BERT用于DNA分子序列，效果不错）

3.没有经过无标记数据预训练





### Retroformer:(无模板逆合成，transformer +图神经网络)

代码：https://github.com/yuewan2/Retroformer

简介：Retroformer 是一种基于 Transformer  的新型架构，它引入了局部注意力头，支持局部重要反应区域与全局反应背景之 间的高效信息交换，它的生成程序对精确的局部区 域也很敏感，不借助任何模板由模型直接生成反应物和产物

模型架构：整体架构包括一个编码器、一个解 码 器 和 两 个 反 应 中 心 标 识 符 。

![image-20240619093909589](.\image-20240619093909589.png)

![image-20240619094005069](.\image-20240619094005069.png)

![image-20240619094106936](.\image-20240619094106936.png)![image-20240619094141286](.\image-20240619094141286.png)

​		**ENCODER:**与在整个模块内计算图形自我注意力的现有图形转 换器（Ying 等人，2021 年；Łukasz Maziarka 等人， 2020 年；2021 年）不同，我们的模型在头部层面对图形信息进行编码。我们指定了两种类型的注意力头： **全局头和局部头**。全局注意头与虚构自我注意头相同 ，其感受野是整个 SMILES 序列。而局部注意头则考 虑分子的拓扑结构。

​		我们在关键向量和边缘特征之间执行元素乘法,然后， 来自全局和局部头部的计算结果沿着隐藏维度进行连 接，并传递到线性层,线性层表示更新后的标记特征 h l+1 ,同时，边缘更新模块是一个全连接层（FFN）， 它将接收和发送标记的更新特征的连接作为输入:![image-20240619094757500](.\image-20240619094757500.png)

​		**反应中心检测：**然后，Retroformer 会预测每个 原子和化学键的反应概率 Prc (.) 并将 S 的反应区域转 换为解码器的注意力接收区域。简而言之，检测到的 反应中心 Src 是 S 的子集。图 2b 显示了预测反应概率的热图可视化。它由两个**全连接层**完成，分别名为 Atom RC Identifier 和 Bond RC  Identifier。![image-20240619095740806](.\image-20240619095740806.png)

​		然后，我们通过以下两种策略之一将原子和键反应概率转换为 Src 中token的反应指标：

​		1.朴素：如果一个token存在于**具有反应活性的边**（即 Prc（e） > 0.5）并且**本身是反应式的**（即 Prc（s） > 0.5），我们朴素的地将它设置为反应性。请注意，**特殊令牌（special token）必须保证是非反应性的**。此策略用于训练和推理阶段。

​		2.搜索：我们对分子图进行**子图搜索，并按其反应中心分数对子图进行排名**：P si∈Src log Prc（si）+P si，sj∈Src log Prc（eij ）。在搜索中，仅考虑具有 Prc（s） > αatom 的原子和具有 Prc（e） > α 键的键，以减少计算时间。详细算法在附录 E.4 中描述。然后，选择前 n 个子图作为反应中心候选。然后，该模型为每个反应中心生成 k/n 反应物，其中 k 是预测反应物的总数。最终结果按反应中心分数和生成分数的总和进行排名。此策略仅在推理阶段使用。

​		**解码器：**解码器将上一步的生成结果、编码器输出 h 和反应中 心 Src 作为输入。与编码器类似，我们也在其交叉注 意模块中引入了两个不同的头。全局头与香草头相同 。相反，局部头部只对检测到的反应中心 Src 可见。它计算的是稀疏交叉注意力，而不是全局交叉关注。与编码器一样，从全局和局部磁头计算出的表征也会 沿着隐藏维度串联起来，并传递到线性层。

训练：我 们 还 建 议 使 用 SMILES 对齐和即时数据增强作为两种额外的训练 策略。

![image-20240619100601552](.\image-20240619100601552.png)

​		**SMILES 对齐**：SMILES 对齐是 Retro- former 的附加学习任务。与机 器翻译类似，源分子和目标分子的 SMILES 序列通常 是部分对齐的。在反应过程中，大部分分子保持不变 。图 4 显示了图和 SMILES 表示法中的这种对齐关系 。图之间的节点配准（即原子映射）可以很容易地转 换为 SMILES 之间的标记配准。这种引导式关注可以促使模型更有效地 理解化学反应。

​		**数据扩充:**我们沿用（Seo 等人，2021 年；Tetko 等人，2020 年 ）在 SMILES 生成模型中使用的相同数据增强技巧， 即生成物的 SMILES 排列和反应物的顺序排列。不过 ，我们并没有扩展现成的训练数据集，而是选择即时 进行扩展。每次迭代时，产品 SMILES 和反应物排序 排列的概率为 50%。这种动态排列允许 l+1 i 本地 Σ sj∈Src q kij T σ( √ d )vj (6) 该模型更加关注标准 SMILES，并使用经过处理的 SMILES 进行正则化。

![image-20240619100912694](.\image-20240619100912694.png)

缺点：由于我们的模型是通过学习源 SMILES 和目标 SMILES 之间的标记排列来训练的，因此预测的注意力可以很容 易地转换为原子映射。

![image-20240619101359174](.\image-20240619101359174.png)

​	该赋值错误地将 [O:11] 与 [N:11] 对齐。这个错误是可以解释的，因为反应物 中的 [HN:8] 和 [O:11] 正是发生化学变化的位置。此外，由于第二种反应物具有对称性，这种天真的原子映射也 无法实现一对一的映射。



暴论：实际上是数据集太小的问题？因为无论怎么改，理论上全局多头自注意力机制都能通过不断地学习、迭代达到相同的效果（如全局注意力机制也可以只关注局部反应，但是这需要大量时间训练）。但如果一昧的增加数据集/增大模型，可能又回到了老路。





逆合成的难点：**逆合成是一种一对多的映射**，而 且更具挑战性，因为可能有几种不同的反应途径来合成 一种目标化合物。

​						**在实际应用中产物种类为1，这是因为在公共基准数据集中只记录了主要产物。不幸的是，这个问题使得逆合成预测比反应预测更加困难，需要重新合成来连接新出现的原子**（如上图逆合成预测中的-BR,-Cl），**从而导致更大的组合搜 索空间



### OGNN（环+自注意力）

**代码**：https://github.com/O-GNN/O-GNN    模型架构：见dual graph

**简介**：在这项工作中，我们 设计了一种新的图神经网络变体--环增强图神经网络（O-GNN）。我们 明确地将环表示纳入 GNN，并与原子和键表示共同更新**除了对化合物中的原子和键进行建模外，还对环进行了明确建模**(具体到代码中应为面特征（face features）)。在 O-GNN 中，**每个环由一个潜在向量表示**，该潜在向量对原子和化学键的表示有贡献，并通过原子和化学键的表示进行迭代更新。**我们主要使用自注意层来进行自适应信息传递，并 使用MLP来引入非线性表征。**在逆反应合成方面，我们将 -GNN 应用于LocalRetro（用OGNN替换掉MPNN层），这是一种基于 GNN 的强大回溯合成方法。**通过利用环信息，无论层数多少，性能都得到了提升**。此外，我们还发现，6 层的 -GNN  与 12 层的 -GNN （无环）性能相当，这显示了在 GNN 中建立环模型的巨大威力。**但是更大的模型并不总能带来更好的验证结果**，14 层O - GNN 的验证 MAE 与 12 层O -GNN 相比略有下降。**总体而言，随着环数、最大环大小和环上 原子数的增加，O-GNN 与不对环建模的变体相比取得了更大的改进**

​		在图神经网络（GNN）中，面特征（face features）通常指的是图结构中更高层次的结构特征。具体来说，面特征可以理解为图中由多个节点和边组成的子结构，例如在分子图中，面特征可以表示由多个原子和化学键组成的环结构。

​		在分子图或其他复杂的图结构中，面特征可以帮助模型捕捉到更复杂的局部结构信息，从而提高模型的表达能力和预测性能。例如，在分子图中，面特征可以表示不同的化学环结构，这些环结构对于分子的性质和反应活性具有重要影响。

在代码中，面特征的处理通常包括以下几个步骤：

1. **初始化面特征**：根据是否需要初始化面特征，可以选择将面特征初始化为零或通过某种方式进行初始化。
2. **聚合边特征**：通过聚合与面相关的边特征来生成面特征。
3. **编码面特征**：使用多层感知器（MLP）或其他编码器对面特征进行编码。
4. **消息传递**：在GNN的消息传递过程中，面特征会与节点特征和边特征一起进行更新和传递。

**通过引入面特征，模型可以更好地捕捉到图结构中的复杂模式和局部结构信息，从而提高模型的性能。**

​		环状化合物是指体系中至少有一个环的分子，自然存在于化学空间中。根据我们从广泛使用的化学库 PubChem（Kim 等人 ，2019 年）中对 1.09 亿个化合物的统计，**90% 以上的化合物至少有一个环**。



**优点**：

​		1.理论分析表明，O-GNN 只需一层就能区分位于不同环上的两个同构子图，而传统的图卷积神经网络需要多层才能区分，这表明 -GNN 更具表现力。

​		2.当网络宽度和高度的乘积不够大时，现有的基于消息传递的 GNN 无法正确捕捉环信息

**模型架构：**![image-20240626163936093](.\image-20240626163936093.png)

		我们的模型由具有不同参数的 L 个相同层组成。各层的结构如上图所示
		E:边  V：点  R：环  U：整个化合物（全局向量？）

**初始化：**

​	我们通过可学习嵌入层初始化 h(i) ，该嵌入层表示其**原子**类型、手性、度数、形式电荷、杂化类型等。同样，我们通过 可学习嵌入层对 h(ij) 进行初始化，可学习嵌入层表示其**键**类型、立体异构类型以及键是否 共轭。 然后，我们通过连接节点嵌入和边缘嵌入来初始化 h(r) ，再用非线性层对其进行转换(即**环**)。最 后，我们用可学习的嵌入来初始化**化合物**（U）表示。在每一层中，我们依次更新节点、键、环 和化合物的表示。我们将经常使用 MLP( )（一种具有一个隐藏层的多层感知网络）来构建 我们的模型。MLP 的输入被串联成一个长向量，并由网络进行处理。

​		(1) 更新键的表示：通过连接的原子、键所属的环和上一层的化合物表示更新键的表示

![image-20240626170713698](.\image-20240626170713698.png)

​		(2) 更新原子表征：我们使用注意力模型将键代表聚合到集中原子中

![image-20240626170729842](.\image-20240626170729842.png)

​		(3) 更新环状表征：使用 MLP 网络更新环状表征

![image-20240626170741002](.\image-20240626170741002.png)

​		(4) 更新化合物表示：![image-20240626170858561](.\image-20240626170858561.png)

**堆叠 O-GNN层后，我们通过一个简单的平均池化层得到图表示，可用于图分类任务。对于节点分类任务 我们可以在 h(L) 上添加一个分类头。**



**展望**：O-GNN 具有巨大的潜力，我们将在未来把它与预训练方法结合起来



### NAG2G（节点对齐+自回归）

代码：https://github.com/dptech-corp/NAG2G

简介：**节点对齐**图到图（NAG2G）模型--一种**基于图的无模板 SSR**（单步逆合成） 方法。该模型采用 Transformer **编码器-解码器框架**，**以自回归的方式生成反应**。NAG2G **将2 维分子图和三维构象结合**起来，保留了全面的分子细节，并通过节点对齐将产物反应物原子映射纳入其中，从而以自动回归的方式确定**逐节点图输出过程**的顺序 。

**先前的问题**：虽然无模板 DL 模型在逆合成预测方面具有灵活性和前景 ，但它们往往忽略了重要的二维分子信息（二维分子图囊括了原子 环境的大量信息，如相邻原子及其连接)，并且在节点生成的原子对齐方面存在困难，导致其性能低于基于模板和半模板的方法。而基于模板的方法的局限性在于其所依赖的库。库可能无法涵盖所有潜在的反应，而且错综复杂的产品和模板结构之间可能存 在不正确的关联。

​	基于一维序列的 模型有几个局限性：

1) 序列忽略了大量的分子拓扑信息；
2) 合法的 SMILES 遵 循复杂的语法规则，这增加了生成有效 SMILES 的难度；
3) 有效利用生成物和反应物之间的原子映射信息对一维表示法来说具有挑战性。如果不对齐，模型性能可能会因生成物和反应物之间原子相关性的丢失而下降。
4) **由于一个分子可以有多个 SMILES式子**当为一个产品生成多个候选反应物时，有可能生成代表同一反应的多个反应物 SMILES，这可能会减少候选反应物的多样性（也就是说模型可能生成很多SMILES式，但其实是一个东西）

无模板学习的理论基础：在没有科学家提供任何先决知识（包括字典、模板、合成物、中间体和编辑策略 ）的情况下，深度学习模型能否学习化学？答案是肯定的。其基本思想认为，**可以用类似于自然语言处理（NLP） 任务的方式来分析分子**



模型架构：UniMol的encoder+自己训的decoder

	Encoder:编码器扮演着学习分子表示形式的关键角色.input见下式
			Oenc = fenc (X, Penc , E, R; θenc )、 X 表示原子特征；Penc 表示一维空间编码(位置编码)，是原子嵌入的补充； E 表示二维图结构固有的边特征；R 对应三维构象中的原子坐标；θenc 封装编码器的 可学习参数，Oenc 是编码器得出的分子表示结果。
			
	Decoder:解码器主要是通过自动回归法逐个节点生成反应物图
			在第i 个时间步，也就是 第i 个生成节点（原子）时，解码器会收到三个不同的输入：
	        1) 编码器的输出，包括有助于编码器和解码器之间交互的键和值。 
	        2) 解码器的输出来自之前的步骤（从 1 到 i-1），这是典型的自回归模型，因为新值的预测是基于之前的值。在迭代过程中，增加了一维位置编码，这对 NAG2G 对齐 编码器（产物）输入和解码器（反应物）输出之间的原子顺序至关重要。
	        3) 当前输出图的图层特征，如节点的度和节点间的最短 路径。将这些图层特征直接整合到模型中会带来效率上的挑战，因为图层特征会随时间步长而变化。为了解决这个问题，我们提出了一种整合这些图层特征的高效方法（见原文图5）。
	        根据上述输入，解码器在第i 个时间步自动生成一个新节点，从原子类型开始然后是相关的形式电荷、相连氢原子的数量，最后是与先前节点（原子）相连的边（ 键的类型）。每个节点的信息都是根据上述预测依次生成的

![image-20240703145203468](.\image-20240703145203468.png)

**节点对齐**：分子图与句子不同，缺乏固有顺序，因为分子中的原子在分配之前没有自然顺序。无序节点不仅给图形生成带 来了挑战，也给编码器的输入数据扩增带来了挑战。为了应对输入和输出数据扩充方面的挑战，并实现灵活的逐节点自动回归生成， 我们提出了一种基于产品-反应物节点对齐的新方法。

​		我们的方法首先由 RDKit 随机 生成产品的 SMILES 序列、37 如图 3 所示。按照 SMILES 序列中的新顺序，我们得到 了**数据增量**输入图的节点序列顺序。随后，使用位置嵌入标记图节点顺序。如图 3 和 图 4 所示，对于已确定顺序的产品图，我们建立了一条唯一且明确的规则，该规则与 逐节点输出的反应物节点顺序相对应。在反应物中，原子可分为两类：与生成物共享 的原子和反应物独有的原子。原子顺序的分配应考虑这两个方面。首先，在生成顺序 时，我们规定反应物中共享原 子 的顺 序 应优先于非共享原子的顺序。为了确保对于 特定的有序产品输入，存在唯一对应的有序反应物，反应物中共享原子的顺序应遵守产品中的顺序。随后，使用 RDKit 将反应物 SMILES 与生成物 SMILES 对齐，以获 得最相似的 SMILES。最后，从对齐的反应物 SMILES 中提取非共享原子的顺序，确保非 共享原子顺序的唯一性。这种方法利用了产品-反应物对齐信息，确保节点生成顺序 与训练过程中的输入图顺序一致，并允许在输入和输出中进行一致的等变量数据扩增 ，从而提高了生成过程的整体稳健性和准确性。<img src=".\image-20240703151552968.png" alt="image-20240703151552968" style="zoom: 80%;" />

<img src=".\image-20240703151626965.png" alt="image-20240703151626965" style="zoom:80%;" />

可以将SMILES换成SELFIES

但是科学语言真的能像自然语言一样通过语境学习吗？



### LORA（用于LLM的低秩适应）

代码：https://github.com/microsoft/LoRA

解读：[LoRA: Low-Rank Adaptation of Large Language Models 全文解读 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/681019425)

https://www.bilibili.com/video/BV1MH4y1g77V/?spm_id_from=333.1007.tianma.2-1-4.click&vd_source=898cb0d612cdcdea138eb015775eb7cb

- **挑战背景**：大型预训练语言模型在全参数微调时存在灵活性不足和昂贵的独立部署成本等挑战。
- **LoRA方法引入**：作者引入了LoRA方法，该方法旨在在大型语言模型上进行**适配**，以降低训练参数数量。与之相比，微调的最大缺点是新模型包含的参数与原始模型一样多。
- **低秩矩阵注入**：LoRA的独特之处在于它在每个Transformer结构层中注入了**可训练的秩分解矩阵**，而不会修改预训练模型的权重。
- **优化显存和训练时间**：相对于传统微调方法，LoRA显著减少了所需的显存和训练时间，降低了资源成本。
- **通用性验证**：LoRA方法在多个任务和多个大型语言模型上进行了广泛验证，包括RoBERTa、DeBERTa、GPT-2和GPT-3，表现出相当甚至更好的性能。

**当前研究动态**

- **部分参数适配**: 针对这一问题，部分研究尝试只对模型的部分参数进行适配，或者为新任务学习外部模块，以此减少模型的总参数数量。

- **带来的挑战**: 这些方法通常会导致推理速度延迟（例如通过加深模型结构）或减少模型可处理的序列长度。

- **性能与效率的平衡**: 虽然这些方法提高了操作效率，但往往未能达到全参数微调的性能水平，从而形成了效率与模型质量的权衡

  ![img](https://pic2.zhimg.com/80/v2-2965bfe36b6c33d4afea39625108fd01_720w.webp)

  ​																	LORA：预训练权重W不受影响，只优化A和B的参数

- **设计灵感**：LoRA方法的设计启发来自于这样一个简单而深刻的观点：**即便是最复杂的模型，在其核心结构上往往隐藏着简单性**（其他人的研究表明 ，学习到的过参数化模型实际上**驻留在较低的内在维度**上。）。这就像是在一个充满书籍的巨大图书馆中，尽管书籍众多，但我们真正需要的知识往往只蕴含在其中的少数几本重要书籍里。**因此，一个超参数化的模型其实依赖于较低的内在维度，这意味着在模型适配过程中，权重的变化也倾向于展示出低“内在秩”。**
- **工作方式**：LoRA的方法就像是在这庞大的图书馆里精准地找到那些关键的书籍，并且专注于对这些书籍进行研究和更新。换句话说，LoRA通过改变模型的一些核心部分（即“关键书籍”），而不是对整个图书馆（即整个模型）进行调整。如`图1`所示，**密集层权重 𝑊 的变化可用较低的 "内在秩"的矩阵，变化矩阵的分解矩阵 和𝐴和𝐵 也是低秩矩阵，通过在适配过程中优化低秩分解矩阵 和𝐴和𝐵 来实现的，同时保持预训练的权重 𝑊 不受影响。**
- **主要优点**：这种方法的优势非常明显。首先，它极大地节省了空间，因为我们只需要专注于那些关键的几本书，而不是整个图书馆的所有书籍。其次，它也在时间和资源上更加高效，因为我们仅仅专注于最重要的部分。LoRA在存储和计算效率上都表现卓越，显著降低了存储需求和任务切换的成本，并减少了对硬件的依赖。此外，LoRA还能与多种现有方法（例如前缀调整）相结合使用，进一步增强其实用性和灵活性。

**方法**：

**低秩参数化更新矩阵**：神经网络由许多执行矩阵乘法的层组成，**通常这些层的权重矩阵是满秩的**。但研究发现，**预训练语言模型的 "本征维度 "较低 ，即使随机投影到较小的子空间，仍能高效学习**。受此启发，作者认为在适配其它任务过程中，**权重的更新** ∆𝑊∈𝑅𝑑×𝑘 **也具有低“内在秩**”。因此，采用低秩分解来约束权重矩阵的更新，预训练权重矩阵 𝑊0∈𝑅𝑑×𝑘 , 其中，，𝑊0+∆𝑊=𝑊0+𝐵𝐴,其中𝐵∈𝑅𝑑×𝑟，𝐴∈𝑅𝑟×𝑘，𝑟≪𝑚𝑖𝑛(𝑑,𝑘) 。

 **训练训练过程：**训练过程中， 𝑊0 保持不变，不接收梯度更新，而 𝐴 和 𝐵 包含可训练参数，通过调整它们可以改变 ∆𝑊 ，从而适应新任务。由于 𝑊0 和 Δ𝑊=𝐵𝐴 的输入相同，对于 ℎ=𝑊0𝑥 ，训练过程修改前向传递为： **ℎ=𝑊0𝑥+Δ𝑊𝑥=𝑊0𝑥+𝐵𝐴𝑥** ，如在`图1`中展示了的重参数化。对 𝐴 使用随机高斯初始化，对 𝐵 初始化为零，因此在训练开始时 ∆𝑊=𝐵𝐴 为零。为了调参时，改变秩 𝑟 对其它训练参数影响较小，我们将 ∆𝑊𝑥×𝛼𝑟 ，简单地将 𝛼 做为常数，并设置为我们尝试的第一个 𝑟 。这是因为使用 Adam 优化时，调整 𝛼 大致相当于适当缩放初始化后调整学习率。

**全微调的泛化：**更一般的微调形式允许训练预训练参数的一个子集。LoRA更进一步，不要求在适应期间对权重矩阵的累积梯度更新具有满秩。这意味着，当将LoRA应用于所有权重矩阵并训练所有偏差时，我们通过将LoRA秩 r 设置为预训练权重矩阵的秩，大致恢复了全面微调的表现力。换句话说，随着我们增加可训练参数的数量（增加LoRA秩r），训练LoRA大致收敛到训练原始模型，而基于适配器的方法收敛到一个MLP，基于前缀的方法收敛到一个不能接受长输入序列的模型。

 **无额外推理延迟：**在生产环境中，我们可以计算并存储，其中和𝑊0+𝐵𝐴，其中𝑊0和𝐵𝐴 ∈ 𝑅𝑑×𝑘 ，然后进行推理。当需要切换到另一个下游任务时，我们可以通过减去 𝐵𝐴 来恢复 𝑊0 ，然后添加不同的 𝐵′𝐴′ ，这是一个非常快速的操作，内存开销很小。这确保了与微调模型相比，在构建上不会引入额外的推理延迟。

**应用方法**: LoRA技术可以应用于神经网络中任何权重矩阵的子集，以减少模型的可训练参数。这对于Transformer架构特别有用，其中自注意力模块包含四个权重( 𝑊𝑞,𝑊𝑘,𝑊𝑣,𝐾𝑜 )矩阵，MLP模块包含两个群权重矩阵。在实践中，**LoRA主要用于优化注意力权重**( 𝑊𝑞,𝑊𝑘,𝑊𝑣 )，并冻结MLP模块，以简化模型并提高参数效率。并将MLP层、LayerNorm层和偏置的实证研究留作未来工作



**我们应该对transformer中的哪些权重矩阵应用LoRA？**

 在有限的参数预算下，研究探讨了在下游任务中获得最佳性能时应该使用LoRA来适应哪些权重类型。研究考虑了GPT-3 175B模型的96层，设置了18M的参数预算，对于适应一种类型的注意权重，秩r为8，对于适应两种类型，秩r为4。结果表明，将所有参数放入 Δ𝑊𝑞 或 Δ𝑊𝑘 中会导致性能显著下降，而**同时适应 𝑊𝑞 和 𝑊𝑣 会产生最佳结果**。因此，**适应更多的权重矩阵比适应具有更大秩的单一类型的权重更为有利**。这一结果强调了在有限参数预算下的权重适应策略。

![img](https://pic1.zhimg.com/80/v2-a3156110f3274b8e8f410d6a11788148_720w.webp)

表5：在GPT-3中将LoRA应用于不同类型的注意权重后，在相同数量的可训练参数下，对WikiSQL和MultiNLI进行的验证准确性。适应Wq和Wv两者在整体性能上表现最佳。我们发现在给定数据集中，不同随机种子之间的标准差是一致的，我们在第一列中进行了报告。

**LORA的最佳秩是什么？**

 该研究关注了在自注意模块中使用不同秩( 𝑟 )的适应矩阵对模型性能的影响。如`表6`所示，研究比较了适应不同类型的权重矩阵组合，包括 、{𝑊𝑞,𝑊𝑣}、{𝑊𝑞,𝑊𝑘,𝑊𝑣,𝑊𝑐} 和仅适应 𝑊𝑞 。令人意外的是，**结果表明LoRA在非常小的 𝑟 值下已经具有竞争力，特别是对于 {𝑊𝑞,𝑊𝑣} 的情况。这表明适应矩阵的更新矩阵∆W可能具有非常小的“固有秩”**。**并且∆Wq 似乎比 ∆Wv 具有更高的 "内在秩"**

 为了支持这一观点，研究还分析了不同r和不同随机种子学习的子空间之间的重叠性。发现增加r并不会覆盖更有意义的子空间，而适应矩阵的前奇异向量方向在模型性能中起着关键作用。这些方向在不同r和不同随机种子下都表现出明显的重叠，说明适应矩阵的秩可以非常低。**增加 r 并不能覆盖更有意义的子空间，这表明低秩适应矩阵就足够了**

 总的来说，研究发现低秩的适应矩阵已经足够有效，前奇异向量方向对于模型性能具有重要意义。这一发现对于理解和优化模型适应过程具有重要意义。

![img](https://pic1.zhimg.com/80/v2-bf0be63f74d785a3a3132cf75ec0b0c0_720w.webp)

表6：在WikiSQL和MultiNLI上用不同的秩r进行验证的准确性。令我们惊讶的是，在这些数据集上，**r小到1就足以适应Wq和Wv**，而单独训练Wq需要更大的r。

**7.3 适应矩阵 Δ𝑊 与 𝑊 相比如何？**

 研究调查了适应矩阵 Δ𝑊 与权重矩阵 𝑊 之间的关系，并解答了几个关键问题。首先，研究发现 Δ𝑊 与 𝑊 之间存在较强的相关性，表明 Δ𝑊 在某种程度上放大了 𝑊 中已有的一些特征。其次， Δ𝑊 并不是简单地重复 𝑊 的前奇异向量方向，而是主要放大了那些在 𝑊 中没有强调的方向。第三， Δ𝑊 的放大因子相当大，这意味着它在特定情况下具有显著的影响力，尤其是对于 𝑟=4 的情况。值得注意的是，当 𝑟=64 时，放大因子较小，如表7所示。具体原因在`附录H.4`节有进一步讨论。

 Δ𝑠 这些发现暗示着低秩的适应矩阵可能会在下游任务中放大通用预训练模型中已学到但未强调的关键特征，为适应预训练语言模型提供了深入的理解。

![img](https://pic3.zhimg.com/80/v2-f6d50e67862e34c48314a721f105069a_720w.webp)

表7：U&gt;WqV&gt;的Frobenius范数，其中U和V分别是（1）∆Wq、（2）Wq或（3）随机矩阵的左/右前r个奇异向量方向。这些权重矩阵来自GPT-3的第48层。



### X-LORA（混合LORA）

代码：X-LoRA codes and examples are available at https://github.com/EricLBuehler/xlora. Model weights and data associated with the X-LoRA discussed in this paper, along with additional examples, are available at https: //huggingface.co/lamm-mit/x-lora. The X-LoRA-Gemma model is available at https://huggingface.co/lamm-mit/x-lora-gemma-7b. The Mistral.rs inference engine is available at https://github.com/EricLBuehler/mistral.rs.

简介：这是一个用于大模型微调的插件。X-LoRA 模型可以轻松地应用于任何现有 的大型语言模型（LLM），而**无需修改底层结构**。这项工作的影响包括获得**具有强大领域知识和跨领域知识整合能 力的可随时扩展和适应的模型**

​		LORA:大型语言模型（LLMs）已经获得了极大的普及，然而，训练这类模型的成本可能很高，尤其是当需要不同的能力集时。低阶适配器（Low-rank adapters， LoRA）等方法已被提出作为一种更有效的替代方法，**但适配器通常侧重于较窄的知识领域**。LoRA 建模的基本概念是使用低秩矩阵添加到原始全矩阵中，并选择这些低秩矩阵作为模型中唯一可训练的部分。由 于只有适配器层是可训练的，这些模型通常**保留了基础模型预训练时获取的知识**，同时使模型更适用于特定 任务，而且在计算需求方面也很高效。由于 LoRA 适配器的训练效率很高，因此有可能利用这种策略开发出 **大量专用模型**。

​		X-lora:在这里，我们提出了一种计算高效的专家**混合**方法，使用一组不同的 LoRA 适配器，这些适配器可以很容易地进行单独训练。



思考：假设开发出大模型 可以参考一下用 









### 用于化学反应预测的自我反馈知识激发方法(prompting+反应类型，用于LLM)

代码：https://github.com/AI-HPC-Research-Team/SLM4CRP

简介：**CRP 的关键在于准确识别各种条件下涉及键断裂和重构的机理和结果**。本文开发出一套**动态提示模板**（prompting）并**通过聚类提取了化学反应可能的反应类型**。然后采用**自适应提示学习**（提示学习包括设计输入 "提示"，引导 LLM 执行特定任务或生成特定类型的反应），将上述先验知识（知识先验指的是预先存在的、特定领域的知识，这些知识可以集成到人工智能模型中，以增强模型的理解和预测能力）注入大语言模型（LLM），提升其表现。

![image-20240628172515175](.\image-20240628172515175.png)



先前问题：静态模板提示会导致 LLMs 中的指导模式僵化，从而可能影响其可遗传性。动态 提示可以解决静态模板的 局限性，将先验知识注入 LLM。数据集通常缺乏 RT 标签（反应类型），只包括反应物和 产物，而识别 RT 可以缩小待探索的化学空间。

优点：我们的方法通过自适应提示学习和自反馈知识激发技术，**将 RT 知识先验与 LLM 相结合**。**它解决了真实世界数据集中 RT 信息稀缺的问 题**，而且动态提示避免了 LLM 中僵化的模式引导，为提高预测准确性提供了一种解决方案。



**数据集**：在本研究中，我们利用了 Mol-Instructions [15] 数据集 中的 "面向分子的指令"，其中包括三个化学反应任务。

**模型方法**：三个阶段：知识提取阶段、数据整理阶段、微调阶段

![image-20240628182514595](.\image-20240628182514595.png)

​	**知识提取阶段**：（在训练集上）

​		1.反复推敲最优聚类方法（即反应类型RT）的选择。

​		首先要嵌入（embedding）训练数据集 Dtrain 的输入和输出，然后将其聚类（使用K-means）为相关群组，并将其注释为 RT。**即通过无监督聚类根据输入输入自动地将训练集划分为N类反应。**经试验，将 输 入 和 输 出 连 接 成 一 个 向 量 （ **concat**(inputvec , outputvec ）效果最好，N=10效果最好。

​		常见的基本反应类型有几种，包 括但不限于合成反应、分解反应、单置换反应和双置换 反应。虽然可以对这些基本类型进行细分，**但过细的分 类可能不利于有效预测模型的推广和效率**。更重要的是 ，随着聚类数量的增加，无监督标记的准确性可能会降 低。此外，聚类数量过多可能会使模型的可解释性复杂 化，从而使用户更难理解和信任模型的预测结果。考虑 到这些因素，**建议簇的数量在 3 到 12 个之间**[45]。

​		2.训练 LLM-RT，从而形成自我反馈聚类机制。

​		![image-20240628174647166](.\image-20240628174647166.png)![image-20240628175128114](.\image-20240628175128114.png)![image-20240628175212312](.\image-20240628175212312.png)

​	**数据整理阶段**:(测试集、验证集)

​		3.冻结的 LLM-RT 在输入的同时使用提示，在验证集和测试集上执行 RT 注释。

​		利用 **经过训练和冻结**的 LLM-RT，我们为验证数据集和测试 数据集注释 RT

​	**微调阶段**:

​		4.结合提示增强功能对 LLM 进行微调

​		为了解决静态模板的限制并提高模型 的泛化能力，我们为模板选择引入了**自适应选择器**。 我们建立了一个指令模板库，为每个任务分配 12 个不 同的模板，共计 36 个模板。在嵌入过程中，数据集 D 中的输入i 和指令模板库中特定任务列表中的所有相应模板都通过 LLM-CRP 模型嵌入。然后，自适应选择 器通过输入嵌入和库中每个模板嵌入之间的向量差异（**欧氏距离**）来评估适应性。对于每个批次，这一过程需要将单个 输入与多个指令进行匹配，以根据适应性分数确定最 适合的指令。**适应性选择对输入适应性最强的模板**，以促进精确的知识注入

​		选定的自适应指令与当前输入的相应 RT 相结合， 形成增强提示。



思路：1.可以利用其他模型（LLM）中获取数据（比如化学反应，比如其他模型预测的错误反应作为难负例），以扩充数据集

​			2.划分反应类型的想法不仅可以用在大模型，这是一种数据增强的方法，在小模型训练时也可以用。

​			3.可以借鉴Mol-Instructions数据集+USPTO划分反应类型。



### T5 BioT5 BioT5+

代码：

​		1.T5(HUG，可直接调用):https://hf-mirror.com/google-t5/t5-base   GIT: https://github.com/google-research/text-to-text-transfer-transformer



T5:LLM三大流派之一（**Encoder-Decoder**）,LLM的三大流派的两个起始模型：GPT-1（Decoder only）、BERT（Encoder only），但是这两个模型针对不同下游不同的NLP任务时还需要进行一定的修改（如添加一些线性层），Google经过庞大的预训练，最终提出了一个通用框架T5模型（Encoder-Decoder）， **将所有NLP任务转化为text to text任务**，**微调时无需再修改模型**，直接在原模型上微调即可。

​	**T5最核心的理念**是：使用前缀任务声明及文本答案生成，统一所有自然语言处理任务的输入和输出。在此之前的几乎所有预训练语言模型，在下游任务微调过程中都需要添加非线性层，将模型的输出转化为任务指定的输出格式。将**每个**文本处理问题都视为 "文本到文本 "问题，即以文本为输入，以新文本为输出

下图所示为T5的输入格式和输出格式。绿色部分表示翻译任务，红色和黄色部分分别表示CoLA（单句分类）和STS-B（文本语义相似度）任务，蓝色部分表示摘要生成任务，左侧的框表示T5的输入样例，右侧的框则是对应的输出结果。

![c29e931cc09049feb2bff69377c94d56](.\c29e931cc09049feb2bff69377c94d56.png)

**T5唯一需要做的就是在输入数据前加上任务声明前缀**，如：

英德翻译：translate English to German：That is good.
情感分类：sentiment：This movie is terrible!

**此时就获得了完整的 T5 模型及其训练方法：**

- Transformer Encoder-Decoder 模型；
- BERT-style 式的文本破坏方法（MLM）；
- Replace Spans 的文本破坏策略（若干个连续词一起替换，只预测被替换的词。例如：Thank you`<X>`me to your party`<Y>`week.|`<X>`for inviting`<Y>`last`<Z>`，用“|”分割T5的输入和输出）；
- 15 %的文本破坏比；
- Replace Spans破坏时小段长度为3。



启发：

​		1.理想情况下， 这种（基于大规模数据集的无监督）预训练能使模型发展出通用能力和知识，然后将其迁移到下游任务中，数据集越大，模型规模越大，效果也就越好。

​		2.迁移学习的一个有益用途是在**低资源任务中获得良好性能**的可能性



**BioT5,BioT5+**:将T5模型用于生物数据集。

亮点：解决了无效分子微笑的产生，上下文信息的利用不足，以及结构化和非结构化知识的平等对待等问题。

​		(1)我们主要集中在 两个生物学模态 ——分子和蛋白质——**以文本作为知识库和桥梁来丰富 分子和蛋白质领域的基础关系 和性质**； 然而，分子、蛋白质和文本代 表了完全不同的语言。这三种不同情态中的同一 个标记具有不同的语义（记 号“C”在自然语言中表示字符 C，在分子中 表示碳原子，在蛋白质中表示半胱氨酸(20 种 氨基酸 之 一 )）。		

​		**我们对分子、蛋白质和文本使用不同的词汇**。在 BioT5 中，**分 子由 SELFIES 字符串表示**，其中每个化学上有意义 的原子团都 包 含 在括号中 (原始 T5 字典是 使用句子片断 (Kudoand Richardson,2018).然 而，直接使用这个分子微笑词典是次优的，因为 一些化学上有意义的标记，如官能团或完整的原 子，将被不准确地识别。如Br被拆分为B和 r)， 并标记 为 SELFIES 标 记 。比如 [C][=C][Br] [C] ， [=C]，[Br]。**对于蛋白质**，为了区分文本中大 写字母的氨基酸，我们为每个氨基酸引入一个 特殊的前缀< p>。 **对于文本**，我们使用与原始 T5 相同的字典。![image-20240702155406040](.\image-20240702155406040.png)

​		(2)我 们使用**多任务预训练**以更全面的方式对这三种 模态之间的联系进行建模。

​		(3) **使用 SELFIES** 提供了更健壮和容错 的分子表示替换SMILES，消除了 SMILES 经常遇到的非法结构的问题。



**BioT5+**，是 BioT5 框架的扩展，专为加强生物研究和药物发现而量身定制。**BioT5+ 融合了几项新功能**：整合 IUPAC 名称以促进分子理 解、纳入来自 bioRxiv 和 PubChem 等来源的大量生物文本和分子数据、多任务结构 调整以实现跨任务的通用性，以及数字标 记化技术以提高数字数据的处理能力。这 些改进使 BioT5+ 能够弥合分子表征与其文 本描述之间的差距，提供对生物实体更全 面的理解，并在很大程度上改进了生物文本和生物序列的基础推理

​		生 物 文 献 中有大量关于分子 和蛋白质的信息。当这些文献中提到一个生 物实体时，其上下文主要围绕着对该实体某 些特征的描述。因此，越来越多的工作致力 于**文本和生物实体的联合建模**。

​		简而言之，BioT5+ 包含 以下重大改进：

​		 (1) 增强对分子的理解：**通过将 IUPAC 名称 整合到 BioT5+ 框架中，该模型可以更深入地 理解分子结构**。这种整合使 BioT5+ 能够解释 通常出现在科学文献中的化学名称，缩小了正 式分子表征（如 SELFIES）与文本描述之间 的差距。因此，这增强了对分子的理解，有助 于对分子特性和活性进行更准确的预测和分析 。

​		 (2) 扩展的生物文本和分子数据：与 BioT5 相比，BioT5+ 包含来自 bioRxiv（Sever 等人 ， 2019 年 ） 和 PubMed （ Canese 和 Weis ， 2013 年；White，2020 年）等来源的大量生物 文本数据，以及来自 PubChem（Kim 等人， 2019 年）的高质量分子数据。这种扩展不仅扩 大了模型的知识基础，还丰富了对生物实体的 文字理解。

​		(3) 多任务指令调整：BioT5+ 针对下游任务 采用多任务指令调整策略，而不是针对每个任 务单独进行专门的模型训练。通过利用统一的 多任务训练框架，BioT5+ 可以无缝整合来自不 同任务的知识，增强其在不同生物和化学领域 的预测能力和泛化能力。

​		(4) 高级数字标记化：鉴于数值表示法的局限性，BioT5+ 从 Llama （Touvron 等人，2023a）模型中汲取灵感， 整合了先进的基于字符的数值标记化策略。 这种技术可以更细致、更一致地表示数值。



数据集：详见BioT5(3.1节)，BioT5+(3.2节)化学可用的有分子-文本数据、小分子数据

启发：

​		1.多模态建模时要区分不同模态下相同字符表示的不同语义r

​		2.采用多模态数据集大有脾益（如文本描述的化学性质、分子-文本数据），分子数据和文本数据联合预训练的有效性。 可以利用chatgpt等模型生成对USPTO数据集中反应的反应原理描述？

​		3.分子-文本的更多工作见BioT5+（2.1节）





数据集：chebi-20：检索自然语言描述的相关分子（可以用于从自然语言生成分子等）





# 数据集

​	多模态:标准4M数据集（COCO SBU VG CC）、 LAion数据集（4B,5B） 

​	CV:Image net 22k(14M)

​    NLP:

# IDEA

仿BLIP 左脚踩右脚生成新数据集（图片——文本对）进而训练模型 类似于题改一改数字

模型的训练可以是一个从难到易的过程？简单题——中等——困难

从人的角度去思考问题：学习是一个从难到易的过程、



有两条路：1.一条是改变模型架构，但这事OGNN已经做了(但是OGNN没有用于无模板生成，只用在了LocalRetro)

​					2.另一条是从数据集角度出发  设计多模态小模型？



按优先级：

