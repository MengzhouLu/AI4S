# 翻墙问题：

**OSError: Can't load tokenizer for 'bert-base-uncased'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'bert-base-uncased' is the correct path to a directory containing all relevant files for a BertTokenizer tokenizer.**

解决方法：HF_ENDPOINT=https://hf-mirror.com python3 blip2_qformer.py



GIT clone 失败：(这个方法有一个弊端，无法提交)

​	git clone https://mirror.ghproxy.com/https://github.com/USERNAME/REPOSITORY



# 多GPU训练问题：

[pytorch 并行训练之DistributedDataParallel（代码样例和解释） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/350301395)

[关于集群分布式torchrun命令踩坑记录（自用）-CSDN博客](https://blog.csdn.net/Komach/article/details/130765773)

torchrun --nproc_per_node=1 train_mymodel.py（nproc_per_node即GPU数量）

# GPU占用问题：

过高：混合精度训练

低：设置dataloader：![7f382ca37634c129b8654b97d778f8b7.jpeg](https://i-blog.csdnimg.cn/blog_migrate/c1a4c73a4047daf4f6d664f4926574e6.jpeg)

# 多数据集训练问题

怎么用：

**Batch_size和learning rate:**DDP中

​	首先明确没有前缀的batch size通常是指单卡的batch size，对于多机多卡有效batch size（effective batch size）=节点数* 每节点卡数* 单卡batch size*梯度累计步数。（**如单卡BZ=32,DDP4卡，则有效BZ=32 *4=128**）

​	现在假设在单卡环境下训练某个模型使用的batch size=32，lr=0.01。这时在4卡环境想重复这个实验，该如何设置batch size和lr呢？简单的回答是，在pytorch或者lightning中，需要将给data_loader的batch_size设为32/4=8，而lr保持不变为0.01。(**这样在主进程上tqdm显示的epoch数实际没有变化，因为DDP4卡时数据会拆成4份，每个GPU只拿到了原先单卡时1/4的数据，而bz也正好设置为了原先的1/4；那么在epoch数实际没有变化时，lr不需要改变**)

​	如果保持BZ=32,则有效BZ=32 *4=128，在主进程上tqdm显示的epoch数为单卡时1/4，所以lr要随之线性增长（或者平方根增长）

​	**注意，如果epoch总数很少，但是一个epoch中batch很多，那么学习率的更新就不应该在每个epoch中，应该在每个batch中**

# LINUX

screen下按Ctrl+a，再按ESC即可翻页

# Token

openai经验:一个有用的经验法则是，对于常见的英语文本，一个标记通常对应于 ~4 个字符的文本。这大约相当于一个单词的 3/4（所以 100 个标记 ~= 75 个单词）。

https://platform.openai.com/tokenizer

# Transformer

## 推理过程

他的推理过程跟forward函数不太一样，具体是dec的处理部分从已知gt（长度）变成了**迭代**。具体地说，是将本轮预测输出的最后一个 Token 叠加到输入文本序列（初始为start token）中，然后再送入到解码器层进行计算，形成下下个 Token。通过这个迭代的过程，形成最终的输出结果。

以“我爱你”到“I love you”为例，对于transformer来讲，在翻译“我爱你”这句话时，先将“我爱你”进行embedding和encoding送入encoder（包含n层encoder layers），encoder layer之间完全是串联关系，最终在第n层encoder layer得到k，v。
对于decoder来讲，t0时刻先输入起始符（S），通过n层decoder layer（均计算masked自注意力与交互注意力（由decoder得到的q与encoder输出的k，v计算）），最终在第n层decoder layer得到当前的预测结果projection（可以理解成“I love you”中的“I”）；在t1时刻，将t0时刻得到的输出“I”和历史输入（S） 输入至decoder，重复t0的过程（计算masked自注意力与交互注意力），最终在第n层decoder layer得到输出“love”。最后经过多次计算，得到完整输出结果“I love you E”（E为终止符，由“you”输入decoder得到）

这个迭代过程可以运用一些算法：比如波束搜索（Beam Search）,贪心算法。

**最容易想到的策略是贪心搜索**，即每一个时间步都取出一个条件概率最大的输出，再将从开始到当前步的结果作为输入去获得下一个时间步的输出，直到模型给出生成结束的标志。例如下图，每一个时间步都取出了条件概率最大一个结果，生成了序列`[A,B,C]`。

![img](https://pic4.zhimg.com/v2-8b97ab38c910e9d6b767a6cd7738a3db_1440w.jpg)

**而beam search**是对贪心策略一个改进。思路也很简单，就是稍微放宽一些考察的范围。在每一个时间步，不再只保留当前分数最高的**1**个输出，而是保留**num_beams**个。当num_beams=1时集束搜索就退化成了贪心搜索。

下图是一个实际的例子，每个时间步有ABCDE共5种可能的输出，即，图中的num_beams=2，也就是说每个时间步都会保留到当前步为止条件概率最优的2个序列。

![img](https://pic4.zhimg.com/v2-a760198d6b851fc38c8d21830d1f27c9_1440w.jpg)

Beam Search示意图

- 在第一个时间步，A和C是最优的两个，因此得到了两个结果`[A],[C]`，其他三个就被抛弃了；
- 第二步会基于这两个结果继续进行生成，在A这个分支可以得到5个候选人，`[AA],[AB],[AC],[AD],[AE]`，C也同理得到5个，此时会对这10个进行统一排名，再保留最优的两个，即图中的`[AB]`和`[CE]`；
- 第三步同理，也会从新的10个候选人里再保留最好的两个，最后得到了`[ABD],[CED]`两个结果。

可以发现，beam search在每一步需要考察的候选人数量是贪心搜索的num_beams倍，因此是一种**牺牲时间换性能**的方法。

接下来要做的重复这个过程，逐步生成单词，直到遇到结束标识符停止。最后得到概率最大的那个生成序列。其概率为：![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/4591e820920d232d2c9d9b29d7ff21cf.png#pic_center)



**不足之处**

2.1 Length normalization：惩罚短句
根据最后的概率公式可知，该算法倾向于选择最短的句子，因为在这个连乘操作中，每个因子都是小于1的数，因子越多，最后的概率就越小。解决这个问题的方式，最后的概率值除以这个生成序列的单词数，这样比较的就是每个单词的平均概率大小。此外，连乘因子较多时，可能会超过浮点数的最小值，可以考虑取对数来缓解这个问题。谷歌给的公式如下：

其中α∈[0,1]，谷歌建议取值为[0.6,0.7]之间，α用于length normalization。

2.2 Coverage normalization：惩罚重复
另外我们在序列到序列任务中经常会发现一个问题，2016 年， 华为诺亚方舟实验室的论文提到，机器翻译的时候会存在over translation or undertranslation due to attention coverage。 作者提出coverage-based atttention机制来解决coverage 问题。 Google machine system 利用了如下的方式进行了length normalization 和 coverage penalty。

还是上述公式，β用于控制coverage penalty


coverage penalty 主要用于使用 Attention 的场合，通过 coverage penalty 可以让 Decoder 均匀地关注于输入序列 x xx 的每一个 token，防止一些 token 获得过多的 Attention。

2.3 End of sentence normalization：抑制长句
有的时候我们发现生成的序列一直生成下去不会停止，有的时候我们可以显式的设置最大生成长度进行控制，这里我们可以采用下式来进行约束：

其中∣ X ∣ |X|∣X∣是source的长度，∣ Y ∣ |Y|∣Y∣是当前target的长度，那么由上式可知，target长度越长的话，上述得分越低，这样就会防止出现生成一直不停止的情况。



## LOSS函数

就是CELOSS，计算每一个位置的损失

CELOSS期望的输入是[A,dim],而decoder的输出是[bz,seq_length,dim],所以需要view(-1,dim)一下变为[bz*seq_length,dim]再算CELOSS

![img](https://pic2.zhimg.com/v2-214b4cb1ebc7d58cb759ad3c05f79b4f_1440w.jpg)

我们用例子中的句子训练模型，希望产生图中所示的概率分布，我们的模型在一个足够大的数据集上，经过足够长时间的训练后，希望输出的概率分布如下图所示：

![img](https://pic4.zhimg.com/v2-faaae02e404b123f688be000762dae8f_1440w.jpg)
