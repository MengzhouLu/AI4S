# 单步逆合成

## 10.14

处理数据集：基本划分为smiles-text格式，目前仅采用smiles,name,decription，还在考虑MOL-INS用哪些数据

mol-ins:selfies to smiles    注意：mol to smiles有些参数要调（应该全部使用RDKIT转化为标准Smiles）

## 10.15

**借助RDKIT标准化smiles**

mol-ins:

​	property_prediction:362100

​	molecular_description_generation:298319

​	retro:129684(用于Fine-tune)

ChEBI-20-MM:

​	test:3297

​	valid:3299

​	train:26402

PUBchem

​	smiles to name:260408

​	smiles to text:261325



split在dataset里面写吧

**部署Q-former**：

​	Q-former超参数：[LAVIS/lavis/projects/blip2/train/pretrain_stage1.yaml at main · salesforce/LAVIS · GitHub](https://github.com/salesforce/LAVIS/blob/main/lavis/projects/blip2/train/pretrain_stage1.yaml)



​	**注意精度问题！目前MOLR默认输出精度为fp32**

​	**BLIP2Qformer的INIT中max_txt_len可能需要修改**（统计一下文本长度，再学习一下tokenizer的用法，可能与Q-query的长度也有关？（Q-query长度也是32））



​	修改BLIP2.PY

​	修改BLIP2Qformer.py (应该可用了)

## 10.16

**文本字符长度分布**：以此确定max_txt_len

Chebi20mm

![image-20241016111808100](.\image-20241016111808100.png)![image-20241016112120280](.\image-20241016112120280.png)

![image-20241016112538799](.\image-20241016112538799.png)

Pubchem:![image-20241016112953565](.\image-20241016112953565.png)![image-20241016113332431](.\image-20241016113332431.png)



![image-20241016113507647](.\image-20241016113507647.png)

MOL-ins:![image-20241016113857990](.\image-20241016113857990.png)

openai经验:一个有用的经验法则是，对于常见的英语文本，一个标记通常对应于 ~4 个字符的文本。这大约相当于一个单词的 3/4（所以 100 个标记 ~= 75 个单词）。

**写Dataset**(多任务训练):

​	**目前的思路是concat数据集**

​	另一种思路是：**按照task分类，然后在一个 epoch先训s2n,再训s2t**



## 10.17

dataset写的可能有问题 需要验证

loss函数的权重是否根据数据集大小调整？

几个trainloader可能不一样大![image-20241017170223351](.\image-20241017170223351.png)

conf里有一些还没定义，还没用到

多GPU

分布式启动工具：torchrun --nproc_per_node=1 train_mymodel.py   （nproc_per_node=实际GPU数量）

多卡测试还没写

## 10.18

BLIP2Qformer第120行    molecular_embeds = molecular_embeds.view(embeddings.size(0), 1, 1024)   是sequence length的问题，应该把切片后的原子也弄进去？

上传到GIT

**统计一下经TOKENizer后的长度**:

MOL-INS:![image-20241018172223378](.\image-20241018172223378.png)

PUBCHEM:![image-20241018173505461](.\image-20241018173505461.png)![image-20241018181544328](.\image-20241018181544328.png)![image-20241018182018347](.\image-20241018182018347.png)![image-20241018182204757](.\image-20241018182204757.png

CHEBL20MM:

![image-20241018182233160](.\image-20241018182233160.png)![image-20241018182359109](.\image-20241018182359109.png)



## 10.19

修改scheduler 修改conf 调整max_txt_len为128

是不是数据集有问题？两个任务不应该合二为一？

## 10.21

设计Decoder，他应该输出两个向量

应该先冻结Q-former训Decoder？或者先用max_txt=144的Q训Decoder（此时冻结Q），训好后再用192的训（此时开放Q）

要不要将分子切片：

​	**1.不切不使用（目前的方式）**  

​	2.切，仅在训Q-former时使用，相当于让Q-former输出一个质量更好的分子embeds

​	3.切，仅在进decoder时使用，并加上位置编码，设置掩码为0（可见），相当于让decoder根据这些再逐个输出向量

​	4.切，全部使用



**为预留切的可能性，目前decoder在处理来自Q的输入时会proprecess增加seqlen维度（默认为1）（如果切，还要考虑每个分子切出来的原子数不一致的问题，即seqlen不一致）**

故在decoder中不使用nn.embeds、不使用投影（投影在Q-former完成）（padding_idx=1）

掩码：来自q-former的输入应该全能看见

​	来自反应物的输入，似乎应该弄一个**start的token**（seqlen由2变3），否则他就能看见自己（**可以看下Q-former的start token经过q后的输入**）

LOSS函数似乎可类似ITC这种  是否引入对比学习LOSS？

最终排名可以借鉴MOLR  /root/autodl-tmp/MolR/src/real_reaction_test/real_reaction_test.py

## 10.22

写ITM LOSS  ITG？  ITM写不了

确认Decoder的输入输出shape  输入   输出要是：【BZ,2,DIM】

下一次训练时self.itm head要改为mtm head

开放不开放Q-former的训练 **暂定是不开放**

**写逆合成数据集**  看一下另一台机器上的USPTO-50K

**目前只保留了两个反应物的那些反应**

为什么一个smiles会被处理成19个（推测最低19个？）





## 10.23

**注意decoder的输入是否需要[::-1]**

因为Q-former的输入是bz,num_queries,768，现在有两种处理方法：

**1.在输入decoder前投影 目前选的是1 将num_q*768投影到d_model

**2.不投影，就将一个分子视为用num_queries,768表示的向量**

**需要保证concat_all_gather聚集时，每块GPU上得到的矩阵相同？**否则torch.linspace就不对了

可以计算两个二维矩阵之间的距离，但需要明确距离的定义。常见的距离计算方式有以下几种：

1. **欧几里得距离**：两个矩阵中对应元素之间的平方差求和，再开平方。对于两个矩阵 和 ，其欧几里得距离可以表示为： 
2. **曼哈顿距离**：计算两个矩阵中对应元素之间绝对差值的总和： 
3. **余弦距离**：可以用于评估两个矩阵（视为向量）之间的相似度。计算公式为： 其中 表示点积， 和 分别是矩阵的范数。
4. **其他距离度量**：还有许多其他距离度量，如切比雪夫距离、马氏距离等，也可以根据需要进行计算。

在计算之前，你需要确定两个矩阵的维度是否相同。如果维度不相同，就无法直接进行上述距离的计算。

写Decoder的generate



## 10.24

修改了Decoder中计算相似度的办法（ITC），成功

做PPT



## 10.25

写在USPTO数据集上测试的办法

predict_reactants_USPTO(self, src,tgt1,tgt2):#更加限定解空间的预测  即对于每个src对应的pre1,pre2,寻找pre1,pre2都与他接近的（解空间不是单个分子，而是分子对） 这个函数似乎不应该要求tgt1,tgt2，尽管这俩没有用

USPTO中有一些相同的分子，这可能会对Decoder的训练以及测试产生影响。如ITC LOSS中出现两个一样的

## 10.26

针对USPTO数据集，应该建立映射

针对自由数据集，应该建立分子字典

写自己的BLIPOUTPUT类

loss：
1.一个反应物的   缺少的这个反应物可以用0向量  结合平均距离，应为反应物1
2.先后顺序  这个是否是人为规定的？是否应该按照反应物与产物经molr后的欧式距离排序  即先想到化学性质相近的![image-20241030111109985](.\image-20241030111109985.png)
3.**loss中应该拉近欧式距离（if 并且远离与他最相似的那个的欧式距离)**

修改MolR的问题

## 10.27-10.30

跑模型

## 10.31

既然有分子字典了 是不是可以映射？  按label那样去搞（也就是传统transformer做法，这需要把分类头从1024维改为分子字典的大小）（这么做本质是没变的，即限定解空间是给定分子集）

删去了计算LOSS时的norm（这一操作应该同步到加载分子字典时，要么都NORM，要么都不NORM,不然正确率很哈人）

## 11.1

loss1:删去NORM

LOSS2:如下

1.因数据集中reac2更近，所以输入给模型的应该先是reac2再是reac1？或者说我们希望模型先预测化学性质相近的

2.损失函数权重自学习？（这个可能不太好）

3.不使用全0start token，而采用src的embedding 

## 11.4

应该增大LOSS中对第二个损失(也就是模型预测的第二个反应物，对应数据集第一个反应物)的惩罚：增大后发现竟然ACC1

写评估函数

## 11.5

改进：现在是从dataset里选模型的输入顺序了

## 11.6

训练简单的MLP和DEC作为对照:MLP失败 DEC失败

MLP：输出2048维向量 前1024作为X1，后1024作为2

DEC：隐藏层变为128 多头变为4 n_layers变为3

可以进一步缩小搜索范围

数据集加权

## **11.7**

**找到了RDKIT中提取官能团的代码**，可以尝试往数据集里加这些东西

## 11.8

设计其他单步逆合成模型的baseline

1.设计decoder

2.判断mrl框架  换用unimol

3.**STAGE3只在uspto上训练**呢？此时可以仿照Flamingo每一层都增加门控机制，并且冻住decoder 详见消融实验部分

4.训练集取消valid 只用test 已

5.如果两个预测物的embedding过近，应该施加惩罚  比如弄两个PROJ头 对比一下（参考Simclr）

6.**进一步缩小搜索范围**

7.**STAGE2：不冻结Q-former**（需要在已经训好的Decoder头上？还是从头开始？） 也就是正规的Q-former训练的第二阶段 参照Cambrian-1,Decoder每一层都聚合特征（每个BLOCK都引入SVA以更好的聚合Q抽出来的特征）

8.是否需要将MRL纳入模型中？仿照SIMCLR的做法（实验在MOLR输出后增加一个proj，梯度爆炸）

9.在pretrain阶段增大数据集？数据集详见https://github.com/junxia97/awesome-pretrain-on-molecules?tab=readme-ov-file

10.num_query是否不需要这么多？**分子表征是否也不需要这么多维度**？（比如限制DEC隐藏层的维度） BERT三万个词（根） bert base的词维度才768维度  我们才多少分子呢？  是不是可以降低DEC维度 **同时在Q的投影层降低维度**

11.Q-Former参数量大收敛更慢，数据量小的场景无法达到LLaVA-1.5这样的性能   或者结合10，既**减少num_q的个数，再减少num_q的维度**

12.**为了确认Q-former有没有用，应该安排一个没有num_q换为molr输出的模型**

13.neg_loss真的有用吗？



STAGE1:pretrainQ-former:这个阶段可以尝试加入官能团数据了，同时扩大数据集，加入分子性质描述等。也要加入官能团的知识



我统计了localretro在USPTO50K测试集的预测结果。对于测试集中的5007个反应，localretro共输出222338个分子（去重后），其中215581个分子不在解空间内（测试集中出现的分子集合），200449个分子不在市售可用分子列表中（即Retro*的叶子节点）



## 11.11

**不能只改变欧氏距离的权重，其他相对应的权重也要改**

无论是先1后2还是先2后1，模型都是先预测的那个准确率低

## 11.12

如果是想合并两种模型，模型保存检查点可以保存两个：一个是LOSS最低的，一个是预测ACC最高的（**也就是我们期望的那个LOSS？**），这两个并不完全挂钩

这是因为：

​	1.只有一个LOSS与ACC挂钩（也就是该模型负责的第二个输出）

​	2.多个LOSS间可能存在互相促进的作用，总体优化方向是总LOSS降低，这就可能导致跟1的冲突，虽然总LOSS下降了，但可能我们期望的那个LOSS反而上升了



Decoder最后的投影层似乎没什么作用（假如要基于标签，可以把这个层用上而不用添加新的投影层）但是基于标签不是那么好，一来是与设计理念背道而驰，二来是字典实在太大了，即使是chatgpt3，字典也才5W

## 11.14

是否可以加上评估函数？评估模型输出的TOPK中每一个化合物的可能性。（比如基于欧氏距离，越小的可能性越高，但是要综合考虑）

**引入标签对齐损失？**再加一个MLP层，用于预测其在字典中的位置

**剔除那些分子长度明显对的**

## 11.15

虽然acc1不一样， 不一样就不一样吧  哪个高就用哪个  比如在ft21高就用ft21存pkl

如果对比试验测试Q-former的效果，可以直接将mrl的embedding expand

## 11.18

![image-20241118115741355](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20241118115741355.png)

**我的模型：（未知反应类型）TOP1 3 5 10分别是68.1 83.4 86.8 90.32**,超过了无模板SOTA PMSR在已知反应类型的表现，和基于模板的SOTA在已知反应类型的情况下也具有相当竞争力（TOP1超过了它）



1.测试集是否与别人相同？

​	**不完全相同**。我的测试集相当于别人测试集的一个**更困难的子集**。

​	原测试集共5007个反应。我的测试集去除掉了所有单反应物的反应，共计1450个。去除原因是在USPTO数据集既无反应条件，又无试剂的情况下**单反应物的反应意义不大**。

​	同时，单反应物的情况对于PMSR等一系列逐个token生成的模型而言是更简单的任务（因为预测的token减少），所以之前的模型不考虑去除掉单反应物的反应。相当于我的模型**在更困难的任务上取得了更好的成绩**。



2.是否数据泄露、数据清洗？

​	从数据集的角度看,逆合成相关任务用了两部分数据集：USPTO-50K和Mol-ins数据集的逆合成子集。两个数据集的划分均为典型做法，未作改变。

​	**暂没有统计**Mol-ins数据集的逆合成训练集中是否泄露了USPTO-50K测试集的数据。参考PMSR在Pistachio数据集训练时没有做数据清洗，故我也没做**数据清洗。**

​	从模型角度看，由于在前向传播时**未传入待预测产物的数据**（如先预测产物1再预测产物2的模型就不传入产物2的数据），且模型在测试时未更新梯度，同时模型处于eval模式，排除了数据泄露可能。

```
#模型架构
reactants_embedding_mrl_input = torch.cat([start_token,reactant1_embedding_mrl],dim=1)
predict_reactants_embedding = self.Decoder(product_embedding_proj,reactants_embedding_mrl_input)#待预测产物2，传入start token和产物1
#测试
model.eval()
with torch.no_grad():
	验证代码
```



3.验证指标？

​	**验证指标为TopK-acc,与他人做法相同**。具体为：

​	（1）对于每个反应，模型直接生成两个反应物列表（反应物1列表，反应物2列表）。每个列表各含有50个smiles，即视为模型预测的前50个反应物。

​	（2）以TOPK=1为例，当且仅当反应物1列表、反应物2列表的第一个产物都与Ground truth对应上时才视为正确。



4.为什么模型预测第二个产物的正确率总是比第一个高？

​	推测是**teacher forcing+看到的信息更多**的缘故。模型预测第二个产物时既可以看到Q-former的信息，又可以看到start token和第一个产物，而模型预测第一个时只能看到前两者。



5.为什么提点这么快？

​	因为模型是采用ensemble融合了先1后2和先2后1模型的输出，之前一直在训先2后1的模型，期间先1后2的模型一直用的是未收敛版。现在俩模型都训好了。

