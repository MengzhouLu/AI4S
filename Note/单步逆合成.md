# 单步逆合成

## 10.14

处理数据集：基本划分为smiles-text格式，目前仅采用smiles,name,decription，还在考虑MOL-INS用哪些数据

mol-ins:selfies to smiles    注意：mol to smiles有些参数要调（应该全部使用RDKIT转化为标准Smiles）

## 10.15

**借助RDKIT标准化smiles**

mol-ins:

​	property_prediction:362100

​	molecular_description_generation:298319

​	retro:129684(用于Fine-tune)

ChEBI-20-MM:

​	test:3297

​	valid:3299

​	train:26402

PUBchem

​	smiles to name:260408

​	smiles to text:261325



split在dataset里面写吧

**部署Q-former**：

​	Q-former超参数：[LAVIS/lavis/projects/blip2/train/pretrain_stage1.yaml at main · salesforce/LAVIS · GitHub](https://github.com/salesforce/LAVIS/blob/main/lavis/projects/blip2/train/pretrain_stage1.yaml)



​	**注意精度问题！目前MOLR默认输出精度为fp32**

​	**BLIP2Qformer的INIT中max_txt_len可能需要修改**（统计一下文本长度，再学习一下tokenizer的用法，可能与Q-query的长度也有关？（Q-query长度也是32））



​	修改BLIP2.PY

​	修改BLIP2Qformer.py (应该可用了)

## 10.16

**文本字符长度分布**：以此确定max_txt_len

Chebi20mm

![image-20241016111808100](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20241016111808100.png)![image-20241016112120280](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20241016112120280.png)

![image-20241016112538799](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20241016112538799.png)

Pubchem:![image-20241016112953565](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20241016112953565.png)![image-20241016113332431](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20241016113332431.png)



![image-20241016113507647](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20241016113507647.png)

MOL-ins:![image-20241016113857990](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20241016113857990.png)

openai经验:一个有用的经验法则是，对于常见的英语文本，一个标记通常对应于 ~4 个字符的文本。这大约相当于一个单词的 3/4（所以 100 个标记 ~= 75 个单词）。

**写Dataset**(多任务训练):

​	**目前的思路是concat数据集**

​	另一种思路是：**按照task分类，然后在一个 epoch先训s2n,再训s2t**



## 10.17

dataset写的可能有问题 需要验证

loss函数的权重是否根据数据集大小调整？

几个trainloader可能不一样大![image-20241017170223351](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20241017170223351.png)

conf里有一些还没定义，还没用到

多GPU

分布式启动工具：torchrun --nproc_per_node=1 train_mymodel.py   （nproc_per_node=实际GPU数量）

多卡测试还没写

## 10.18

BLIP2Qformer第120行    molecular_embeds = molecular_embeds.view(embeddings.size(0), 1, 1024)   是sequence length的问题，应该把切片后的原子也弄进去？

上传到GIT

**统计一下经TOKENizer后的长度**:

MOL-INS:![image-20241018172223378](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20241018172223378.png)

PUBCHEM:![image-20241018173505461](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20241018173505461.png)![image-20241018181544328](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20241018181544328.png)![image-20241018182018347](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20241018182018347.png)![image-20241018182204757](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20241018182204757.png

CHEBL20MM:

![image-20241018182233160](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20241018182233160.png)![image-20241018182359109](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20241018182359109.png)



## 10.19

修改scheduler 修改conf 调整max_txt_len为128

是不是数据集有问题？两个任务不应该合二为一？

## 10.21

设计Decoder，他应该输出两个向量

应该先冻结Q-former训Decoder？或者先用max_txt=144的Q训Decoder（此时冻结Q），训好后再用192的训（此时开放Q）

要不要将分子切片：

​	**1.不切不使用（目前的方式）**  

​	2.切，仅在训Q-former时使用，相当于让Q-former输出一个质量更好的分子embeds

​	3.切，仅在进decoder时使用，并加上位置编码，设置掩码为0（可见），相当于让decoder根据这些再逐个输出向量

​	4.切，全部使用



**为预留切的可能性，目前decoder在处理来自Q的输入时会proprecess增加seqlen维度（默认为1）（如果切，还要考虑每个分子切出来的原子数不一致的问题，即seqlen不一致）**

故在decoder中不使用nn.embeds、不使用投影（投影在Q-former完成）（padding_idx=1）

掩码：来自q-former的输入应该全能看见

​	来自反应物的输入，似乎应该弄一个**start的token**（seqlen由2变3），否则他就能看见自己（**可以看下Q-former的start token经过q后的输入**）

LOSS函数似乎可类似ITC这种  是否引入对比学习LOSS？

最终排名可以借鉴MOLR  /root/autodl-tmp/MolR/src/real_reaction_test/real_reaction_test.py

## 10.22

写ITM LOSS  ITG？  ITM写不了

确认Decoder的输入输出shape  输入   输出要是：【BZ,2,DIM】

下一次训练时self.itm head要改为mtm head

开放不开放Q-former的训练 **暂定是不开放**

**写逆合成数据集**  看一下另一台机器上的USPTO-50K

**目前只保留了两个反应物的那些反应**

为什么一个smiles会被处理成19个（推测最低19个？）





## 10.23

**注意decoder的输入是否需要[::-1]**

因为Q-former的输入是bz,num_queries,768，现在有两种处理方法：

**1.在输入decoder前投影 目前选的是1 将num_q*768投影到d_model

**2.不投影，就将一个分子视为用num_queries,768表示的向量**

**需要保证concat_all_gather聚集时，每块GPU上得到的矩阵相同？**否则torch.linspace就不对了

可以计算两个二维矩阵之间的距离，但需要明确距离的定义。常见的距离计算方式有以下几种：

1. **欧几里得距离**：两个矩阵中对应元素之间的平方差求和，再开平方。对于两个矩阵 和 ，其欧几里得距离可以表示为： 
2. **曼哈顿距离**：计算两个矩阵中对应元素之间绝对差值的总和： 
3. **余弦距离**：可以用于评估两个矩阵（视为向量）之间的相似度。计算公式为： 其中 表示点积， 和 分别是矩阵的范数。
4. **其他距离度量**：还有许多其他距离度量，如切比雪夫距离、马氏距离等，也可以根据需要进行计算。

在计算之前，你需要确定两个矩阵的维度是否相同。如果维度不相同，就无法直接进行上述距离的计算。

写Decoder的generate



## 10.24

修改了Decoder中计算相似度的办法（ITC），成功

做PPT



## 10.25

写在USPTO数据集上测试的办法

predict_reactants_USPTO(self, src,tgt1,tgt2):#更加限定解空间的预测  即对于每个src对应的pre1,pre2,寻找pre1,pre2都与他接近的（解空间不是单个分子，而是分子对） 这个函数似乎不应该要求tgt1,tgt2，尽管这俩没有用

USPTO中有一些相同的分子，这可能会对Decoder的训练以及测试产生影响。如ITC LOSS中出现两个一样的

## 10.26

针对USPTO数据集，应该建立映射

针对自由数据集，应该建立分子字典

写自己的BLIPOUTPUT类

loss：
1.一个反应物的   缺少的这个反应物可以用0向量  结合平均距离，应为反应物1
2.先后顺序  这个是否是人为规定的？是否应该按照反应物与产物经molr后的欧式距离排序  即先想到化学性质相近的![image-20241030111109985](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20241030111109985.png)
3.**loss中应该拉近欧式距离（if 并且远离与他最相似的那个的欧式距离)**

修改MolR的问题

## 10.27-10.30

跑模型

## 10.31

既然有分子字典了 是不是可以映射？  按label那样去搞（也就是传统transformer做法，这需要把分类头从1024维改为分子字典的大小）（这么做本质是没变的，即限定解空间是给定分子集）

删去了计算LOSS时的norm（这一操作应该同步到加载分子字典时，要么都NORM，要么都不NORM,不然正确率很哈人）

## 11.1

loss1:删去NORM

LOSS2:如下

1.因数据集中reac2更近，所以输入给模型的应该先是reac2再是reac1？或者说我们希望模型先预测化学性质相近的

2.损失函数权重自学习？（这个可能不太好）

3.不使用全0start token，而采用src的embedding 

## 11.4

应该增大LOSS中对第二个损失(也就是模型预测的第二个反应物，对应数据集第一个反应物)的惩罚：增大后发现竟然ACC1

写评估函数

## 11.5

改进：现在是从dataset里选模型的输入顺序了

## 11.6

训练简单的MLP和DEC作为对照

MLP：输出2048维向量 前1024作为X1，后1024作为2

DEC：隐藏层变为128 多头变为4 n_layers变为3

可以进一步缩小搜索范围
